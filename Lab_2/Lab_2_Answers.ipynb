{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1) Random sampling (torch.randn, torch.rand, torch.distributions module)\n",
    "# 2) Demonstration of central limit theorem through simulation\n",
    "# 3) Autodiff and computational graph construction (grad fields for tensors, backward method (and how it discards the implicitly created computational graph), optimizers, loss functions)\n",
    "# 4) Application of autodiff to a very simple problem (like fitting a normal and/or a beta distribution through maximum likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Generating some data\n",
    "In order to do this lab you're gonna need some data. Usually I wouldn't suggest doing this but why don't we just make up our data. Using PyTorch's __[built in distribution sampling functions](https://pytorch.org/docs/stable/distributions.html)__ create a dataset sampled from a Bernoulli, and another dataset sampled from a normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from a normal distribution\n",
    "First, draw a sample from a normal distribution using the parameters below. Story your data in the variable named `X_norm`, and print out the sample mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inaccurate sample mean: 1.5726956129074097, sample std: 0.2193295955657959\n"
     ]
    }
   ],
   "source": [
    "trueMu = 1.491# mean\n",
    "trueSig = 0.1876# standard deviation\n",
    "nNormal = 10\n",
    "#### your code here ####\n",
    "normalDist = torch.distributions.normal.Normal(loc = trueMu, scale = trueSig)\n",
    "X_norm = normalDist.sample(sample_shape = [nNormal,])\n",
    "print(\"Inaccurate sample mean: {}, sample std: {}\".format(X_norm.mean(), np.std(torch.Tensor.numpy(X_norm))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How close are the sample estimates of the mean and standard deviation to the true values? How can you improve the sample estimation of these values? What is the theorem that backs up your answer? Please describe how you might do that below and then implement your solution. Continue to store your data in the variable `X_norm`.\n",
    "\n",
    "Note, we care more about you explaining that you know what you're doing, rather than how close you can get your estimates. \n",
    "### 1) Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More accurate sample mean: 1.4929146766662598, sample std: 0.18604473769664764\n"
     ]
    }
   ],
   "source": [
    "#### your code here ####\n",
    "nNormal = 10000\n",
    "X_norm = normalDist.sample(sample_shape = [nNormal,])\n",
    "print(\"More accurate sample mean: {}, sample std: {}\".format(X_norm.mean(), np.std(torch.Tensor.numpy(X_norm))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary data generation\n",
    "Follow the same steps as above, but instead of a normal distribution we want binary data. What distribution can you use to sample binary data? Please ensure that the sample estimate probability of seeing a value of `1` is close to `trueP` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample estimate P(1): 0.2287999987602234\n"
     ]
    }
   ],
   "source": [
    "trueP = 0.23\n",
    "nBinary = 10000\n",
    "#### your code here ####\n",
    "bern = torch.distributions.bernoulli.Bernoulli(torch.tensor([trueP]))\n",
    "X_bin = bern.sample(sample_shape = [nBinary,])\n",
    "print(\"Sample estimate P(1): {}\".format(X_bin.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Maximum Likelihood Estimation\n",
    "You'll be learning more about maximum likelihood estimation (MLE) next week, but you'll have a chance to gain some intuition, and see how one can implement MLE here.\n",
    "\n",
    "In the previous section you learned how more data can help you estimate the parameters of distributions that observations are drawn from. In the example of data drawn from a normal distribution it's intuitive how we can estimate the mean and standard deviation which parameterize the distribution which generated the data. Put another way, given the data we've seen and an idea for the function which generated this data (ie a normal distribution) we want find the parameters that likely describe how this data was generated. \n",
    "\n",
    "Let's examine some of the moving parts. We have some data, $X \\in R^{n}$, and we know that each observation $x^{i}$ was generated with probability defined by $f(x^{i}|\\theta)$ where $\\theta$ are the parameters that help us define this function. I know this is a little handwavy but just bear with me here. You'll get a better theoretical understanding in class. Let's look at the example of the normal distribution as a data generating function. Here $f(x^{i}|\\theta) = f(x^{i}|\\mu, \\sigma) = \\frac{1}{2\\pi\\sigma^2}exp(-\\frac{(x^{i}-\\mu)^2}{2\\sigma^2})$ which isthe probability density function of the __[normal distribution](https://en.wikipedia.org/wiki/Normal_distribution)__. \n",
    "\n",
    "So that's the probability of observing one $x^i$, but what about all $n$ of those dudes? Well since we're dealing with indpendently and identically distributed data we just multiply that function over and over to get the likelihood of observing our data. That is $f(X|\\mu, \\sigma) = \\prod_{i = 1}^{n}f(x^{i}|\\mu, \\sigma) = \\frac{1}{2\\pi\\sigma^2}^{\\frac{n}{2}}exp(-\\frac{\\sum_{i=1}^{n}(x^{i}-\\mu)^2}{2\\sigma^2})$\n",
    "\n",
    "Ok, so that was alot. But essentially we're just deriving the probability of seeing all our data. The next and final step is to take the log of the final equation above. We do this because of numerical stability. When you're dealing with small numbers, like fractions, repeatedly muliplying them can lead to underflow. $$L(X|\\mu, \\sigma) = \\frac{-n}{2}log(2\\pi\\sigma^2) - \\frac{1}{2\\pi\\sigma^2}\\sum_{i=1}^{n}(x^i - \\mu)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During MLE our goal is to find the parameters $\\theta$ which maximize the function $L(X|\\theta)$. In the cell below fix $\\sigma$ at the true value of `0.1876` and calculate the likelihood function for the data observed with varied $\\mu$ between 1 and 2 with a step size of 0.05. Which value of $\\mu$ maximizes the function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### your code here ####\n",
    "muList = np.arange(1, 2.05, 0.05)\n",
    "logLikList = []\n",
    "n = X_norm.shape[0]\n",
    "for mu in muList:\n",
    "    logLik = (-(n/2)*np.log(2*np.pi*(trueSig**2)) - (1/(2*(trueSig**2)))*np.sum((X_norm.data.numpy() - mu)**2))\n",
    "    logLikList.append(logLik)\n",
    "plt.plot(muList, logLikList)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE: Normal\n",
    "So from above we can see that at the true values of $\\sigma$ and $\\mu$ the log likelihood is maximized. How can we learn what these parameters should be in order to maximize the MLE? With a little PyTorch and gradient descent we can do just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Mu: tensor([0.5265], requires_grad=True)\n",
      "Initial Sigma: tensor([0.1850], requires_grad=True)\n",
      "log likelihood: [-133777.72], learned mu = [0.52654177], learned sigma [0.18501782]\n",
      "log likelihood: [-10645.307], learned mu = [1.0949963], learned sigma [1.0618695]\n",
      "log likelihood: [2628.2944], learned mu = [1.4929128], learned sigma [0.18604474]\n",
      "log likelihood: [2628.2954], learned mu = [1.4929147], learned sigma [0.18604474]\n",
      "log likelihood: [2628.2954], learned mu = [1.4929147], learned sigma [0.18604474]\n",
      "log likelihood: [2628.2954], learned mu = [1.4929147], learned sigma [0.18604474]\n",
      "log likelihood: [2628.2954], learned mu = [1.4929147], learned sigma [0.18604474]\n",
      "log likelihood: [2628.2954], learned mu = [1.4929147], learned sigma [0.18604474]\n",
      "log likelihood: [2628.2954], learned mu = [1.4929147], learned sigma [0.18604474]\n",
      "log likelihood: [2628.2954], learned mu = [1.4929147], learned sigma [0.18604474]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the parameters to learn. Notice how requires_grad is set to true. This is because we want\n",
    "# to update this parameter with the the MLE.\n",
    "learnedMu = torch.autograd.Variable(torch.rand(1), requires_grad = True)\n",
    "learnedSigma = torch.autograd.Variable(torch.rand(1), requires_grad = True)\n",
    "print(\"Initial Mu: {}\".format(learnedMu))\n",
    "print(\"Initial Sigma: {}\".format(learnedSigma))\n",
    "\n",
    "\n",
    "# This says how big of a step should we take each time we update our parameters.\n",
    "# there is some instability in the updates, so you might have to run a couple of times to get\n",
    "# values that look like what they should be.\n",
    "learningRate = 0.000001\n",
    "# learnedProb.zero_()\n",
    "n = X_norm.shape[0]\n",
    "for myIter in range(1000):\n",
    "    #Instantiate the MLE for a normal distribution here using torch functions.\n",
    "    #### your code here ####\n",
    "    MLE = -((n/2)*torch.log(2*np.pi*(learnedSigma**2)) + (1/(2*(learnedSigma**2)))*torch.sum((X_norm - learnedMu)**2))\n",
    "    #### end code here ####\n",
    "    MLE.backward()\n",
    "    # Now we run over the data multiple times, and each time we calculate how far away we are from\n",
    "    # the maximized MLE and update our parameters accordingly\n",
    "    if myIter % 100 == 0:\n",
    "        print(\"log likelihood: {}, learned mu = {}, learned sigma {}\".format(MLE.data.numpy(),\n",
    "                                                                          learnedMu.data.numpy(),\n",
    "                                                                          learnedSigma.data.numpy()))\n",
    "    # SGD update for our learned parameters\n",
    "    learnedMu.data = learnedMu.data + learnedMu.grad.data*learningRate\n",
    "    learnedSigma.data = learnedSigma.data + learnedSigma.grad.data*learningRate\n",
    "    # Clear out the gradient information\n",
    "    learnedMu.grad.data.zero_()\n",
    "    learnedSigma.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE: Bernoulli\n",
    "Finally, using the binary data we generated using a Bernoulli distribution let's learn the true value for $p$. Similarly to the above formulation you'll need to create a parameter to learn, as well as define the MLE for our binary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood: -5757.607421875, learned prob = [0.356143], update = [-5553.4194]\n",
      "log likelihood: -5378.24267578125, learned prob = [0.22880003], update = [0.]\n",
      "log likelihood: -5378.24267578125, learned prob = [0.22880003], update = [0.]\n",
      "log likelihood: -5378.24267578125, learned prob = [0.22880003], update = [0.]\n",
      "log likelihood: -5378.24267578125, learned prob = [0.22880003], update = [0.]\n",
      "log likelihood: -5378.24267578125, learned prob = [0.22880003], update = [0.]\n",
      "log likelihood: -5378.24267578125, learned prob = [0.22880003], update = [0.]\n",
      "log likelihood: -5378.24267578125, learned prob = [0.22880003], update = [0.]\n",
      "log likelihood: -5378.24267578125, learned prob = [0.22880003], update = [0.]\n",
      "log likelihood: -5378.24267578125, learned prob = [0.22880003], update = [0.]\n"
     ]
    }
   ],
   "source": [
    "# Define the variable p variable to learn\n",
    "#### your code here ####\n",
    "learnedP = torch.autograd.Variable(torch.rand(1), requires_grad = True)\n",
    "#### end code here ####\n",
    "# Once again, running this can be a little temperamental, so if you're not getting the right answer\n",
    "# run it a few more times just ot be sure\n",
    "learningRate = 0.00001\n",
    "for myIter in range(1000):\n",
    "    # define the MLE for iid data from a Bernoulli\n",
    "    #### your code here ####\n",
    "    MLE = torch.sum(torch.log(X_bin*learnedP + (1-X_bin)*(1-learnedP)))\n",
    "    #### end code here ####\n",
    "    MLE.backward()\n",
    "    if myIter % 100 == 0:\n",
    "        print(\"log likelihood: {}, learned prob = {}, update = {}\".format(MLE.data.numpy(),\n",
    "                                                                          learnedP.data.numpy(),\n",
    "                                                                          learnedP.grad.data.numpy()))\n",
    "    # Update your learned parameter with a gradient descent step (similar to the normal case)\n",
    "    #### your code here ####\n",
    "    learnedP.data = learnedP.data + learnedP.grad.data*learningRate\n",
    "    #### end code here ####\n",
    "    learnedP.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Computational Methods)",
   "language": "python",
   "name": "compmeth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
