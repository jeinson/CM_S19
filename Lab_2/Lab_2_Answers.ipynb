{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![smbcComic](https://i0.wp.com/flowingdata.com/wp-content/uploads/2014/06/Paranormal-distribution-smaller.png?w=400&ssl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Generating some data\n",
    "In order to do this lab you're gonna need some data. Holding to the tradition of data science, let's make it up as we go. Using PyTorch's __[built in distribution sampling functions](https://pytorch.org/docs/stable/distributions.html)__ create a dataset sampled from a Bernoulli, and another dataset sampled from a normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from a normal distribution\n",
    "First, draw a sample from a normal distribution using the parameters below. Store your data in the variable named `X_norm`, and print out the sample mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inaccurate sample mean: 1.4832658767700195, sample std: 0.18185360729694366\n"
     ]
    }
   ],
   "source": [
    "trueMu = 1.491# mean\n",
    "trueSig = 0.1876# standard deviation\n",
    "nNormal = 10\n",
    "#### your code here ####\n",
    "normalDist = torch.distributions.normal.Normal(loc = trueMu, scale = trueSig)\n",
    "X_norm = normalDist.sample(sample_shape = [nNormal,])\n",
    "print(\"Inaccurate sample mean: {}, sample std: {}\".format(X_norm.mean(), np.std(torch.Tensor.numpy(X_norm))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How close are the sample estimates of the mean and standard deviation to the true values? How can you improve the sample estimation of these values? What is the theorem that backs up your answer? Please describe how you might do that below and then implement your solution.** Continue to store your data in the variable `X_norm`.\n",
    "\n",
    "Note, we care more about you explaining that you know what you're doing, rather than how close you can get your estimates. \n",
    "### Your answer here\n",
    "With the law of large numbers the more samples we take the more accurate our estimation of the mean will become"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More accurate sample mean: 1.4925638437271118, sample std: 0.18688896298408508\n"
     ]
    }
   ],
   "source": [
    "#### your code here ####\n",
    "nNormal = 10000\n",
    "X_norm = normalDist.sample(sample_shape = [nNormal,])\n",
    "print(\"More accurate sample mean: {}, sample std: {}\".format(X_norm.mean(), np.std(torch.Tensor.numpy(X_norm))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary data generation\n",
    "Follow the same steps as above, but instead of a normal distribution we want binary data. What distribution can you use to sample binary data? Make sure that the sample estimate probability of seeing a value of `1` is close to `trueP`. **Please plot the mean of your sample as you increase n. Say from 10 to 10,000** and confirm that the mean does converge to `trueP`. Remember to store your largest sample in the variable `X_bin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample estimate P(1): 0.225600004196167\n"
     ]
    }
   ],
   "source": [
    "trueP = 0.23\n",
    "#### your code here ####\n",
    "nArray = np.arange(10, 10001, 10)\n",
    "estimatedMeans = np.ones(nArray.shape)\n",
    "bern = torch.distributions.bernoulli.Bernoulli(torch.tensor([trueP]))\n",
    "for idx, nBinary in enumerate(nArray):\n",
    "    X_bin = bern.sample(sample_shape = [nBinary,])\n",
    "    estimatedMeans[idx] = X_bin.mean()\n",
    "    \n",
    "plt.plot(nArray, estimatedMeans)\n",
    "plt.ylim([0,0.5])\n",
    "plt.hlines(trueP, np.min(nArray), np.max(nArray))\n",
    "plt.show()\n",
    "print(\"Sample estimate P(1): {}\".format(X_bin.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Maximum Likelihood Estimation\n",
    "You'll be learning more about maximum likelihood estimation (MLE) next week, but you'll have a chance to gain some intuition, and see how one can implement MLE here.\n",
    "\n",
    "In the previous section you learned how more data can help you estimate the parameters of distributions that observations are drawn from. In the example of data drawn from a normal distribution it's intuitive how we can estimate the mean and standard deviation which parameterize the distribution which generated the data. Put another way, given the data we've seen and an idea for the function which generated this data (ie a normal distribution) we want find the parameters that likely describe how this data was generated. \n",
    "\n",
    "Let's examine some of the moving parts. We have some data, $X \\in R^{n}$, and we know that each observation $x^{i}$ was generated with probability defined by $f(x^{i};\\theta)$ where $\\theta$ are the parameters that help us define this function. I know this is a little handwavy but just bear with me here. You'll get a better theoretical understanding in class. Let's look at the example of the normal distribution as a data generating function. Here $f(x^{i};\\theta) = f(x^{i};\\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{(x^{i}-\\mu)^2}{2\\sigma^2})$ which is the probability density function of the __[normal distribution](https://en.wikipedia.org/wiki/Normal_distribution)__. \n",
    "\n",
    "So that's the probability of observing one $x^i$, but what about all $n$ of those dudes? Well since we're dealing with indpendently and identically distributed data (or independent trials of data drawn from the same distribution) we just multiply that function over and over to get the likelihood of observing our data. Recall that $f(X;\\mu, \\sigma) = f(x^1, x^2,...,x^n;\\mu, \\sigma)$ is a joint distribution over all the variables in $X$. Because of independence this joint can be rewritten as the product of the likelihood of every $x^i$. That is $f(X;\\mu, \\sigma) = \\prod_{i = 1}^{n}f(x^{i}|\\mu, \\sigma) = \\frac{1}{2\\pi\\sigma^2}^{\\frac{n}{2}}exp(-\\frac{\\sum_{i=1}^{n}(x^{i}-\\mu)^2}{2\\sigma^2})$. The last step skips over some algebra when multiplying the probability density function of the normal $n$ times. \n",
    "\n",
    "Ok, so that was alot. But essentially we're just deriving the probability of seeing all our data. The next and final step is to take the log of the final equation above. We do this because of numerical stability. When you're dealing with small numbers, like fractions, repeatedly muliplying them can lead to underflow. $$L(X;\\mu, \\sigma) = \\frac{-n}{2}log(2\\pi\\sigma^2) - \\frac{1}{2\\pi\\sigma^2}\\sum_{i=1}^{n}(x^i - \\mu)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During MLE our goal is to find the parameters $\\theta$ which maximize the function $L(X;\\theta)$. In the cell below fix $\\sigma$ at the true value of `0.1876` and calculate the likelihood function for the data observed with varied $\\mu$ between 1 and 2 with a step size of 0.05. Which value of $\\mu$ maximizes the function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### your code here ####\n",
    "muList = np.arange(1, 2.05, 0.05)\n",
    "logLikList = []\n",
    "n = X_norm.shape[0]\n",
    "for mu in muList:\n",
    "    logLik = (-(n/2)*np.log(2*np.pi*(trueSig**2)) - (1/(2*(trueSig**2)))*np.sum((X_norm.data.numpy() - mu)**2))\n",
    "    logLikList.append(logLik)\n",
    "plt.plot(muList, logLikList)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE: Normal\n",
    "So from above we can see that at the true values of $\\sigma$ and $\\mu$ the log likelihood is maximized. How can we learn what these parameters should be in order to maximize the MLE? With a little PyTorch and gradient descent we can do just that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors, Variables and Computational graphs\n",
    "Insert Wizard of Oz \"Oh my\" joke here. Ok, moving on. If we're going to learn the parameters which maximize the MLE, we need to know which parameters to maximize, and how to run the MLE function as well as how to perform back propagation. Let's look at each of these pieces separately. \n",
    "\n",
    "### Tensors\n",
    "You can kind of think of Tensors and PyTorch's specialized NumPy matrices. How are they like NumPy matrices? They... well... they can hold values in a matrix (single or multidimensional) format. And how are the specialized? Unlike NumPy matrices Tensors can make use of GPU resources to significantly speed up matrix operations, which make up a bulk of what you need to do in deep learning.\n",
    "\n",
    "### Variables (or Tensors with a gradient)\n",
    "It used to be that PyTorch had a separate object called a Variable which were just like Tensors, except they're meant to hold values which will change as we learn their optimal or \"true\" value. In the most current version of PyTroch these two objects have been merged into the Tensor object. When you have a Tensor which needs to store data, but also needs to be updated as we learn its optimal or \"true\" value you create a tensor but require that the gradient be stored. You'll see this in the code below.\n",
    "\n",
    "Tensors hold onto a few important attributes which you can access through `Tensor.grad.data` and `Tensor.data`. One holds onto the actual values of the object, while the other holds onto the gradient. As we learn more about machine learning in this class the reason why these two pieces of information are important will become clear. One important thing to know is that every time a variable is back propogated through the gradient **accumulates** and is not replaced so you have to manually zero out the gradient with `Tensorgrad.data.zero_()`.\n",
    "\n",
    "\n",
    "### Computational graphs\n",
    "Computational graphs are structures which store chains of multiplications, additions, and other math functions (sin, exponential, log, Hadamard product, etc) in such a way that calculating the gradient via the chain rule is simple. Again, you will learn more about why it's important for us to do this in the future, but for now all you need to know is that PyTorch creates computational graphs which allow you to easily calculate the derivative using the chain rule. \n",
    "\n",
    "A computational graph is created only once you run code which starts applying math functions, multiplications, and additions to Tensors. This means it's a dynamic computation graph. And once you calculate the derivative using a backward pass it's destroyed, meaning that all the information needed to calculate the derivatives is gone. There's more to this, but if you want to learn more about it I would suggest this __[video](https://www.youtube.com/watch?v=nbJ-2G2GXL0)__ and this __[site](https://towardsdatascience.com/getting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec)__.\n",
    "\n",
    "## An example \n",
    "Let's see some of this in action. I've implemented code to learn how to maximize the MLE for data drawn from a normal distribution by learning the best $\\mu$ and $\\sigma$ parameters. But let's face it, it's gonna take me a while to figure out what I'm doing. So there are 4 versions of code below. All but the last one has a bug in it that prevents it from doing what we want. **Identify the bug in each version, and what needs to be fixed**. Of course you can peak ahead to see what changed, but looking at each version by itself before moving on to the next will be helpful for future debugging sessions.\n",
    "\n",
    "Tip: The third example might be kind of hard to spot because the code runs, but it never learns anything. Why might this be? What's going on with the gradient?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE for normally distributed data: Version 1\n",
    "#### What went wrong?\n",
    "Mu and Sigma were not constructed to have keep track of their gradient and so a backward call cannot be performed since no elements of this computational graph keep track of a gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Mu: tensor([0.8832])\n",
      "Initial Sigma: tensor([0.3356])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-63597dec9baa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mMLE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearnedSigma\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearnedSigma\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_norm\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlearnedMu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmyIter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mMLE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Now we run over the data multiple times, and each time we calculate how far away we are from\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# the maximized MLE and update our parameters accordingly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Computational_methods/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Computational_methods/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Taking our randomly generated data and wrapping it in a Variable call\n",
    "# When we set requires_grad=False this means that we don't want to update these parameters\n",
    "# and don't need to compute gradients with respect to these Variables. .\n",
    "x = torch.autograd.Variable(X_norm, requires_grad=False)\n",
    "\n",
    "\n",
    "# Instantiate the parameters to learn. Notice how requires_grad is set to true. This is because we want\n",
    "# to update this parameter with the the MLE.\n",
    "learnedMu = torch.rand(1, requires_grad = False)\n",
    "learnedSigma = torch.rand(1, requires_grad = False)\n",
    "print(\"Initial Mu: {}\".format(learnedMu))\n",
    "print(\"Initial Sigma: {}\".format(learnedSigma))\n",
    "\n",
    "\n",
    "# This says how big of a step should we take each time we update our parameters.\n",
    "# there is some instability in the updates, so you might have to run a couple of times to get\n",
    "# values that look like what they should be.\n",
    "learningRate = 0.000001\n",
    "# learnedProb.zero_()\n",
    "n = X_norm.shape[0]\n",
    "#Instantiate the MLE for a normal distribution here using torch functions.\n",
    "MLE = -((n/2)*torch.log(2*np.pi*(learnedSigma**2)) + (1/(2*(learnedSigma**2)))*torch.sum((X_norm - learnedMu)**2))\n",
    "for myIter in range(1000):\n",
    "    MLE.backward()\n",
    "    # Now we run over the data multiple times, and each time we calculate how far away we are from\n",
    "    # the maximized MLE and update our parameters accordingly\n",
    "    if myIter % 100 == 0:\n",
    "        print(\"log likelihood: {}, learned mu = {}, learned sigma {}\".format(MLE.data.numpy(),\n",
    "                                                                          learnedMu.data.numpy(),\n",
    "                                                                          learnedSigma.data.numpy()))\n",
    "    # SGD update for our learned parameters\n",
    "    learnedMu.data = learnedMu.data + learnedMu.grad.data*learningRate\n",
    "    learnedSigma.data = learnedSigma.data + learnedSigma.grad.data*learningRate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE for normally distributed data: Version 2\n",
    "#### What went wrong?\n",
    "PyTorch creates a dynamic computational and each time the `backward()` function is called it needs to be recreated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Mu: tensor([0.9468], requires_grad=True)\n",
      "Initial Sigma: tensor([0.9989], requires_grad=True)\n",
      "log likelihood: [-10846.087], learned mu = [0.9468204], learned sigma [0.9989336]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-90ff5d757475>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mMLE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearnedSigma\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearnedSigma\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_norm\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlearnedMu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmyIter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mMLE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Now we run over the data multiple times, and each time we calculate how far away we are from\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# the maximized MLE and update our parameters accordingly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Computational_methods/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Computational_methods/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "# Taking our randomly generated data and wrapping it in a Variable call\n",
    "# When we set requires_grad=False this means that we don't want to update these parameters\n",
    "# and don't need to compute gradients with respect to these Variables. .\n",
    "x = torch.autograd.Variable(X_norm, requires_grad=False)\n",
    "\n",
    "\n",
    "# Instantiate the parameters to learn. Notice how requires_grad is set to true. This is because we want\n",
    "# to update this parameter with the the MLE.\n",
    "learnedMu = torch.rand(1, requires_grad = True)\n",
    "learnedSigma = torch.rand(1, requires_grad = True)\n",
    "print(\"Initial Mu: {}\".format(learnedMu))\n",
    "print(\"Initial Sigma: {}\".format(learnedSigma))\n",
    "\n",
    "\n",
    "# This says how big of a step should we take each time we update our parameters.\n",
    "# there is some instability in the updates, so you might have to run a couple of times to get\n",
    "# values that look like what they should be.\n",
    "learningRate = 0.000001\n",
    "# learnedProb.zero_()\n",
    "n = X_norm.shape[0]\n",
    "#Instantiate the MLE for a normal distribution here using torch functions.\n",
    "MLE = -((n/2)*torch.log(2*np.pi*(learnedSigma**2)) + (1/(2*(learnedSigma**2)))*torch.sum((X_norm - learnedMu)**2))\n",
    "for myIter in range(1000):\n",
    "    MLE.backward()\n",
    "    # Now we run over the data multiple times, and each time we calculate how far away we are from\n",
    "    # the maximized MLE and update our parameters accordingly\n",
    "    if myIter % 100 == 0:\n",
    "        print(\"log likelihood: {}, learned mu = {}, learned sigma {}\".format(MLE.data.numpy(),\n",
    "                                                                          learnedMu.data.numpy(),\n",
    "                                                                          learnedSigma.data.numpy()))\n",
    "    # SGD update for our learned parameters\n",
    "    learnedMu.data = learnedMu.data + learnedMu.grad.data*learningRate\n",
    "    learnedSigma.data = learnedSigma.data + learnedSigma.grad.data*learningRate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE for normally distributed data: Version 3\n",
    "#### What went wrong?\n",
    "This is a bit tricky, but the gradient is not zeroed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Mu: tensor([0.6567], requires_grad=True)\n",
      "Initial Sigma: tensor([0.1744], requires_grad=True)\n",
      "log likelihood: [-112328.336], learned mu = [0.65667707], learned sigma [0.17439967]\n",
      "log likelihood: [-58064.9], learned mu = [28.158922], learned sigma [129.86131]\n",
      "log likelihood: [-64957.516], learned mu = [55.463387], learned sigma [258.53287]\n",
      "log likelihood: [-68989.195], learned mu = [82.68352], learned sigma [386.81738]\n",
      "log likelihood: [-71849.61], learned mu = [109.848366], learned sigma [514.85004]\n",
      "log likelihood: [-74068.266], learned mu = [136.97191], learned sigma [642.695]\n",
      "log likelihood: [-75881.016], learned mu = [164.06245], learned sigma [770.3904]\n",
      "log likelihood: [-77413.64], learned mu = [191.12547], learned sigma [897.9612]\n",
      "log likelihood: [-78741.26], learned mu = [218.16492], learned sigma [1025.4254]\n",
      "log likelihood: [-79912.27], learned mu = [245.18372], learned sigma [1152.7961]\n"
     ]
    }
   ],
   "source": [
    "# Taking our randomly generated data and wrapping it in a Variable call\n",
    "# When we set requires_grad=False this means that we don't want to update these parameters\n",
    "# and don't need to compute gradients with respect to these Variables. .\n",
    "x = torch.autograd.Variable(X_norm, requires_grad=False)\n",
    "\n",
    "\n",
    "# Instantiate the parameters to learn. Notice how requires_grad is set to true. This is because we want\n",
    "# to update this parameter with the the MLE.\n",
    "learnedMu = torch.rand(1, requires_grad = True)\n",
    "learnedSigma = torch.rand(1, requires_grad = True)\n",
    "print(\"Initial Mu: {}\".format(learnedMu))\n",
    "print(\"Initial Sigma: {}\".format(learnedSigma))\n",
    "\n",
    "\n",
    "# This says how big of a step should we take each time we update our parameters.\n",
    "# there is some instability in the updates, so you might have to run a couple of times to get\n",
    "# values that look like what they should be.\n",
    "learningRate = 0.000001\n",
    "# learnedProb.zero_()\n",
    "n = X_norm.shape[0]\n",
    "for myIter in range(1000):\n",
    "    #Instantiate the MLE for a normal distribution here using torch functions.\n",
    "    MLE = -((n/2)*torch.log(2*np.pi*(learnedSigma**2)) + (1/(2*(learnedSigma**2)))*torch.sum((X_norm - learnedMu)**2))\n",
    "    MLE.backward()\n",
    "    # Now we run over the data multiple times, and each time we calculate how far away we are from\n",
    "    # the maximized MLE and update our parameters accordingly\n",
    "    if myIter % 100 == 0:\n",
    "        print(\"log likelihood: {}, learned mu = {}, learned sigma {}\".format(MLE.data.numpy(),\n",
    "                                                                          learnedMu.data.numpy(),\n",
    "                                                                          learnedSigma.data.numpy()))\n",
    "    # SGD update for our learned parameters\n",
    "    learnedMu.data = learnedMu.data + learnedMu.grad.data*learningRate\n",
    "    learnedSigma.data = learnedSigma.data + learnedSigma.grad.data*learningRate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE for normally distributed data: Version 4\n",
    "Note that this should work. But if it doesn't run it a coupel of times. The optimization can be a bit unstable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Mu: tensor([0.2723], requires_grad=True)\n",
      "Initial Sigma: tensor([0.8703], requires_grad=True)\n",
      "log likelihood: [-17860.031], learned mu = [0.27234834], learned sigma [0.87027013]\n",
      "log likelihood: [-4868.256], learned mu = [1.2235541], learned sigma [0.5400847]\n",
      "log likelihood: [2583.022], learned mu = [1.4925636], learned sigma [0.18688895]\n",
      "log likelihood: [2583.022], learned mu = [1.4925636], learned sigma [0.18688895]\n",
      "log likelihood: [2583.022], learned mu = [1.4925636], learned sigma [0.18688895]\n",
      "log likelihood: [2583.022], learned mu = [1.4925636], learned sigma [0.18688895]\n",
      "log likelihood: [2583.022], learned mu = [1.4925636], learned sigma [0.18688895]\n",
      "log likelihood: [2583.022], learned mu = [1.4925636], learned sigma [0.18688895]\n",
      "log likelihood: [2583.022], learned mu = [1.4925636], learned sigma [0.18688895]\n",
      "log likelihood: [2583.022], learned mu = [1.4925636], learned sigma [0.18688895]\n"
     ]
    }
   ],
   "source": [
    "# Taking our randomly generated data and wrapping it in a Variable call\n",
    "# When we set requires_grad=False this means that we don't want to update these parameters\n",
    "# and don't need to compute gradients with respect to these Variables. .\n",
    "x = torch.autograd.Variable(X_norm, requires_grad=False)\n",
    "\n",
    "\n",
    "# Instantiate the parameters to learn. Notice how requires_grad is set to true. This is because we want\n",
    "# to update this parameter with the the MLE.\n",
    "learnedMu = torch.rand(1, requires_grad = True)\n",
    "learnedSigma = torch.rand(1, requires_grad = True)\n",
    "print(\"Initial Mu: {}\".format(learnedMu))\n",
    "print(\"Initial Sigma: {}\".format(learnedSigma))\n",
    "\n",
    "\n",
    "# This says how big of a step should we take each time we update our parameters.\n",
    "# there is some instability in the updates, so you might have to run a couple of times to get\n",
    "# values that look like what they should be.\n",
    "learningRate = 0.000001\n",
    "# learnedProb.zero_()\n",
    "n = X_norm.shape[0]\n",
    "for myIter in range(1000):\n",
    "    #Instantiate the MLE for a normal distribution here using torch functions.\n",
    "    MLE = -((n/2)*torch.log(2*np.pi*(learnedSigma**2)) + (1/(2*(learnedSigma**2)))*torch.sum((X_norm - learnedMu)**2))\n",
    "    MLE.backward()\n",
    "    # Now we run over the data multiple times, and each time we calculate how far away we are from\n",
    "    # the maximized MLE and update our parameters accordingly\n",
    "    if myIter % 100 == 0:\n",
    "        print(\"log likelihood: {}, learned mu = {}, learned sigma {}\".format(MLE.data.numpy(),\n",
    "                                                                          learnedMu.data.numpy(),\n",
    "                                                                          learnedSigma.data.numpy()))\n",
    "    # SGD update for our learned parameters\n",
    "    learnedMu.data = learnedMu.data + learnedMu.grad.data*learningRate\n",
    "    learnedSigma.data = learnedSigma.data + learnedSigma.grad.data*learningRate\n",
    "    # Clear out the gradient information\n",
    "    learnedMu.grad.data.zero_()\n",
    "    learnedSigma.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE: Bernoulli\n",
    "Finally, using the binary data we generated using a Bernoulli distribution let's learn the true value for $p$. Similarly to the above formulation you'll need to create a parameter to learn, as well as define the MLE for our binary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood: -5467.62939453125, learned prob = [0.2969513], update = [-3417.6665]\n",
      "log likelihood: -5339.048828125, learned prob = [0.22560014], update = [0.00390625]\n",
      "log likelihood: -5339.048828125, learned prob = [0.22560014], update = [0.00390625]\n",
      "log likelihood: -5339.048828125, learned prob = [0.22560014], update = [0.00390625]\n",
      "log likelihood: -5339.048828125, learned prob = [0.22560014], update = [0.00390625]\n",
      "log likelihood: -5339.048828125, learned prob = [0.22560014], update = [0.00390625]\n",
      "log likelihood: -5339.048828125, learned prob = [0.22560014], update = [0.00390625]\n",
      "log likelihood: -5339.048828125, learned prob = [0.22560014], update = [0.00390625]\n",
      "log likelihood: -5339.048828125, learned prob = [0.22560014], update = [0.00390625]\n",
      "log likelihood: -5339.048828125, learned prob = [0.22560014], update = [0.00390625]\n"
     ]
    }
   ],
   "source": [
    "# Define the variable p variable to learn\n",
    "#### your code here ####\n",
    "learnedP = torch.rand(1, requires_grad = True)\n",
    "#### end code here ####\n",
    "# Once again, running this can be a little temperamental, so if you're not getting the right answer\n",
    "# run it a few more times just ot be sure\n",
    "learningRate = 0.00001\n",
    "for myIter in range(1000):\n",
    "    # define the MLE for iid data from a Bernoulli\n",
    "    #### your code here ####\n",
    "    MLE = torch.sum(torch.log(X_bin*learnedP + (1-X_bin)*(1-learnedP)))\n",
    "    #### end code here ####\n",
    "    MLE.backward()\n",
    "    if myIter % 100 == 0:\n",
    "        print(\"log likelihood: {}, learned prob = {}, update = {}\".format(MLE.data.numpy(),\n",
    "                                                                          learnedP.data.numpy(),\n",
    "                                                                          learnedP.grad.data.numpy()))\n",
    "    # Update your learned parameter with a gradient descent step (similar to the normal case)\n",
    "    #### your code here ####\n",
    "    learnedP.data = learnedP.data + learnedP.grad.data*learningRate\n",
    "    #### end code here ####\n",
    "    learnedP.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Computational Methods)",
   "language": "python",
   "name": "computational_methods"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
