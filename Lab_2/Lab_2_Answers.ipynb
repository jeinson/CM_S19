{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Random sampling (torch.randn, torch.rand, torch.distributions module)\n",
    "# 2) Demonstration of central limit theorem through simulation\n",
    "# 3) Autodiff and computational graph construction (grad fields for tensors, backward method (and how it discards the implicitly created computational graph), optimizers, loss functions)\n",
    "# 4) Application of autodiff to a very simple problem (like fitting a normal and/or a beta distribution through maximum likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Generating some data\n",
    "In order to do this lab you're gonna need some data. Usually I wouldn't suggest doing this but why don't we just make up our data. Using PyTorch's __[built in distribution sampling functions](https://pytorch.org/docs/stable/distributions.html)__ create a dataset sampled from a Bernoulli, and another dataset sampled from a normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from a normal distribution\n",
    "First, draw a sample from a normal distribution using the parameters below. Story your data in the variable named `X_norm`, and print out the sample mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inaccurate sample mean: 1.5139328241348267, sample std: 0.1719292849302292\n"
     ]
    }
   ],
   "source": [
    "trueMu = 1.491# mean\n",
    "trueSig = 0.1876# standard deviation\n",
    "nNormal = 10\n",
    "#### your code here ####\n",
    "normalDist = torch.distributions.normal.Normal(loc = trueMu, scale = trueSig)\n",
    "X_norm = torch.autograd.Variable(normalDist.sample(sample_shape = [nNormal,]))\n",
    "print(\"Inaccurate sample mean: {}, sample std: {}\".format(X_norm.mean(), np.std(torch.Tensor.numpy(X_norm))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How close are the sample estimates of the mean and standard deviation to the true values? How can you improve the sample estimation of these values? What is the theorem that backs up your answer? Please describe how you might do that below and then implement your solution. Continue to store your data in the variable `X_norm`.\n",
    "\n",
    "Note, we care more about you explaining that you know what you're doing, rather than how close you can get your estimates. \n",
    "### 1) Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More accurate sample mean: 1.4918864965438843, sample std: 0.18744029104709625\n"
     ]
    }
   ],
   "source": [
    "#### your code here ####\n",
    "nNormal = 10000\n",
    "X_norm = torch.autograd.Variable(normalDist.sample(sample_shape = [nNormal,]))\n",
    "print(\"More accurate sample mean: {}, sample std: {}\".format(X_norm.mean(), np.std(torch.Tensor.numpy(X_norm))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary data generation\n",
    "Follow the same steps as above, but instead of a normal distribution we want binary data. What distribution can you use to sample binary data? Please ensure that the sample estimate probability of seeing a value of `1` is close to `trueP` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample estimate P(1): 0.23499999940395355\n"
     ]
    }
   ],
   "source": [
    "trueP = 0.23\n",
    "nBinary = 10000\n",
    "#### your code here ####\n",
    "bern = torch.distributions.bernoulli.Bernoulli(torch.tensor([trueP]))\n",
    "# X_bin = torch.autograd.Variable(bern.sample(sample_shape = [100,]))\n",
    "X_bin = bern.sample(sample_shape = [nBinary,])\n",
    "print(\"Sample estimate P(1): {}\".format(X_bin.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Maximum Likelihood Expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE: Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5881], requires_grad=True)\n",
      "tensor([0.7432], requires_grad=True)\n",
      "log likelihood: [13933.847], learned mu = [0.588112], learned sigma [0.7431544]\n",
      "log likelihood: [-2553.5645], learned mu = [1.49186], learned sigma [0.1874403]\n",
      "log likelihood: [-2553.5645], learned mu = [1.4918864], learned sigma [0.18744029]\n",
      "log likelihood: [-2553.5645], learned mu = [1.4918864], learned sigma [0.18744029]\n",
      "log likelihood: [-2553.5645], learned mu = [1.4918864], learned sigma [0.18744029]\n",
      "log likelihood: [-2553.5645], learned mu = [1.4918864], learned sigma [0.18744029]\n",
      "log likelihood: [-2553.5645], learned mu = [1.4918864], learned sigma [0.18744029]\n",
      "log likelihood: [-2553.5645], learned mu = [1.4918864], learned sigma [0.18744029]\n",
      "log likelihood: [-2553.5645], learned mu = [1.4918864], learned sigma [0.18744029]\n",
      "log likelihood: [-2553.5645], learned mu = [1.4918864], learned sigma [0.18744029]\n"
     ]
    }
   ],
   "source": [
    "learnedMu = torch.autograd.Variable(torch.rand(1), requires_grad = True)\n",
    "learnedSigma = torch.autograd.Variable(torch.rand(1), requires_grad = True)\n",
    "print(learnedMu)\n",
    "print(learnedSigma)\n",
    "\n",
    "\n",
    "learningRate = 0.000001\n",
    "# learnedProb.zero_()\n",
    "n = X_norm.shape[0]\n",
    "for myIter in range(1000):\n",
    "    lossFunc = ((n/2)*torch.log(2*np.pi*(learnedSigma**2)) + (1/(2*(learnedSigma**2)))*torch.sum((X_norm - learnedMu)**2))\n",
    "    lossFunc.backward()\n",
    "    if myIter % 100 == 0:\n",
    "        print(\"log likelihood: {}, learned mu = {}, learned sigma {}\".format(lossFunc.data.numpy(),\n",
    "                                                                          learnedMu.data.numpy(),\n",
    "                                                                          learnedSigma.data.numpy()))\n",
    "    learnedMu.data = learnedMu.data - learnedMu.grad.data*learningRate\n",
    "    learnedSigma.data = learnedSigma.data - learnedSigma.grad.data*learningRate\n",
    "    learnedMu.grad.data.zero_()\n",
    "    learnedSigma.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE: Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learnedP = torch.autograd.Variable(torch.rand(1), requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood: 15082.716796875, learned prob = [0.85383964], update = [49587.496]\n",
      "log likelihood: 5452.47607421875, learned prob = [0.23499978], update = [0.]\n",
      "log likelihood: 5452.47607421875, learned prob = [0.23499978], update = [0.]\n",
      "log likelihood: 5452.47607421875, learned prob = [0.23499978], update = [0.]\n",
      "log likelihood: 5452.47607421875, learned prob = [0.23499978], update = [0.]\n",
      "log likelihood: 5452.47607421875, learned prob = [0.23499978], update = [0.]\n",
      "log likelihood: 5452.47607421875, learned prob = [0.23499978], update = [0.]\n",
      "log likelihood: 5452.47607421875, learned prob = [0.23499978], update = [0.]\n",
      "log likelihood: 5452.47607421875, learned prob = [0.23499978], update = [0.]\n",
      "log likelihood: 5452.47607421875, learned prob = [0.23499978], update = [0.]\n"
     ]
    }
   ],
   "source": [
    "learningRate = 0.00001\n",
    "# learnedProb.zero_()\n",
    "for myIter in range(1000):\n",
    "    lossFunc = -torch.sum(torch.log(X_bin*learnedP + (1-X_bin)*(1-learnedP)))\n",
    "    lossFunc.backward()\n",
    "    if myIter % 100 == 0:\n",
    "        print(\"log likelihood: {}, learned prob = {}, update = {}\".format(lossFunc.data.numpy(),\n",
    "                                                                          learnedP.data.numpy(),\n",
    "                                                                          learnedP.grad.data.numpy()))\n",
    "    learnedP.data = learnedP.data - learnedP.grad.data*learningRate\n",
    "    learnedP.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Computational Methods)",
   "language": "python",
   "name": "computational_methods"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
