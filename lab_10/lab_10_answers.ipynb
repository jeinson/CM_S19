{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lab 10: Word Embeddings\n",
    "Thinking of using stuff from here\n",
    "https://gist.github.com/mbednarski/da08eb297304f7a66a3840e857e060a0\n",
    "\n",
    "conda install -c conda-forge tqdm\n",
    "\n",
    "conda install -c conda-forge ipywidgets\n",
    "conda install -c anaconda nltk \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Janitorial Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCorpus = [\"First of all, quit grinnin’ like an idiot. Indians ain’t supposed to smile like that. Get stoic.\",\n",
    "             \"No. Like this. You gotta look mean, or people won’t respect you.\",\n",
    "              \" people will run all over you if you don’t look mean.\",\n",
    "              \"You gotta look like a warrior. You gotta look like you just came back from killing a buffalo.\",\n",
    "             \"But our tribe never hunted buffalo. We were fishermen.\"\n",
    "             \"What? You wanna look like you just came back from catching a fish?\",\n",
    "             \"This ain’t dances with salmon, you know. Thomas, you gotta look like a warrior.\"]\n",
    "maxDocs = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ob2285/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500 pub med abstracts\n"
     ]
    }
   ],
   "source": [
    "# Read in pubmed corpus into a text file\n",
    "\n",
    "import glob\n",
    "pubMedDataFolderPath = \"data/pubMed_corpus/\"\n",
    "pubMedDataFiles = glob.glob(pubMedDataFolderPath + \"*.txt\")\n",
    "pubMedCorpus = [\"\"]*len(pubMedDataFiles)\n",
    "for idx, pubMedDataPath in enumerate(pubMedDataFiles):\n",
    "    with open(pubMedDataPath, \"r\") as pubMedFile:\n",
    "        text = pubMedFile.read().strip()\n",
    "        pubMedCorpus[idx] = text\n",
    "pubMedCorpus = pubMedCorpus[0:maxDocs]\n",
    "print(\"{} pub med abstracts\".format(len(pubMedCorpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500 ap articles\n"
     ]
    }
   ],
   "source": [
    "# Read in the ap corpus\n",
    "apTextFile = \"data/ap.txt\"\n",
    "apCorpus = []\n",
    "readText = False\n",
    "with open(apTextFile) as apDataFile:\n",
    "    for line in apDataFile:\n",
    "        if readText:\n",
    "            apCorpus.append(line.strip())\n",
    "            readText = False\n",
    "        if line == \"<TEXT>\\n\":\n",
    "            readText = True\n",
    "apCorpus = apCorpus[0:maxDocs]\n",
    "print(\"{} ap articles\".format(len(apCorpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ob2285/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ob2285/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "def removePunctuation(myStr):\n",
    "    excludedCharacters = string.punctuation + \"’\"\n",
    "    newStr = \"\".join(char for char in myStr if char not in excludedCharacters)\n",
    "    return(newStr)\n",
    "def removeStopWords(tokenList):\n",
    "    newTokenList = [tok for tok in tokenList if tok not in stopwords.words('english')]\n",
    "    return(newTokenList)\n",
    "def cleanDocStr(docStr):\n",
    "    docStr = docStr.lower()\n",
    "    docStr = removePunctuation(docStr)\n",
    "    docStr = re.sub('\\d', '%d%', docStr)\n",
    "    docStrTokenized = nltk.tokenize.word_tokenize(docStr)\n",
    "    myStopWords = set(stopwords.words('english'))\n",
    "    docStrTokenized = [tok for tok in docStrTokenized if tok not in myStopWords]\n",
    "    return(docStrTokenized)\n",
    "def tokenize_corpus(corpus):\n",
    "    tokens = [cleanDocStr(x) for x in corpus]\n",
    "    return tokens\n",
    "\n",
    "apCorpusTokenized = tokenize_corpus(apCorpus)\n",
    "pubMedCorpusTokenized = tokenize_corpus(pubMedCorpus)\n",
    "testCorpusTokenized = tokenize_corpus(testCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building ap corpus vocabulary\n",
      "Vocab size: 8330\n",
      "ap data tokenized in 0.16865897178649902 seconds\n",
      "\n",
      "Building pubMed corpus vocabulary\n",
      "Vocab size: 4527\n",
      "pubmed data tokenized in 0.07795190811157227 seconds\n",
      "\n",
      "Building test corpus vocabulary\n",
      "Vocab size: 38\n",
      "test data tokenized in 0.00021338462829589844 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from collections import Counter\n",
    "\n",
    "minVocabOccurence = 5\n",
    "\n",
    "def extractVocabMappers(tokenizedCorpus, vocabSizeMax = None, minVocabOccurence = 0):\n",
    "    UNK = \"<UNK>\"\n",
    "    flattenedCorpus = [item for sublist in tokenizedCorpus for item in sublist]\n",
    "    wordCounts = Counter(flattenedCorpus).most_common()\n",
    "    wordCounts = [(w, c) for w,c in wordCounts if c > minVocabOccurence]\n",
    "#     wordCounts = wordCounts.most_common(vocabSizeMax)\n",
    "    vocabulary = [word for word, count in wordCounts]\n",
    "    \n",
    "    # below is more readable but significantly slower code\n",
    "    if False:\n",
    "        vocabulary = []\n",
    "        for sentence in tqdm(tokenizedCorpus):\n",
    "            for token in sentence:\n",
    "                if token not in vocabulary:\n",
    "                    vocabulary.append(token)\n",
    "    vocabulary.append(UNK)\n",
    "    print(\"Vocab size: {}\".format(len(vocabulary)))\n",
    "    word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "    idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "    newTokenizedCorpus = []# all words missing from vocab replaced with <UNK>\n",
    "    for doc in tokenizedCorpus:\n",
    "        newDoc = [word if word in word2idx else UNK for word in doc]\n",
    "        newTokenizedCorpus.append(newDoc)\n",
    "    return(word2idx, idx2word, wordCounts, newTokenizedCorpus)\n",
    "\n",
    "start = time.time()\n",
    "print(\"Building ap corpus vocabulary\")\n",
    "word2Idx_ap, idx2Word_ap, vocabCount_ap, finalTokenizedCorpus_ap = extractVocabMappers(apCorpusTokenized,\n",
    "#                                                                                        vocabSizeMax = maxVocabSize,\n",
    "                                                                                      minVocabOccurence = minVocabOccurence)\n",
    "print(\"ap data tokenized in {} seconds\\n\".format(time.time() - start))\n",
    "start = time.time()\n",
    "print(\"Building pubMed corpus vocabulary\")\n",
    "word2Idx_pubMed, idx2Word_pubMed, vocabCount_pubMed, finalTokenizedCorpus_pubMed = extractVocabMappers(pubMedCorpusTokenized,\n",
    "#                                                                                                        vocabSizeMax = maxVocabSize,\n",
    "                                                                                                      minVocabOccurence = minVocabOccurence)\n",
    "print(\"pubmed data tokenized in {} seconds\\n\".format(time.time() - start))\n",
    "start = time.time()\n",
    "print(\"Building test corpus vocabulary\")\n",
    "word2Idx_test, idx2Word_test, vocabCount_test, finalTokenizedCorpus_test = extractVocabMappers(testCorpusTokenized,\n",
    "#                                                                                                vocabSizeMax = maxVocabSize,\n",
    "                                                                                              minVocabOccurence = 0)\n",
    "print(\"test data tokenized in {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### BATCH VERSION ######\n",
    "\n",
    "\n",
    "def generateObservations(tokenizedCorpus, word2Idx):\n",
    "    window_size = 3\n",
    "    idxPairs = []\n",
    "    for sentence in tokenizedCorpus:\n",
    "        for center_word_pos in range(len(sentence)):\n",
    "            # for each window position\n",
    "            for w in range(-window_size, window_size + 1):\n",
    "                context_word_pos = center_word_pos + w\n",
    "                # make sure not jump out sentence\n",
    "                if context_word_pos < 0 or context_word_pos >= len(sentence) or center_word_pos == context_word_pos:\n",
    "                    continue\n",
    "                idxPairs.append((sentence[center_word_pos], sentence[context_word_pos]))\n",
    "\n",
    "    idxPairs = np.array(idxPairs)\n",
    "    return(idxPairs)\n",
    "\n",
    "\n",
    "def generateWordSamplingProb(vocabCount, word2Idx):\n",
    "    wordSampleProbs = [0.0]*len(vocabCount)\n",
    "    numWords = np.sum([count**0.75 for word, count in vocabCount])\n",
    "    for idx in range(len(vocabCount)):\n",
    "        w,c = vocabCount[idx]\n",
    "        wordSampleProbs[word2Idx[w]] = (c**0.75)/(numWords)\n",
    "        \n",
    "        \n",
    "        \n",
    "    wordSampleProbs = []\n",
    "    numWords = np.sum([count for word, count in vocabCount])\n",
    "    for w,c in vocabCount:\n",
    "        wordSampleProbs.extend([word2Idx[w]] * int(((c/numWords)**0.75)/0.001))\n",
    "    return(wordSampleProbs)\n",
    "    \n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocabSize, embedSize, vocabCount, word2Idx):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.vocabSize = vocabSize\n",
    "        self.word2Idx = word2Idx\n",
    "        self.centerEmbeddings = nn.Parameter(torch.randn(vocabSize,\n",
    "                                                     embedSize).float(), requires_grad=True)\n",
    "        self.contextEmbeddings = nn.Parameter(torch.randn(vocabSize,\n",
    "                                                      embedSize).float(), requires_grad=True)\n",
    "        \n",
    "#         initrange = (2.0 / (vocabSize + embedSize)) ** 0.5  # Xavier init\n",
    "        nn.init.xavier_uniform_(self.contextEmbeddings)\n",
    "#         self.output_emb.weight.data.uniform_(-0, 0)\n",
    "        nn.init.uniform_(self.centerEmbeddings, -0,0)\n",
    "        \n",
    "        self.wordSampleProbs = generateWordSamplingProb(vocabCount, word2Idx)\n",
    "        self.logSigmoid = nn.LogSigmoid()\n",
    "    def getNegSample(self, k, centerWords):\n",
    "        vocabSizeWithoutUnk = self.vocabSize - 1\n",
    "        negSamples = []\n",
    "        for centerWord in centerWords:\n",
    "            negSample = random.sample(self.wordSampleProbs, k)\n",
    "            while self.word2Idx[centerWord] in negSample:\n",
    "                negSample = random.sample(self.wordSampleProbs, k)\n",
    "            negSamples.append(negSample)\n",
    "        return(negSamples)\n",
    "    def forward(self, center, context, negSampleIndices):\n",
    "        embedCenter = self.centerEmbeddings[center]#.view((1, -1))\n",
    "        embedContext = self.contextEmbeddings[context]#.view((1, -1))       \n",
    "        posVal = self.logSigmoid(torch.sum(embedContext * embedCenter, dim = 1)).squeeze()\n",
    "        negSampleIndices = torch.autograd.Variable(torch.LongTensor(negSampleIndices))\n",
    "        negVal = torch.bmm(self.contextEmbeddings[negSampleIndices], embedCenter.unsqueeze(2)).squeeze(2)\n",
    "        negVal = self.logSigmoid(-torch.sum(negVal, dim = 1)).squeeze()\n",
    "        logProb = -(posVal + negVal).mean()\n",
    "        return(logProb)\n",
    "\n",
    "\n",
    "def train_skipgram(embeddingSize, trainingData, vocabCount, word2Idx, idx2Word, k, referenceWords):\n",
    "    print(\"training on {} observations\".format(len(trainingData)))\n",
    "    losses = []\n",
    "    loss_fn = nn.MSELoss()\n",
    "    model = SkipGram(vocabSize = len(word2Idx), embedSize = embeddingSize,\n",
    "                     vocabCount = vocabCount, word2Idx = word2Idx)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    listNearestWords(model = model, idx2Word = idx2Word,\n",
    "     referenceWords = referenceWords, topN = 5)\n",
    "    batchSize = 1024\n",
    "    #         listNearestWords(model = model, idx2Word = idx2Word,\n",
    "#                  referenceWords = referenceWords, topN = 5)\n",
    "    for epoch in tqdm_notebook(range(n_epoch), position = 0):\n",
    "        total_loss = .0\n",
    "        avgLoss = 0.0\n",
    "        iteration = 0\n",
    "        for step in tqdm_notebook(range(0, len(trainingData), batchSize), position = 1):\n",
    "            endIdx = np.min([(step+batchSize), len(trainingData)])\n",
    "            myBatch = trainingData[step:(step+batchSize)]\n",
    "            centerWords = [elem[0] for elem in myBatch]\n",
    "            contextWords = [elem[1] for elem in myBatch]\n",
    "            negSamples = model.getNegSample(k = k, centerWords = centerWords)\n",
    "            centerIDs = [word2Idx[idx] for idx in centerWords]\n",
    "            contextIDs = [word2Idx[idx] for idx in contextWords]\n",
    "            model.zero_grad()\n",
    "            loss = model(centerIDs, contextIDs, negSampleIndices = negSamples)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.data.numpy()\n",
    "            avgLoss += loss.data.numpy()\n",
    "            iteration += 1\n",
    "            if iteration % 500 == 0:\n",
    "                avgLoss = avgLoss/(500)\n",
    "                print(\"avg loss: {}\".format(avgLoss))\n",
    "#                 avgLoss = 0.0\n",
    "#             if iteration % 2000 == 0:\n",
    "#                 listNearestWords(model = model, idx2Word = idx2Word,\n",
    "#                  referenceWords = referenceWords, topN = 5)\n",
    "        losses.append(total_loss)    \n",
    "        print(\"Loss at epoch {}: {}\".format(epoch, total_loss/iteration))\n",
    "        if epoch % 1 == 0 and epoch != 0:\n",
    "            listNearestWords(model = model, idx2Word = idx2Word,\n",
    "                         referenceWords = referenceWords, topN = 5)\n",
    "    return(model, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "def listNearestWords(model, idx2Word, referenceWords, topN):\n",
    "    assert len(idx2Word) == len(model.word2Idx), \"Possibly passed in two different vocabularies\"\n",
    "    embeddings = model.centerEmbeddings.data.numpy()\n",
    "    distMat = cdist(embeddings, embeddings, metric = \"cosine\")\n",
    "    for word in referenceWords:\n",
    "        wordIdx = model.word2Idx[word]\n",
    "        closestIndices = np.argsort(distMat[wordIdx,:])[0:topN]\n",
    "        closestWords = [(idx2Word[idx], distMat[wordIdx, idx]) for idx in closestIndices]\n",
    "        for elem in closestWords:\n",
    "            print(elem)\n",
    "        print(\"*\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# embd_size = 100\n",
    "# learning_rate = 0.001\n",
    "# n_epoch = 60\n",
    "# idxPairsTest = generateObservations(tokenizedCorpus = finalTokenizedCorpus_test, word2Idx = word2Idx_test)\n",
    "# sg_model, sg_losses = train_skipgram(embeddingSize = 5, trainingData = idxPairsTest, vocabCount = vocabCount_test,\n",
    "#                                      word2Idx = word2Idx_test, k = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on 2640138 observations\n",
      "('%', nan)\n",
      "('coral', nan)\n",
      "('austerity', nan)\n",
      "('varied', nan)\n",
      "('hardware', nan)\n",
      "**************************************************\n",
      "\n",
      "('%', nan)\n",
      "('coral', nan)\n",
      "('austerity', nan)\n",
      "('varied', nan)\n",
      "('hardware', nan)\n",
      "**************************************************\n",
      "\n",
      "('%', nan)\n",
      "('coral', nan)\n",
      "('austerity', nan)\n",
      "('varied', nan)\n",
      "('hardware', nan)\n",
      "**************************************************\n",
      "\n",
      "('%', nan)\n",
      "('coral', nan)\n",
      "('austerity', nan)\n",
      "('varied', nan)\n",
      "('hardware', nan)\n",
      "**************************************************\n",
      "\n",
      "('%', nan)\n",
      "('coral', nan)\n",
      "('austerity', nan)\n",
      "('varied', nan)\n",
      "('hardware', nan)\n",
      "**************************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e3df28083764dfca0b400908bdc57e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=60), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac08f295fe19477fbc3b257b4844dd3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2579), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddingSize = 50\n",
    "learning_rate = 0.01\n",
    "n_epoch = 60\n",
    "idxPairsAP = generateObservations(tokenizedCorpus = finalTokenizedCorpus_ap, word2Idx = word2Idx_ap)\n",
    "sg_model_ap, sg_losses_ap = train_skipgram(embeddingSize = embeddingSize, trainingData = idxPairsAP,\n",
    "                                     vocabCount = vocabCount_ap,\n",
    "                                     word2Idx = word2Idx_ap, idx2Word = idx2Word_ap, k = 20,\n",
    "                                          referenceWords = [\"bush\", \"soviet\", \"president\", \"military\", \"american\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embeddingSize = 50\n",
    "learning_rate = 0.001\n",
    "n_epoch = 3\n",
    "idxPairsPubMed = generateObservations(tokenizedCorpus = finalTokenizedCorpus_pubMed, word2Idx = word2Idx_pubMed)\n",
    "sg_model_pubMed, sg_losses_pubMed = train_skipgram(embeddingSize = embeddingSize, trainingData = idxPairsPubMed,\n",
    "                                     vocabCount = vocabCount_pubMed,\n",
    "                                     word2Idx = word2Idx_pubMed, idx2Word = idx2Word_pubMed, k = 15,\n",
    "                                                  referenceWords = [\"clinical\", \"obesity\", \"microbial\", \"microbiome\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "def listNearestWords(model, idx2Word, referenceWords, topN):\n",
    "    assert len(idx2Word) == len(model.word2Idx), \"Possibly passed in two different vocabularies\"\n",
    "    embeddings = model.centerEmbeddings.data.numpy()\n",
    "    distMat = cdist(embeddings, embeddings, metric = \"cosine\")\n",
    "    for word in referenceWords:\n",
    "        wordIdx = model.word2Idx[word]\n",
    "#         print(np.argmin(distMat[wordIdx,:]))\n",
    "        closestIndices = np.argsort(distMat[wordIdx,:])[0:topN]\n",
    "        closestWords = [(idx2Word[idx], distMat[wordIdx, idx]) for idx in closestIndices]\n",
    "        for elem in closestWords:\n",
    "            print(elem)\n",
    "        print(\"*\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# listNearestWords(model = sg_model_ap, idx2Word = idx2Word_ap,\n",
    "#                  referenceWords = [\"bush\", \"soviet\", \"stock\", \"dukakis\"], topN = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# listNearestWords(model = sg_model_pubMed, idx2Word = idx2Word_pubMed,\n",
    "#                  referenceWords = [\"cancer\", \"drug\", \"microbiome\"], topN = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Domains Affect Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg time: 5.7178974151611327e-05\n",
      "torch.Size([1000, 1])\n",
      "avg time: 2.7730941772460937e-05\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "w1 = nn.Parameter(torch.randn(1000, 100).float(), requires_grad=True)\n",
    "w2 = nn.Parameter(torch.randn(1000,  100).float(), requires_grad=True)\n",
    "nIters = 1000\n",
    "negSampleSize = 1000\n",
    "start = time.time()\n",
    "for i in range(1000):\n",
    "    temp = torch.mm(w2[0:negSampleSize], torch.t(w2[0].view(1, -1)))\n",
    "print(\"avg time: {}\".format((time.time() - start)/nIters))\n",
    "print(temp.shape)\n",
    "\n",
    "\n",
    "\n",
    "w1 = w1.data.numpy()\n",
    "w2 = w2.data.numpy()\n",
    "start = time.time()\n",
    "for i in range(1000):\n",
    "    temp = np.matmul(w2[0:negSampleSize], w1[0])\n",
    "print(\"avg time: {}\".format((time.time() - start)/nIters))\n",
    "print(temp.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg time: 1.8839120864868163e-05\n",
      "torch.Size([15, 1])\n"
     ]
    }
   ],
   "source": [
    "w1 = nn.Parameter(torch.randn(1000, 100).float(), requires_grad=True)\n",
    "w2 = nn.Parameter(torch.randn(1000,  100).float(), requires_grad=True)\n",
    "nIters = 1000\n",
    "negSampleSize = 15\n",
    "start = time.time()\n",
    "for i in range(1000):\n",
    "    temp = torch.mm(w2[0:negSampleSize], torch.t(w2[0].view(1, -1)))\n",
    "print(\"avg time: {}\".format((time.time() - start)/nIters))\n",
    "print(temp.shape)\n",
    "\n",
    "\n",
    "\n",
    "# w1 = w1.data.numpy()\n",
    "# w2 = w2.data.numpy()\n",
    "# start = time.time()\n",
    "# for i in range(1000):\n",
    "#     temp = np.matmul(w2[0:negSampleSize], w1[0])\n",
    "# print(\"avg time: {}\".format((time.time() - start)/nIters))\n",
    "# print(temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = w1.data.numpy()\n",
    "w2 = w2.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15,)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(w2[0:15], w1[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['first',\n",
       "  'quit',\n",
       "  'grinnin',\n",
       "  'like',\n",
       "  'idiot',\n",
       "  'indians',\n",
       "  'aint',\n",
       "  'supposed',\n",
       "  'smile',\n",
       "  'like',\n",
       "  'get',\n",
       "  'stoic'],\n",
       " ['like', 'gotta', 'look', 'mean', 'people', 'wont', 'respect'],\n",
       " ['people', 'run', 'dont', 'look', 'mean'],\n",
       " ['gotta',\n",
       "  'look',\n",
       "  'like',\n",
       "  'warrior',\n",
       "  'gotta',\n",
       "  'look',\n",
       "  'like',\n",
       "  'came',\n",
       "  'back',\n",
       "  'killing',\n",
       "  'buffalo'],\n",
       " ['tribe',\n",
       "  'never',\n",
       "  'hunted',\n",
       "  'buffalo',\n",
       "  'fishermenwhat',\n",
       "  'wanna',\n",
       "  'look',\n",
       "  'like',\n",
       "  'came',\n",
       "  'back',\n",
       "  'catching',\n",
       "  'fish'],\n",
       " ['aint',\n",
       "  'dances',\n",
       "  'salmon',\n",
       "  'know',\n",
       "  'thomas',\n",
       "  'gotta',\n",
       "  'look',\n",
       "  'like',\n",
       "  'warrior']]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalTokenizedCorpus_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Computational Methods)",
   "language": "python",
   "name": "compmeth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
