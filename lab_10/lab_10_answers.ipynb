{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lab 10: Word Embeddings\n",
    "Thinking of using stuff from here\n",
    "https://gist.github.com/mbednarski/da08eb297304f7a66a3840e857e060a0\n",
    "\n",
    "conda install -c conda-forge tqdm\n",
    "\n",
    "conda install -c conda-forge ipywidgets\n",
    "conda install -c anaconda nltk \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Janitorial Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCorpus = [\"First of all, quit grinnin’ like an idiot. Indians ain’t supposed to smile like that. Get stoic.\",\n",
    "             \"No. Like this. You gotta look mean, or people won’t respect you.\",\n",
    "              \" people will run all over you if you don’t look mean.\",\n",
    "              \"You gotta look like a warrior. You gotta look like you just came back from killing a buffalo.\",\n",
    "             \"But our tribe never hunted buffalo. We were fishermen.\"\n",
    "             \"What? You wanna look like you just came back from catching a fish?\",\n",
    "             \"This ain’t dances with salmon, you know. Thomas, you gotta look like a warrior.\"]\n",
    "maxDocs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ob2285/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 pub med abstracts\n"
     ]
    }
   ],
   "source": [
    "# Read in pubmed corpus into a text file\n",
    "\n",
    "import glob\n",
    "pubMedDataFolderPath = \"data/pubMed_corpus/\"\n",
    "pubMedDataFiles = glob.glob(pubMedDataFolderPath + \"*.txt\")\n",
    "pubMedCorpus = [\"\"]*len(pubMedDataFiles)\n",
    "for idx, pubMedDataPath in enumerate(pubMedDataFiles):\n",
    "    with open(pubMedDataPath, \"r\") as pubMedFile:\n",
    "        text = pubMedFile.read().strip()\n",
    "        pubMedCorpus[idx] = text\n",
    "pubMedCorpus = pubMedCorpus[0:maxDocs]\n",
    "print(\"{} pub med abstracts\".format(len(pubMedCorpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 ap articles\n"
     ]
    }
   ],
   "source": [
    "# Read in the ap corpus\n",
    "apTextFile = \"data/ap.txt\"\n",
    "apCorpus = []\n",
    "readText = False\n",
    "with open(apTextFile) as apDataFile:\n",
    "    for line in apDataFile:\n",
    "        if readText:\n",
    "            apCorpus.append(line.strip())\n",
    "            readText = False\n",
    "        if line == \"<TEXT>\\n\":\n",
    "            readText = True\n",
    "apCorpus = apCorpus[0:maxDocs]\n",
    "print(\"{} ap articles\".format(len(apCorpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ob2285/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "def removePunctuation(myStr):\n",
    "    excludedCharacters = string.punctuation + \"’\"\n",
    "    newStr = \"\".join(char for char in myStr if char not in excludedCharacters)\n",
    "    return(newStr)\n",
    "def removeStopWords(tokenList):\n",
    "    newTokenList = [tok for tok in tokenList if tok not in stopwords.words('english')]\n",
    "    return(newTokenList)\n",
    "def tokenize_corpus(corpus):\n",
    "    tokens = [removeStopWords(removePunctuation(x).lower().split()) for x in corpus]\n",
    "    return tokens\n",
    "\n",
    "apCorpusTokenized = tokenize_corpus(apCorpus)\n",
    "pubMedCorpusTokenized = tokenize_corpus(pubMedCorpus)\n",
    "testCorpusTokenized = tokenize_corpus(testCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building ap corpus vocabulary\n",
      "Vocab size: 4123\n",
      "ap data tokenized in 0.051291465759277344 seconds\n",
      "\n",
      "Building pubMed corpus vocabulary\n",
      "Vocab size: 2333\n",
      "pubmed data tokenized in 0.026688814163208008 seconds\n",
      "\n",
      "Building test corpus vocabulary\n",
      "Vocab size: 36\n",
      "test data tokenized in 0.0004177093505859375 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from collections import Counter\n",
    "\n",
    "minVocabOccurence = 5\n",
    "\n",
    "def extractVocabMappers(tokenizedCorpus, vocabSizeMax = None, minVocabOccurence = 0):\n",
    "    UNK = \"<UNK>\"\n",
    "    flattenedCorpus = [item for sublist in tokenizedCorpus for item in sublist]\n",
    "    wordCounts = Counter(flattenedCorpus).most_common()\n",
    "    wordCounts = [(w, c) for w,c in wordCounts if c > minVocabOccurence]\n",
    "#     wordCounts = wordCounts.most_common(vocabSizeMax)\n",
    "    vocabulary = [word for word, count in wordCounts]\n",
    "    \n",
    "    # below is more readable but significantly slower code\n",
    "    if False:\n",
    "        vocabulary = []\n",
    "        for sentence in tqdm(tokenizedCorpus):\n",
    "            for token in sentence:\n",
    "                if token not in vocabulary:\n",
    "                    vocabulary.append(token)\n",
    "    vocabulary.append(UNK)\n",
    "    print(\"Vocab size: {}\".format(len(vocabulary)))\n",
    "    word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "    idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "    newTokenizedCorpus = []# all words missing from vocab replaced with <UNK>\n",
    "    for doc in tokenizedCorpus:\n",
    "        newDoc = [word if word in word2idx else UNK for word in doc]\n",
    "        newTokenizedCorpus.append(newDoc)\n",
    "    return(word2idx, idx2word, wordCounts, newTokenizedCorpus)\n",
    "\n",
    "start = time.time()\n",
    "print(\"Building ap corpus vocabulary\")\n",
    "word2Idx_ap, idx2Word_ap, vocabCount_ap, finalTokenizedCorpus_ap = extractVocabMappers(apCorpusTokenized,\n",
    "#                                                                                        vocabSizeMax = maxVocabSize,\n",
    "                                                                                      minVocabOccurence = minVocabOccurence)\n",
    "print(\"ap data tokenized in {} seconds\\n\".format(time.time() - start))\n",
    "start = time.time()\n",
    "print(\"Building pubMed corpus vocabulary\")\n",
    "word2Idx_pubMed, idx2Word_pubMed, vocabCount_pubMed, finalTokenizedCorpus_pubMed = extractVocabMappers(pubMedCorpusTokenized,\n",
    "#                                                                                                        vocabSizeMax = maxVocabSize,\n",
    "                                                                                                      minVocabOccurence = minVocabOccurence)\n",
    "print(\"pubmed data tokenized in {} seconds\\n\".format(time.time() - start))\n",
    "start = time.time()\n",
    "print(\"Building test corpus vocabulary\")\n",
    "word2Idx_test, idx2Word_test, vocabCount_test, finalTokenizedCorpus_test = extractVocabMappers(testCorpusTokenized,\n",
    "#                                                                                                vocabSizeMax = maxVocabSize,\n",
    "                                                                                              minVocabOccurence = 0)\n",
    "print(\"test data tokenized in {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateObservations(tokenizedCorpus, word2Idx):\n",
    "    window_size = 2\n",
    "    idxPairs = []\n",
    "    # for each sentence\n",
    "    for sentence in tokenizedCorpus:\n",
    "#         indices = [word2Idx[word] for word in sentence]\n",
    "        # for each word, threated as center word\n",
    "        for center_word_pos in range(len(sentence)):\n",
    "            # for each window position\n",
    "            for w in range(-window_size, window_size + 1):\n",
    "                context_word_pos = center_word_pos + w\n",
    "                # make soure not jump out sentence\n",
    "                if context_word_pos < 0 or context_word_pos >= len(sentence) or center_word_pos == context_word_pos:\n",
    "                    continue\n",
    "                idxPairs.append((sentence[center_word_pos], sentence[context_word_pos]))\n",
    "\n",
    "    idxPairs = np.array(idxPairs) # it will be useful to have this as numpy array\n",
    "    return(idxPairs)\n",
    "\n",
    "\n",
    "def generateWordSamplingProb(vocabCount, word2Idx):\n",
    "    wordSampleProbs = [0.0]*len(vocabCount)\n",
    "    numWords = np.sum([count**0.75 for word, count in vocabCount])\n",
    "    for idx in range(len(vocabCount)):\n",
    "        w,c = vocabCount[idx]\n",
    "        wordSampleProbs[word2Idx[w]] = (c**0.75)/(numWords)\n",
    "        \n",
    "        \n",
    "        \n",
    "    wordSampleProbs = []\n",
    "    numWords = np.sum([count for word, count in vocabCount])\n",
    "    for w,c in vocabCount:\n",
    "#         w,c = vocabCount[idx]\n",
    "        wordSampleProbs.extend([word2Idx[w]] * int(((c/numWords)**0.75)/0.001))\n",
    "    return(wordSampleProbs)\n",
    "    \n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocabSize, embedSize, vocabCount, word2Idx):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.vocabSize = vocabSize\n",
    "        self.word2Idx = word2Idx\n",
    "#         self.centerEmbeddings = nn.Embedding(vocab_size, embd_size)\n",
    "#         self.contextEmbeddings = nn.Embedding(vocab_size, embd_size)\n",
    "#         self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
    "        \n",
    "        self.centerEmbeddings = nn.Parameter(torch.randn(vocabSize,\n",
    "                                                     embedSize).float(), requires_grad=True)\n",
    "        self.contextEmbeddings = nn.Parameter(torch.randn(vocabSize,\n",
    "                                                      embedSize).float(), requires_grad=True)\n",
    "        \n",
    "#         initrange = (2.0 / (vocabSize + embedSize)) ** 0.5  # Xavier init\n",
    "        nn.init.xavier_uniform_(self.contextEmbeddings)\n",
    "        nn.init.xavier_uniform_(self.centerEmbeddings)\n",
    "        \n",
    "        self.wordSampleProbs = generateWordSamplingProb(vocabCount, word2Idx)\n",
    "        self.logSigmoid = nn.LogSigmoid()\n",
    "#         self.paramList = nn.ModuleList([self.centerEmbeddings, self.contextEmbeddings] )\n",
    "    def getNegSample(self, k, centerWord):\n",
    "        vocabSizeWithoutUnk = self.vocabSize - 1\n",
    "#         negSample = np.random.choice(vocabSizeWithoutUnk,\n",
    "#                                      size = k, replace = True, p = self.wordSampleProbs)\n",
    "        negSample = random.sample(self.wordSampleProbs, k)\n",
    "        while self.word2Idx[centerWord] in negSample:\n",
    "#             negSample = np.random.choice(vocabSizeWithoutUnk,\n",
    "#                                          size = k, replace = True, p = self.wordSampleProbs)\n",
    "            negSample = random.sample(self.wordSampleProbs, k)\n",
    "        return(negSample)\n",
    "    def forward(self, center, context, negSampleIndices = None):\n",
    "#         focus = torch.autograd.Variable(torch.LongTensor([0]))\n",
    "#         context = torch.autograd.Variable(torch.LongTensor([0]))\n",
    "#         allEmbeddingIdxs = torch.autograd.Variable(torch.LongTensor([np.arange(0,self.vocabSize)]))\n",
    "\n",
    "\n",
    "#         embedCenter = self.centerEmbeddings(center).view((1, -1))\n",
    "#         embedContext = self.contextEmbeddings(context).view((1, -1))\n",
    "# #         print(allEmbeddingIdxs)\n",
    "#         allContextEmbeddings = self.contextEmbeddings(allEmbeddingIdxs).squeeze()\n",
    "#         num = torch.exp(torch.mm(embedContext, torch.t(embedCenter)))\n",
    "#         denom = torch.exp(torch.mm(allContextEmbeddings, torch.t(embedCenter))).sum()\n",
    "#         logProb = torch.log(num/denom)\n",
    "        embedCenter = self.centerEmbeddings[center].view((1, -1))\n",
    "        embedContext = self.contextEmbeddings[context].view((1, -1))       \n",
    "        if negSampleIndices is not None:\n",
    "#             print(\"hey\")\n",
    "#             posVal = self.logSigmoid(torch.mm(embedContext, torch.t(embedCenter)))\n",
    "#             print(posVal)\n",
    "            posVal = self.logSigmoid(torch.sum(embedContext * embedCenter))\n",
    "#             print(posVal)\n",
    "#             start = time.time()\n",
    "#             for i in range(1000):\n",
    "            negVal = torch.mm(self.contextEmbeddings[negSampleIndices], torch.t(embedCenter))\n",
    "            negVal = self.logSigmoid(-torch.sum(negVal))\n",
    "#             print(\"avg time: {}\".format((time.time() - start)/100))\n",
    "#             print(torch.mm(self.contextEmbeddings[negSampleIndices], torch.t(embedCenter)).shape)\n",
    "#             1/0\n",
    "            logProb = posVal + negVal\n",
    "        else:\n",
    "#             allEmbeddingIdxs = torch.autograd.Variable(torch.LongTensor([np.arange(0,self.vocabSize)]))\n",
    "\n",
    "\n",
    "\n",
    "    #         print(allEmbeddingIdxs)\n",
    "    #         allContextEmbeddings = self.contextEmbeddings(allEmbeddingIdxs).squeeze()\n",
    "            num = torch.exp(torch.mm(embedContext, torch.t(embedCenter)))\n",
    "#             start = time.time()\n",
    "#             for i in range(1000):\n",
    "            denom = torch.exp(torch.mm(self.contextEmbeddings, torch.t(embedCenter))).sum()\n",
    "#             print(\"avg time: {}\".format((time.time() - start)/100))\n",
    "#             print(torch.exp(torch.mm(self.contextEmbeddings, torch.t(embedCenter))).shape)\n",
    "#             1/0\n",
    "            logProb = torch.log(num/denom)\n",
    "#         print(logProb)\n",
    "        return(logProb)\n",
    "\n",
    "\n",
    "def train_skipgram(embeddingSize, trainingData, vocabCount, word2Idx, idx2Word, k, referenceWords):\n",
    "    print(\"training on {} observations\".format(len(trainingData)))\n",
    "    losses = []\n",
    "    loss_fn = nn.MSELoss()\n",
    "    model = SkipGram(vocabSize = len(word2Idx), embedSize = embeddingSize,\n",
    "                     vocabCount = vocabCount, word2Idx = word2Idx)\n",
    "#     print(model)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    listNearestWords(model = model, idx2Word = idx2Word,\n",
    "     referenceWords = referenceWords, topN = 5)\n",
    "    batchSize = 32\n",
    "    for epoch in tqdm_notebook(range(n_epoch), position = 0):\n",
    "#         listNearestWords(model = model, idx2Word = idx2Word,\n",
    "#                  referenceWords = referenceWords, topN = 5)\n",
    "        total_loss = .0\n",
    "        avgLoss = 0.0\n",
    "        iteration = 0\n",
    "#         for step in range(0, len(trainingData), batchSize):\n",
    "        for in_w, out_w in tqdm_notebook(trainingData, position = 1):\n",
    "#             endIdx = np.min((i+batchSize), len(trainingData))\n",
    "#             myBatch = trainingData[i:(i+batchSize)]\n",
    "            if k is not None:\n",
    "                negSamples = model.getNegSample(k = k, centerWord = in_w)\n",
    "            else:\n",
    "                negSamples = None\n",
    "#             print(\"neg samples found\")\n",
    "#             print(negSamples)\n",
    "            in_w_var = word2Idx[in_w]#torch.autograd.Variable(torch.LongTensor([word2Idx[in_w]]))\n",
    "            out_w_var = word2Idx[out_w]#torch.autograd.Variable(torch.LongTensor([word2Idx[out_w]]))\n",
    "#             if in_w in word2Idx:\n",
    "#                 in_w_var = word2Idx[in_w]#torch.autograd.Variable(torch.LongTensor([word2Idx[in_w]]))\n",
    "#             else:\n",
    "#                 in_w_var = word2Idx[\"<UNK>\"]\n",
    "#             if out_w in word2Idx:\n",
    "#                 out_w_var = word2Idx[out_w]#torch.autograd.Variable(torch.LongTensor([word2Idx[out_w]]))\n",
    "#             else:\n",
    "#                 out_w_var = word2Idx[\"<UNK>\"]\n",
    "            \n",
    "            model.zero_grad()\n",
    "            log_probs = model(in_w_var, out_w_var, negSampleIndices = negSamples)\n",
    "            loss = -log_probs#loss_fn(log_probs[0], torch.autograd.Variable(torch.Tensor([1])))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.data.numpy()\n",
    "            avgLoss += loss.data.numpy()\n",
    "            iteration += 1\n",
    "            if iteration % 10000 == 0:\n",
    "                avgLoss = total_loss/(iteration)\n",
    "                print(\"avg loss: {}\".format(avgLoss))\n",
    "                avgLoss = 0.0\n",
    "            if iteration % 20000 == 0:\n",
    "                listNearestWords(model = model, idx2Word = idx2Word,\n",
    "                 referenceWords = referenceWords, topN = 5)\n",
    "        losses.append(total_loss)    \n",
    "        print(f'Loss at epoch {epoch}: {total_loss/len(trainingData)}')\n",
    "        listNearestWords(model = model, idx2Word = idx2Word,\n",
    "                     referenceWords = referenceWords, topN = 5)\n",
    "    return(model, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### BATCH VERSION ######\n",
    "\n",
    "\n",
    "def generateObservations(tokenizedCorpus, word2Idx):\n",
    "    window_size = 5\n",
    "    idxPairs = []\n",
    "    # for each sentence\n",
    "    for sentence in tokenizedCorpus:\n",
    "#         indices = [word2Idx[word] for word in sentence]\n",
    "        # for each word, threated as center word\n",
    "        for center_word_pos in range(len(sentence)):\n",
    "            # for each window position\n",
    "            for w in range(-window_size, window_size + 1):\n",
    "                context_word_pos = center_word_pos + w\n",
    "                # make soure not jump out sentence\n",
    "                if context_word_pos < 0 or context_word_pos >= len(sentence) or center_word_pos == context_word_pos:\n",
    "                    continue\n",
    "                idxPairs.append((sentence[center_word_pos], sentence[context_word_pos]))\n",
    "\n",
    "    idxPairs = np.array(idxPairs) # it will be useful to have this as numpy array\n",
    "    return(idxPairs)\n",
    "\n",
    "\n",
    "def generateWordSamplingProb(vocabCount, word2Idx):\n",
    "    wordSampleProbs = [0.0]*len(vocabCount)\n",
    "    numWords = np.sum([count**0.75 for word, count in vocabCount])\n",
    "    for idx in range(len(vocabCount)):\n",
    "        w,c = vocabCount[idx]\n",
    "        wordSampleProbs[word2Idx[w]] = (c**0.75)/(numWords)\n",
    "        \n",
    "        \n",
    "        \n",
    "    wordSampleProbs = []\n",
    "    numWords = np.sum([count for word, count in vocabCount])\n",
    "    for w,c in vocabCount:\n",
    "#         w,c = vocabCount[idx]\n",
    "        wordSampleProbs.extend([word2Idx[w]] * int(((c/numWords)**0.75)/0.001))\n",
    "    return(wordSampleProbs)\n",
    "    \n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocabSize, embedSize, vocabCount, word2Idx):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.vocabSize = vocabSize\n",
    "        self.word2Idx = word2Idx\n",
    "#         self.centerEmbeddings = nn.Embedding(vocab_size, embd_size)\n",
    "#         self.contextEmbeddings = nn.Embedding(vocab_size, embd_size)\n",
    "#         self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
    "        \n",
    "        self.centerEmbeddings = nn.Parameter(torch.randn(vocabSize,\n",
    "                                                     embedSize).float(), requires_grad=True)\n",
    "        self.contextEmbeddings = nn.Parameter(torch.randn(vocabSize,\n",
    "                                                      embedSize).float(), requires_grad=True)\n",
    "        \n",
    "#         initrange = (2.0 / (vocabSize + embedSize)) ** 0.5  # Xavier init\n",
    "        nn.init.xavier_uniform_(self.contextEmbeddings)\n",
    "        nn.init.xavier_uniform_(self.centerEmbeddings)\n",
    "        \n",
    "        self.wordSampleProbs = generateWordSamplingProb(vocabCount, word2Idx)\n",
    "        self.logSigmoid = nn.LogSigmoid()\n",
    "#         self.paramList = nn.ModuleList([self.centerEmbeddings, self.contextEmbeddings] )\n",
    "    def getNegSample(self, k, centerWords):\n",
    "        vocabSizeWithoutUnk = self.vocabSize - 1\n",
    "#         negSample = np.random.choice(vocabSizeWithoutUnk,\n",
    "#                                      size = k, replace = True, p = self.wordSampleProbs)\n",
    "        negSamples = []\n",
    "        for centerWord in centerWords:\n",
    "            negSample = random.sample(self.wordSampleProbs, k)\n",
    "            while self.word2Idx[centerWord] in negSample:\n",
    "                negSample = random.sample(self.wordSampleProbs, k)\n",
    "            negSamples.append(negSample)\n",
    "        return(negSamples)\n",
    "    def forward(self, center, context, negSampleIndices = None):\n",
    "#         focus = torch.autograd.Variable(torch.LongTensor([0]))\n",
    "#         context = torch.autograd.Variable(torch.LongTensor([0]))\n",
    "#         allEmbeddingIdxs = torch.autograd.Variable(torch.LongTensor([np.arange(0,self.vocabSize)]))\n",
    "\n",
    "\n",
    "#         embedCenter = self.centerEmbeddings(center).view((1, -1))\n",
    "#         embedContext = self.contextEmbeddings(context).view((1, -1))\n",
    "# #         print(allEmbeddingIdxs)\n",
    "#         allContextEmbeddings = self.contextEmbeddings(allEmbeddingIdxs).squeeze()\n",
    "#         num = torch.exp(torch.mm(embedContext, torch.t(embedCenter)))\n",
    "#         denom = torch.exp(torch.mm(allContextEmbeddings, torch.t(embedCenter))).sum()\n",
    "#         logProb = torch.log(num/denom)\n",
    "#         print(\"center indices\\n\",center)\n",
    "#         print(\"context indices\\n\",context)\n",
    "        embedCenter = self.centerEmbeddings[center]#.view((1, -1))\n",
    "        embedContext = self.contextEmbeddings[context]#.view((1, -1))       \n",
    "        if negSampleIndices is not None:\n",
    "#             print(\"hey\")\n",
    "#             posVal = self.logSigmoid(torch.mm(embedContext, torch.t(embedCenter)))\n",
    "#             print(posVal)\n",
    "            posVal = self.logSigmoid(torch.sum(embedContext * embedCenter, dim = 1)).squeeze()\n",
    "#             print(posVal)\n",
    "#             start = time.time()\n",
    "#             for i in range(1000):\n",
    "#             print(torch.autograd.Variable(torch.LongTensor(negSampleIndices)))\n",
    "            negSampleIndices = torch.autograd.Variable(torch.LongTensor(negSampleIndices))\n",
    "#             print(self.contextEmbeddings[negSampleIndices].shape)\n",
    "#             print(embedCenter.shape)\n",
    "            negVal = torch.bmm(self.contextEmbeddings[negSampleIndices], embedCenter.unsqueeze(2)).squeeze(2)\n",
    "            negVal = self.logSigmoid(-torch.sum(negVal, dim = 1)).squeeze()\n",
    "#             print(\"avg time: {}\".format((time.time() - start)/100))\n",
    "#             print(torch.mm(self.contextEmbeddings[negSampleIndices], torch.t(embedCenter)).shape)\n",
    "#             1/0\n",
    "#             print(posVal.shape)\n",
    "#             print(negVal.shape)\n",
    "#             1/0\n",
    "            logProb = -(posVal + negVal).mean()\n",
    "        else:\n",
    "#             allEmbeddingIdxs = torch.autograd.Variable(torch.LongTensor([np.arange(0,self.vocabSize)]))\n",
    "\n",
    "\n",
    "\n",
    "    #         print(allEmbeddingIdxs)\n",
    "    #         allContextEmbeddings = self.contextEmbeddings(allEmbeddingIdxs).squeeze()\n",
    "            num = torch.exp(torch.mm(embedContext, torch.t(embedCenter)))\n",
    "#             start = time.time()\n",
    "#             for i in range(1000):\n",
    "            denom = torch.exp(torch.mm(self.contextEmbeddings, torch.t(embedCenter))).sum()\n",
    "#             print(\"avg time: {}\".format((time.time() - start)/100))\n",
    "#             print(torch.exp(torch.mm(self.contextEmbeddings, torch.t(embedCenter))).shape)\n",
    "#             1/0\n",
    "            logProb = torch.log(num/denom)\n",
    "#         print(logProb)\n",
    "        return(logProb)\n",
    "\n",
    "\n",
    "def train_skipgram(embeddingSize, trainingData, vocabCount, word2Idx, idx2Word, k, referenceWords):\n",
    "    print(\"training on {} observations\".format(len(trainingData)))\n",
    "    losses = []\n",
    "    loss_fn = nn.MSELoss()\n",
    "    model = SkipGram(vocabSize = len(word2Idx), embedSize = embeddingSize,\n",
    "                     vocabCount = vocabCount, word2Idx = word2Idx)\n",
    "#     print(model)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    listNearestWords(model = model, idx2Word = idx2Word,\n",
    "     referenceWords = referenceWords, topN = 5)\n",
    "    batchSize = 512\n",
    "    for epoch in tqdm_notebook(range(n_epoch), position = 0):\n",
    "#         listNearestWords(model = model, idx2Word = idx2Word,\n",
    "#                  referenceWords = referenceWords, topN = 5)\n",
    "        total_loss = .0\n",
    "        avgLoss = 0.0\n",
    "        iteration = 0\n",
    "        for step in tqdm_notebook(range(0, len(trainingData), batchSize), position = 1):\n",
    "#         for in_w, out_w in tqdm_notebook(trainingData, position = 1):\n",
    "            endIdx = np.min([(step+batchSize), len(trainingData)])\n",
    "            myBatch = trainingData[step:(step+batchSize)]\n",
    "#             print(myBatch)\n",
    "            centerWords = [elem[0] for elem in myBatch]\n",
    "            contextWords = [elem[1] for elem in myBatch]\n",
    "#             print(centerWords)\n",
    "            if k is not None:\n",
    "                negSamples = model.getNegSample(k = k, centerWords = centerWords)\n",
    "            else:\n",
    "                negSamples = None\n",
    "#             print(\"neg samples found\")\n",
    "#             print(negSamples)\n",
    "            centerIDs = [word2Idx[idx] for idx in centerWords]#torch.autograd.Variable(torch.LongTensor([word2Idx[in_w]]))\n",
    "            contextIDs = [word2Idx[idx] for idx in contextWords]#torch.autograd.Variable(torch.LongTensor([word2Idx[out_w]]))\n",
    "#             if in_w in word2Idx:\n",
    "#                 in_w_var = word2Idx[in_w]#torch.autograd.Variable(torch.LongTensor([word2Idx[in_w]]))\n",
    "#             else:\n",
    "#                 in_w_var = word2Idx[\"<UNK>\"]\n",
    "#             if out_w in word2Idx:\n",
    "#                 out_w_var = word2Idx[out_w]#torch.autograd.Variable(torch.LongTensor([word2Idx[out_w]]))\n",
    "#             else:\n",
    "#                 out_w_var = word2Idx[\"<UNK>\"]\n",
    "            \n",
    "            model.zero_grad()\n",
    "            loss = model(centerIDs, contextIDs, negSampleIndices = negSamples)\n",
    "#             print(loss)\n",
    "#             print(model.centerEmbeddings.mean())\n",
    "#             loss = -log_probs#loss_fn(log_probs[0], torch.autograd.Variable(torch.Tensor([1])))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.data.numpy()\n",
    "            avgLoss += loss.data.numpy()\n",
    "            iteration += 1\n",
    "            if iteration % 500 == 0:\n",
    "                avgLoss = avgLoss/(500)\n",
    "                print(\"avg loss: {}\".format(avgLoss))\n",
    "#                 avgLoss = 0.0\n",
    "#             if iteration % 2000 == 0:\n",
    "#                 listNearestWords(model = model, idx2Word = idx2Word,\n",
    "#                  referenceWords = referenceWords, topN = 5)\n",
    "        losses.append(total_loss)    \n",
    "        print(f'Loss at epoch {epoch}: {total_loss/iteration}')\n",
    "        listNearestWords(model = model, idx2Word = idx2Word,\n",
    "                     referenceWords = referenceWords, topN = 5)\n",
    "    return(model, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "def listNearestWords(model, idx2Word, referenceWords, topN):\n",
    "    assert len(idx2Word) == len(model.word2Idx), \"Possibly passed in two different vocabularies\"\n",
    "    embeddings = model.centerEmbeddings.data.numpy()\n",
    "#     embeddings = model.contextEmbeddings.data.numpy()\n",
    "    distMat = cdist(embeddings, embeddings, metric = \"cosine\")\n",
    "    for word in referenceWords:\n",
    "        wordIdx = model.word2Idx[word]\n",
    "#         print(np.argmin(distMat[wordIdx,:]))\n",
    "        closestIndices = np.argsort(distMat[wordIdx,:])[0:topN]\n",
    "        closestWords = [(idx2Word[idx], distMat[wordIdx, idx]) for idx in closestIndices]\n",
    "        for elem in closestWords:\n",
    "            print(elem)\n",
    "        print(\"*\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# embd_size = 100\n",
    "# learning_rate = 0.001\n",
    "# n_epoch = 60\n",
    "# idxPairsTest = generateObservations(tokenizedCorpus = finalTokenizedCorpus_test, word2Idx = word2Idx_test)\n",
    "# sg_model, sg_losses = train_skipgram(embeddingSize = 5, trainingData = idxPairsTest, vocabCount = vocabCount_test,\n",
    "#                                      word2Idx = word2Idx_test, k = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on 1195636 observations\n",
      "('bush', 0.0)\n",
      "('ny', 0.5722449167339645)\n",
      "('degrees', 0.58268658862842)\n",
      "('fitzwater', 0.59456595395935)\n",
      "('review', 0.6011509501625227)\n",
      "**************************************************\n",
      "\n",
      "('soviet', 0.0)\n",
      "('mind', 0.5187260751221663)\n",
      "('trump', 0.5457773526336366)\n",
      "('chemicals', 0.5646036039292068)\n",
      "('shortly', 0.5694139908820548)\n",
      "**************************************************\n",
      "\n",
      "('president', 1.1102230246251565e-16)\n",
      "('service', 0.5355918862435511)\n",
      "('peaceful', 0.5405652230804536)\n",
      "('keeping', 0.5586084850826956)\n",
      "('scheme', 0.5725764880682768)\n",
      "**************************************************\n",
      "\n",
      "('military', 1.1102230246251565e-16)\n",
      "('44', 0.5150267718218834)\n",
      "('ive', 0.5166936271712037)\n",
      "('californias', 0.5347822524786181)\n",
      "('cavazos', 0.5514847430360431)\n",
      "**************************************************\n",
      "\n",
      "('american', 0.0)\n",
      "('relative', 0.463291456093445)\n",
      "('apartment', 0.4872718948682221)\n",
      "('search', 0.5156335821608905)\n",
      "('talks', 0.5300147132595971)\n",
      "**************************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "947e4fbceab24e7d91425cbb4b3ff38e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=60), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e2fef6da96444b879a2f2a02486e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 1.3492616987228394\n",
      "avg loss: 1.21932355284214\n",
      "avg loss: 1.1854211774611376\n",
      "avg loss: 1.161834837811618\n",
      "Loss at epoch 0: 1.2151192926788983\n",
      "('bush', 0.0)\n",
      "('said', 0.1647495999569355)\n",
      "('also', 0.1761844887434828)\n",
      "('<UNK>', 0.18910804454657448)\n",
      "('percent', 0.1922690944961828)\n",
      "**************************************************\n",
      "\n",
      "('soviet', 0.0)\n",
      "('<UNK>', 0.06951568610943837)\n",
      "('said', 0.06973861499696665)\n",
      "('would', 0.08786484580133114)\n",
      "('years', 0.11004755355922058)\n",
      "**************************************************\n",
      "\n",
      "('president', 0.0)\n",
      "('would', 0.07809489798761304)\n",
      "('<UNK>', 0.08611170296054982)\n",
      "('said', 0.09290938252018188)\n",
      "('new', 0.10922523419730479)\n",
      "**************************************************\n",
      "\n",
      "('military', 0.0)\n",
      "('would', 0.2330165741270377)\n",
      "('percent', 0.23949223567287792)\n",
      "('also', 0.24962935078846005)\n",
      "('years', 0.2715237974312834)\n",
      "**************************************************\n",
      "\n",
      "('american', 0.0)\n",
      "('<UNK>', 0.136442419580748)\n",
      "('said', 0.1470133354347164)\n",
      "('people', 0.15711472670405913)\n",
      "('year', 0.1673182639282409)\n",
      "**************************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07da61007a7545219854db80c28fae38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 1.11722132563591\n",
      "avg loss: 1.0835749207139016\n"
     ]
    }
   ],
   "source": [
    "embeddingSize = 50\n",
    "learning_rate = 0.1\n",
    "n_epoch = 60\n",
    "idxPairsAP = generateObservations(tokenizedCorpus = finalTokenizedCorpus_ap, word2Idx = word2Idx_ap)\n",
    "sg_model_ap, sg_losses_ap = train_skipgram(embeddingSize = embeddingSize, trainingData = idxPairsAP,\n",
    "                                     vocabCount = vocabCount_ap,\n",
    "                                     word2Idx = word2Idx_ap, idx2Word = idx2Word_ap, k = 20,\n",
    "                                          referenceWords = [\"bush\", \"soviet\", \"president\", \"military\", \"american\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embeddingSize = 50\n",
    "learning_rate = 0.001\n",
    "n_epoch = 3\n",
    "idxPairsPubMed = generateObservations(tokenizedCorpus = finalTokenizedCorpus_pubMed, word2Idx = word2Idx_pubMed)\n",
    "sg_model_pubMed, sg_losses_pubMed = train_skipgram(embeddingSize = embeddingSize, trainingData = idxPairsPubMed,\n",
    "                                     vocabCount = vocabCount_pubMed,\n",
    "                                     word2Idx = word2Idx_pubMed, idx2Word = idx2Word_pubMed, k = 15,\n",
    "                                                  referenceWords = [\"clinical\", \"obesity\", \"microbial\", \"microbiome\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "def listNearestWords(model, idx2Word, referenceWords, topN):\n",
    "    assert len(idx2Word) == len(model.word2Idx), \"Possibly passed in two different vocabularies\"\n",
    "    embeddings = model.centerEmbeddings.data.numpy()\n",
    "    distMat = cdist(embeddings, embeddings, metric = \"cosine\")\n",
    "    for word in referenceWords:\n",
    "        wordIdx = model.word2Idx[word]\n",
    "#         print(np.argmin(distMat[wordIdx,:]))\n",
    "        closestIndices = np.argsort(distMat[wordIdx,:])[0:topN]\n",
    "        closestWords = [(idx2Word[idx], distMat[wordIdx, idx]) for idx in closestIndices]\n",
    "        for elem in closestWords:\n",
    "            print(elem)\n",
    "        print(\"*\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('bush', 2.220446049250313e-16)\n",
      "('would', 0.245172107921238)\n",
      "('one', 0.2542772042354078)\n",
      "('last', 0.26639998381930596)\n",
      "('<UNK>', 0.2878025745606383)\n",
      "('said', 0.2929848618692378)\n",
      "('government', 0.3014082414158188)\n",
      "('president', 0.3090025777889278)\n",
      "('also', 0.31906111583316044)\n",
      "('two', 0.32561755407928095)\n",
      "**************************************************\n",
      "\n",
      "('soviet', 0.0)\n",
      "('also', 0.26371813187150006)\n",
      "('us', 0.2754812328259487)\n",
      "('one', 0.2757421412874528)\n",
      "('government', 0.28862726565176866)\n",
      "('said', 0.2917928720369569)\n",
      "('people', 0.32292569939644367)\n",
      "('would', 0.3236237758736388)\n",
      "('could', 0.32972287786631693)\n",
      "('two', 0.33019233004600756)\n",
      "**************************************************\n",
      "\n",
      "('stock', 0.0)\n",
      "('jacksons', 0.5139301958926246)\n",
      "('mention', 0.5246623586362416)\n",
      "('library', 0.5472043281618073)\n",
      "('produce', 0.5616844124533857)\n",
      "('crisis', 0.5690514279470098)\n",
      "('expectations', 0.5712754139059562)\n",
      "('midday', 0.5726643036563688)\n",
      "('case', 0.5733000645351246)\n",
      "('angolan', 0.5740486019632982)\n",
      "**************************************************\n",
      "\n",
      "('dukakis', 0.0)\n",
      "('neighbor', 0.5323852886472744)\n",
      "('public', 0.5348249757043845)\n",
      "('surviving', 0.5371684477894781)\n",
      "('23', 0.5402720445714361)\n",
      "('hands', 0.548594344205205)\n",
      "('honest', 0.5527368906564407)\n",
      "('oats', 0.5557959501279155)\n",
      "('actions', 0.5566082591235357)\n",
      "('rappaport', 0.5600067781772219)\n",
      "**************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "listNearestWords(model = sg_model_ap, idx2Word = idx2Word_ap,\n",
    "                 referenceWords = [\"bush\", \"soviet\", \"stock\", \"dukakis\"], topN = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cancer', 1.1102230246251565e-16)\n",
      "('considerations', 0.5357610920425107)\n",
      "('mg2deficient', 0.600471133270606)\n",
      "('day', 0.6008124548358114)\n",
      "('numerous', 0.6035130472995224)\n",
      "('classifier', 0.6086428712998291)\n",
      "('ratio', 0.6112479094677603)\n",
      "('kappa', 0.611541393621376)\n",
      "('metagenome', 0.6140737843923152)\n",
      "('manuscript', 0.6154556158382964)\n",
      "**************************************************\n",
      "\n",
      "('drug', 2.220446049250313e-16)\n",
      "('complexity', 0.5405080936495106)\n",
      "('59', 0.5560538680379796)\n",
      "('workers', 0.5597953026126847)\n",
      "('found', 0.5682576198020208)\n",
      "('regions', 0.5741363153547547)\n",
      "('clinical', 0.5891034315026564)\n",
      "('created', 0.6040641849145176)\n",
      "('templates', 0.6098548625635937)\n",
      "('true', 0.610169663686098)\n",
      "**************************************************\n",
      "\n",
      "('microbiome', 1.1102230246251565e-16)\n",
      "('therefore', 0.4981334883145526)\n",
      "('delivery', 0.5116358247873145)\n",
      "('eating', 0.5532279306206287)\n",
      "('identification', 0.5647300339111128)\n",
      "('experts', 0.5667386588455259)\n",
      "('middleaged', 0.5876831614139415)\n",
      "('836', 0.5935795305745988)\n",
      "('profiles', 0.6068106348836213)\n",
      "('opens', 0.613859273268841)\n",
      "**************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "listNearestWords(model = sg_model_pubMed, idx2Word = idx2Word_pubMed,\n",
    "                 referenceWords = [\"cancer\", \"drug\", \"microbiome\"], topN = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"cancer\" in sg_model.word2Idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'said': 0,\n",
       " 'percent': 1,\n",
       " 'year': 2,\n",
       " 'new': 3,\n",
       " 'us': 4,\n",
       " 'years': 5,\n",
       " 'people': 6,\n",
       " 'one': 7,\n",
       " 'would': 8,\n",
       " 'two': 9,\n",
       " 'also': 10,\n",
       " 'soviet': 11,\n",
       " 'president': 12,\n",
       " 'police': 13,\n",
       " 'last': 14,\n",
       " 'oil': 15,\n",
       " 'government': 16,\n",
       " 'bank': 17,\n",
       " 'officials': 18,\n",
       " 'could': 19,\n",
       " 'ago': 20,\n",
       " 'first': 21,\n",
       " 'national': 22,\n",
       " 'state': 23,\n",
       " 'million': 24,\n",
       " 'states': 25,\n",
       " 'prices': 26,\n",
       " 'three': 27,\n",
       " 'official': 28,\n",
       " 'reported': 29,\n",
       " 'back': 30,\n",
       " 'monday': 31,\n",
       " 'dukakis': 32,\n",
       " 'rose': 33,\n",
       " 'rate': 34,\n",
       " 'bush': 35,\n",
       " 'fire': 36,\n",
       " 'war': 37,\n",
       " 'get': 38,\n",
       " 'military': 39,\n",
       " 'economic': 40,\n",
       " 'thursday': 41,\n",
       " 'company': 42,\n",
       " 'time': 43,\n",
       " 'made': 44,\n",
       " 'saying': 45,\n",
       " 'today': 46,\n",
       " 'american': 47,\n",
       " 'since': 48,\n",
       " 'roberts': 49,\n",
       " 'dont': 50,\n",
       " 'federal': 51,\n",
       " 'told': 52,\n",
       " 'mrs': 53,\n",
       " 'noriega': 54,\n",
       " 'forces': 55,\n",
       " 'months': 56,\n",
       " 'may': 57,\n",
       " 'rating': 58,\n",
       " 'good': 59,\n",
       " 'friday': 60,\n",
       " 'long': 61,\n",
       " 'day': 62,\n",
       " 'top': 63,\n",
       " 'united': 64,\n",
       " 'saudi': 65,\n",
       " 'use': 66,\n",
       " 'economy': 67,\n",
       " 'campaign': 68,\n",
       " 'gorbachev': 69,\n",
       " 'killed': 70,\n",
       " 'officers': 71,\n",
       " 'billion': 72,\n",
       " 'take': 73,\n",
       " 'jackson': 74,\n",
       " 'program': 75,\n",
       " 'washington': 76,\n",
       " 'end': 77,\n",
       " 'union': 78,\n",
       " 'gas': 79,\n",
       " 'gorbachevs': 80,\n",
       " 'see': 81,\n",
       " 'went': 82,\n",
       " '10': 83,\n",
       " 'foreign': 84,\n",
       " 'peres': 85,\n",
       " 'iraq': 86,\n",
       " 'report': 87,\n",
       " 'news': 88,\n",
       " 'many': 89,\n",
       " 'waste': 90,\n",
       " 'died': 91,\n",
       " 'believe': 92,\n",
       " 'man': 93,\n",
       " 'september': 94,\n",
       " 'part': 95,\n",
       " 'home': 96,\n",
       " 'city': 97,\n",
       " '1987': 98,\n",
       " 'tuesday': 99,\n",
       " 'five': 100,\n",
       " 'panama': 101,\n",
       " 'month': 102,\n",
       " 'country': 103,\n",
       " 'early': 104,\n",
       " 'letter': 105,\n",
       " 'increase': 106,\n",
       " 'business': 107,\n",
       " 'system': 108,\n",
       " 'saturday': 109,\n",
       " 'shot': 110,\n",
       " 'several': 111,\n",
       " 'group': 112,\n",
       " 'wednesday': 113,\n",
       " 'offer': 114,\n",
       " 'make': 115,\n",
       " 'think': 116,\n",
       " 'work': 117,\n",
       " 'earlier': 118,\n",
       " 'sunday': 119,\n",
       " 'want': 120,\n",
       " 'far': 121,\n",
       " 'past': 122,\n",
       " 'businesses': 123,\n",
       " 'week': 124,\n",
       " 'plan': 125,\n",
       " 'price': 126,\n",
       " 'magellan': 127,\n",
       " 'spacecraft': 128,\n",
       " 'contact': 129,\n",
       " 'employees': 130,\n",
       " 'polish': 131,\n",
       " 'study': 132,\n",
       " 'agents': 133,\n",
       " 'school': 134,\n",
       " 'spokesman': 135,\n",
       " 'outside': 136,\n",
       " 'least': 137,\n",
       " 'public': 138,\n",
       " 'plant': 139,\n",
       " 'even': 140,\n",
       " 'front': 141,\n",
       " 'security': 142,\n",
       " 'world': 143,\n",
       " 'operating': 144,\n",
       " 'continue': 145,\n",
       " 'talks': 146,\n",
       " 'help': 147,\n",
       " 'statement': 148,\n",
       " 'night': 149,\n",
       " 'lost': 150,\n",
       " 'service': 151,\n",
       " 'november': 152,\n",
       " 'inflation': 153,\n",
       " 'goods': 154,\n",
       " 'life': 155,\n",
       " 'average': 156,\n",
       " 'area': 157,\n",
       " 'services': 158,\n",
       " 'cuban': 159,\n",
       " 'head': 160,\n",
       " 'another': 161,\n",
       " 'four': 162,\n",
       " 'lot': 163,\n",
       " 'like': 164,\n",
       " 'found': 165,\n",
       " 'investigation': 166,\n",
       " 'minister': 167,\n",
       " 'come': 168,\n",
       " 'well': 169,\n",
       " 'liberace': 170,\n",
       " 'museum': 171,\n",
       " 'days': 172,\n",
       " 'called': 173,\n",
       " 'feet': 174,\n",
       " 'still': 175,\n",
       " 'got': 176,\n",
       " 'administration': 177,\n",
       " 'defense': 178,\n",
       " 'diplomatic': 179,\n",
       " 'democratic': 180,\n",
       " 'skins': 181,\n",
       " 'used': 182,\n",
       " 'support': 183,\n",
       " 'forest': 184,\n",
       " 'leader': 185,\n",
       " 'victims': 186,\n",
       " 'england': 187,\n",
       " 'production': 188,\n",
       " 'october': 189,\n",
       " 'john': 190,\n",
       " 'period': 191,\n",
       " 'second': 192,\n",
       " 'expected': 193,\n",
       " 'problems': 194,\n",
       " 'real': 195,\n",
       " 'agency': 196,\n",
       " 'blackowned': 197,\n",
       " 'teacher': 198,\n",
       " 'george': 199,\n",
       " 'family': 200,\n",
       " 'iraqi': 201,\n",
       " 'ministry': 202,\n",
       " 'san': 203,\n",
       " 'near': 204,\n",
       " 'relations': 205,\n",
       " 'came': 206,\n",
       " 'thats': 207,\n",
       " 'general': 208,\n",
       " 'political': 209,\n",
       " 'going': 210,\n",
       " 'way': 211,\n",
       " 'across': 212,\n",
       " 'york': 213,\n",
       " 'great': 214,\n",
       " 'following': 215,\n",
       " 'organization': 216,\n",
       " 'announced': 217,\n",
       " 'around': 218,\n",
       " 'started': 219,\n",
       " 'might': 220,\n",
       " 'im': 221,\n",
       " 'problem': 222,\n",
       " 'control': 223,\n",
       " 'taking': 224,\n",
       " 'committee': 225,\n",
       " 'miles': 226,\n",
       " '15': 227,\n",
       " 'six': 228,\n",
       " 'august': 229,\n",
       " 'increased': 230,\n",
       " 'annual': 231,\n",
       " 'nations': 232,\n",
       " 'pictures': 233,\n",
       " 'programs': 234,\n",
       " 'often': 235,\n",
       " 'commission': 236,\n",
       " 'arabia': 237,\n",
       " 'soviets': 238,\n",
       " 'arco': 239,\n",
       " 'guerrillas': 240,\n",
       " 'baker': 241,\n",
       " 'fbi': 242,\n",
       " 'films': 243,\n",
       " 'x': 244,\n",
       " 'apparently': 245,\n",
       " 'know': 246,\n",
       " 'students': 247,\n",
       " 'third': 248,\n",
       " 'primary': 249,\n",
       " 'bechtel': 250,\n",
       " 'israel': 251,\n",
       " 'rappaport': 252,\n",
       " 'never': 253,\n",
       " 'border': 254,\n",
       " 'asked': 255,\n",
       " 'according': 256,\n",
       " 'sales': 257,\n",
       " 'party': 258,\n",
       " 'become': 259,\n",
       " 'whether': 260,\n",
       " 'telephone': 261,\n",
       " 'late': 262,\n",
       " 'bloomberg': 263,\n",
       " 'including': 264,\n",
       " 'office': 265,\n",
       " 'car': 266,\n",
       " 'working': 267,\n",
       " 'away': 268,\n",
       " 'black': 269,\n",
       " 'began': 270,\n",
       " 'former': 271,\n",
       " 'star': 272,\n",
       " 'wife': 273,\n",
       " 'something': 274,\n",
       " 'theres': 275,\n",
       " 'brought': 276,\n",
       " 'reporters': 277,\n",
       " 'house': 278,\n",
       " 'international': 279,\n",
       " 'cuba': 280,\n",
       " 'syria': 281,\n",
       " 'department': 282,\n",
       " 'presidential': 283,\n",
       " 'efforts': 284,\n",
       " 'southern': 285,\n",
       " 'drug': 286,\n",
       " 'trade': 287,\n",
       " 'members': 288,\n",
       " 'leaders': 289,\n",
       " 'workers': 290,\n",
       " 'vote': 291,\n",
       " 'highest': 292,\n",
       " 'mayor': 293,\n",
       " 'governor': 294,\n",
       " 'republican': 295,\n",
       " 'reports': 296,\n",
       " 'industrial': 297,\n",
       " 'fear': 298,\n",
       " 'ferrets': 299,\n",
       " 'children': 300,\n",
       " 'doctors': 301,\n",
       " 'small': 302,\n",
       " 'compared': 303,\n",
       " 'talked': 304,\n",
       " 'set': 305,\n",
       " 'future': 306,\n",
       " 'biggest': 307,\n",
       " 'co': 308,\n",
       " 'change': 309,\n",
       " 'dog': 310,\n",
       " 'injured': 311,\n",
       " 'next': 312,\n",
       " 'tanks': 313,\n",
       " 'noted': 314,\n",
       " 'rebel': 315,\n",
       " 'worker': 316,\n",
       " 'communist': 317,\n",
       " 'europe': 318,\n",
       " 'need': 319,\n",
       " 'firefighters': 320,\n",
       " 'county': 321,\n",
       " 'orr': 322,\n",
       " 'czechoslovak': 323,\n",
       " 'ask': 324,\n",
       " 'iran': 325,\n",
       " 'survey': 326,\n",
       " 'hospital': 327,\n",
       " 'maxwell': 328,\n",
       " 'mpaa': 329,\n",
       " 'name': 330,\n",
       " 'identified': 331,\n",
       " '40': 332,\n",
       " 'serious': 333,\n",
       " 'condition': 334,\n",
       " 'door': 335,\n",
       " 'didnt': 336,\n",
       " 'appeared': 337,\n",
       " 'gun': 338,\n",
       " 'adult': 339,\n",
       " 'authorities': 340,\n",
       " 'senior': 341,\n",
       " 'offered': 342,\n",
       " 'without': 343,\n",
       " 'says': 344,\n",
       " 'things': 345,\n",
       " 'labor': 346,\n",
       " 'interested': 347,\n",
       " 'heard': 348,\n",
       " 'better': 349,\n",
       " 'music': 350,\n",
       " '2': 351,\n",
       " 'secretary': 352,\n",
       " 'number': 353,\n",
       " 'natural': 354,\n",
       " 'west': 355,\n",
       " 'reagan': 356,\n",
       " 'trouble': 357,\n",
       " 'powell': 358,\n",
       " 'within': 359,\n",
       " 'decision': 360,\n",
       " 'americans': 361,\n",
       " 'best': 362,\n",
       " 'senate': 363,\n",
       " 'call': 364,\n",
       " 'interests': 365,\n",
       " 'japan': 366,\n",
       " 'documents': 367,\n",
       " 'value': 368,\n",
       " 'toward': 369,\n",
       " 'election': 370,\n",
       " 'chairman': 371,\n",
       " 'career': 372,\n",
       " 'making': 373,\n",
       " 'issue': 374,\n",
       " 'increases': 375,\n",
       " 'manufacturing': 376,\n",
       " 'line': 377,\n",
       " 'caused': 378,\n",
       " 'given': 379,\n",
       " 'sen': 380,\n",
       " 'sometimes': 381,\n",
       " 'start': 382,\n",
       " 'based': 383,\n",
       " 'recent': 384,\n",
       " 'financial': 385,\n",
       " 'energy': 386,\n",
       " 'lay': 387,\n",
       " 'market': 388,\n",
       " 'earnings': 389,\n",
       " 'scientists': 390,\n",
       " 'venus': 391,\n",
       " 'earth': 392,\n",
       " 'radar': 393,\n",
       " 'bad': 394,\n",
       " 'details': 395,\n",
       " 'rebels': 396,\n",
       " 'agreement': 397,\n",
       " 'threat': 398,\n",
       " 'katyn': 399,\n",
       " 'tass': 400,\n",
       " 'moscow': 401,\n",
       " 'gasoline': 402,\n",
       " 'train': 403,\n",
       " 'site': 404,\n",
       " 'embassy': 405,\n",
       " 'director': 406,\n",
       " 'elephant': 407,\n",
       " 'hispanic': 408,\n",
       " 'societe': 409,\n",
       " 'tie': 410,\n",
       " 'shelter': 411,\n",
       " 'mcguire': 412,\n",
       " 'murder': 413,\n",
       " 'morning': 414,\n",
       " 'release': 415,\n",
       " 'boys': 416,\n",
       " 'jammed': 417,\n",
       " 'death': 418,\n",
       " 'wall': 419,\n",
       " 'complex': 420,\n",
       " 'high': 421,\n",
       " 'behind': 422,\n",
       " 'inc': 423,\n",
       " 'proposed': 424,\n",
       " 'pipeline': 425,\n",
       " '1': 426,\n",
       " 'point': 427,\n",
       " 'reached': 428,\n",
       " 'quoted': 429,\n",
       " 'memo': 430,\n",
       " 'robert': 431,\n",
       " 'denied': 432,\n",
       " 'go': 433,\n",
       " 'thought': 434,\n",
       " 'important': 435,\n",
       " 'put': 436,\n",
       " 'position': 437,\n",
       " 'concern': 438,\n",
       " 'took': 439,\n",
       " 'woman': 440,\n",
       " 'jewelry': 441,\n",
       " 'tried': 442,\n",
       " 'inside': 443,\n",
       " 'money': 444,\n",
       " 'place': 445,\n",
       " 'tied': 446,\n",
       " '29': 447,\n",
       " 'history': 448,\n",
       " 'upon': 449,\n",
       " 'pennsylvania': 450,\n",
       " 'penn': 451,\n",
       " 'invasion': 452,\n",
       " 'countries': 453,\n",
       " 'air': 454,\n",
       " 'force': 455,\n",
       " 'base': 456,\n",
       " 'court': 457,\n",
       " 'song': 458,\n",
       " 'sure': 459,\n",
       " 'hard': 460,\n",
       " 'enough': 461,\n",
       " 'panamanian': 462,\n",
       " 'foster': 463,\n",
       " 'operations': 464,\n",
       " 'central': 465,\n",
       " 'spokeswoman': 466,\n",
       " 'power': 467,\n",
       " 'later': 468,\n",
       " 'policies': 469,\n",
       " 'intelligence': 470,\n",
       " 'released': 471,\n",
       " 'export': 472,\n",
       " 'times': 473,\n",
       " 'plants': 474,\n",
       " 'half': 475,\n",
       " 'brush': 476,\n",
       " 'western': 477,\n",
       " 'stirbois': 478,\n",
       " 'capital': 479,\n",
       " 'ran': 480,\n",
       " 'member': 481,\n",
       " '1977': 482,\n",
       " 'deputy': 483,\n",
       " 'worst': 484,\n",
       " 'cant': 485,\n",
       " 'awards': 486,\n",
       " 'together': 487,\n",
       " '17': 488,\n",
       " 'robb': 489,\n",
       " 'level': 490,\n",
       " 'responsibility': 491,\n",
       " 'major': 492,\n",
       " 'troubles': 493,\n",
       " 'rise': 494,\n",
       " 'index': 495,\n",
       " 'sector': 496,\n",
       " 'higher': 497,\n",
       " 'spending': 498,\n",
       " '04': 499,\n",
       " 'say': 500,\n",
       " 'health': 501,\n",
       " 'sale': 502,\n",
       " 'north': 503,\n",
       " 'strait': 504,\n",
       " 'award': 505,\n",
       " 'much': 506,\n",
       " 'cut': 507,\n",
       " 'language': 508,\n",
       " 'houston': 509,\n",
       " 'evening': 510,\n",
       " 'peace': 511,\n",
       " 'various': 512,\n",
       " 'different': 513,\n",
       " 'lead': 514,\n",
       " 'enron': 515,\n",
       " 'largest': 516,\n",
       " 'per': 517,\n",
       " 'assets': 518,\n",
       " 'signal': 519,\n",
       " 'loss': 520,\n",
       " 'however': 521,\n",
       " 'face': 522,\n",
       " 'april': 523,\n",
       " 'words': 524,\n",
       " 'others': 525,\n",
       " 'costs': 526,\n",
       " 'loans': 527,\n",
       " 'difficult': 528,\n",
       " 'island': 529,\n",
       " 'order': 530,\n",
       " 'ground': 531,\n",
       " 'heavy': 532,\n",
       " 'numbers': 533,\n",
       " 'ortega': 534,\n",
       " 'overall': 535,\n",
       " 'sides': 536,\n",
       " 'contra': 537,\n",
       " 'ii': 538,\n",
       " 'graves': 539,\n",
       " 'nkvd': 540,\n",
       " 'speech': 541,\n",
       " 'truth': 542,\n",
       " 'chemical': 543,\n",
       " 'judge': 544,\n",
       " 'consumer': 545,\n",
       " 'less': 546,\n",
       " 'pittsburgh': 547,\n",
       " 'arms': 548,\n",
       " 'enforcement': 549,\n",
       " 'acres': 550,\n",
       " 'seen': 551,\n",
       " 'source': 552,\n",
       " 'gallon': 553,\n",
       " 'reforms': 554,\n",
       " 'firms': 555,\n",
       " 'zoo': 556,\n",
       " 'bombs': 557,\n",
       " 'generale': 558,\n",
       " 'classroom': 559,\n",
       " 'atlantic': 560,\n",
       " 'attempted': 561,\n",
       " 'assault': 562,\n",
       " 'related': 563,\n",
       " 'friends': 564,\n",
       " 'boy': 565,\n",
       " 'talking': 566,\n",
       " 'knew': 567,\n",
       " 'little': 568,\n",
       " '22': 569,\n",
       " 'nothing': 570,\n",
       " 'big': 571,\n",
       " 'h': 572,\n",
       " 'meeting': 573,\n",
       " 'thurston': 574,\n",
       " 'chief': 575,\n",
       " 'charles': 576,\n",
       " 'spent': 577,\n",
       " 'whose': 578,\n",
       " 'building': 579,\n",
       " 'trying': 580,\n",
       " '1985': 581,\n",
       " 'promised': 582,\n",
       " 'anonymity': 583,\n",
       " 'acknowledged': 584,\n",
       " 'planned': 585,\n",
       " 'francisco': 586,\n",
       " 'vice': 587,\n",
       " 'swiss': 588,\n",
       " 'development': 589,\n",
       " 'project': 590,\n",
       " 'anything': 591,\n",
       " 'proposals': 592,\n",
       " 'believed': 593,\n",
       " 'speaking': 594,\n",
       " 'sources': 595,\n",
       " 'secret': 596,\n",
       " 'thing': 597,\n",
       " 'person': 598,\n",
       " 'town': 599,\n",
       " 'comment': 600,\n",
       " 'gunman': 601,\n",
       " 'hostage': 602,\n",
       " 'case': 603,\n",
       " 'produced': 604,\n",
       " 'refused': 605,\n",
       " 'receipts': 606,\n",
       " 'cash': 607,\n",
       " 'hands': 608,\n",
       " 'moved': 609,\n",
       " 'getting': 610,\n",
       " 'due': 611,\n",
       " 'oct': 612,\n",
       " 'todays': 613,\n",
       " 'stock': 614,\n",
       " 'exchange': 615,\n",
       " 'thousands': 616,\n",
       " 'turkey': 617,\n",
       " 'men': 618,\n",
       " 'convicted': 619,\n",
       " 'women': 620,\n",
       " '1986': 621,\n",
       " 'right': 622,\n",
       " '23': 623,\n",
       " 'arrived': 624,\n",
       " 'actor': 625,\n",
       " 'valentines': 626,\n",
       " 'send': 627,\n",
       " '50': 628,\n",
       " '14': 629,\n",
       " 'law': 630,\n",
       " 'tax': 631,\n",
       " 'payments': 632,\n",
       " 'sanctions': 633,\n",
       " 'yet': 634,\n",
       " 'pressure': 635,\n",
       " 'remove': 636,\n",
       " 'white': 637,\n",
       " 'provide': 638,\n",
       " 'immediate': 639,\n",
       " 'troops': 640,\n",
       " 'living': 641,\n",
       " 'view': 642,\n",
       " 'sent': 643,\n",
       " 'weeks': 644,\n",
       " 'congress': 645,\n",
       " 'soldiers': 646,\n",
       " 'established': 647,\n",
       " 'seven': 648,\n",
       " 'south': 649,\n",
       " 'retail': 650,\n",
       " 'required': 651,\n",
       " 'animals': 652,\n",
       " 'illegal': 653,\n",
       " 'especially': 654,\n",
       " 'candidate': 655,\n",
       " 'michael': 656,\n",
       " 'nominee': 657,\n",
       " 'votes': 658,\n",
       " 'nearly': 659,\n",
       " 'leadership': 660,\n",
       " '1982': 661,\n",
       " 'percentage': 662,\n",
       " 'jobs': 663,\n",
       " 'lived': 664,\n",
       " 'camps': 665,\n",
       " 'eastern': 666,\n",
       " 'sang': 667,\n",
       " 'drood': 668,\n",
       " 'daughter': 669,\n",
       " 'johnson': 670,\n",
       " 'strong': 671,\n",
       " 'gains': 672,\n",
       " 'education': 673,\n",
       " 'strength': 674,\n",
       " 'attention': 675,\n",
       " 'society': 676,\n",
       " 'note': 677,\n",
       " 'considering': 678,\n",
       " 'factories': 679,\n",
       " 'utilities': 680,\n",
       " 'nine': 681,\n",
       " 'likely': 682,\n",
       " 'board': 683,\n",
       " 'capacity': 684,\n",
       " '05': 685,\n",
       " 'reflecting': 686,\n",
       " 'light': 687,\n",
       " 'equipment': 688,\n",
       " 'particularly': 689,\n",
       " 'lower': 690,\n",
       " 'declined': 691,\n",
       " 'output': 692,\n",
       " 'total': 693,\n",
       " 'bites': 694,\n",
       " 'university': 695,\n",
       " 'although': 696,\n",
       " 'tour': 697,\n",
       " 'mind': 698,\n",
       " 'wont': 699,\n",
       " 'double': 700,\n",
       " 'visited': 701,\n",
       " 'un': 702,\n",
       " 'final': 703,\n",
       " 'crowd': 704,\n",
       " 'deep': 705,\n",
       " 'holding': 706,\n",
       " 'calling': 707,\n",
       " 'immediately': 708,\n",
       " 'interview': 709,\n",
       " 'current': 710,\n",
       " 'reduction': 711,\n",
       " 'quarter': 712,\n",
       " 'among': 713,\n",
       " 'income': 714,\n",
       " 'test': 715,\n",
       " 'manager': 716,\n",
       " 'hear': 717,\n",
       " 'find': 718,\n",
       " 'space': 719,\n",
       " 'jim': 720,\n",
       " 'previous': 721,\n",
       " 'stations': 722,\n",
       " 'turned': 723,\n",
       " 'garbo': 724,\n",
       " 'finally': 725,\n",
       " 'book': 726,\n",
       " 'tells': 727,\n",
       " 'cuts': 728,\n",
       " 'assistance': 729,\n",
       " 'largely': 730,\n",
       " 'named': 731,\n",
       " 'march': 732,\n",
       " 'needs': 733,\n",
       " 'measures': 734,\n",
       " 'signed': 735,\n",
       " 'boston': 736,\n",
       " 'large': 737,\n",
       " 'lawsuit': 738,\n",
       " 'arab': 739,\n",
       " 'm1s': 740,\n",
       " 'primarily': 741,\n",
       " 'considered': 742,\n",
       " 'weapons': 743,\n",
       " 'bring': 744,\n",
       " 'weve': 745,\n",
       " 'sandinista': 746,\n",
       " 'governments': 747,\n",
       " 'agreed': 748,\n",
       " 'bermudez': 749,\n",
       " 'civilian': 750,\n",
       " 'issued': 751,\n",
       " 'havana': 752,\n",
       " 'mexico': 753,\n",
       " 'stalin': 754,\n",
       " 'massacre': 755,\n",
       " 'blamed': 756,\n",
       " 'captured': 757,\n",
       " 'beginning': 758,\n",
       " 'east': 759,\n",
       " 'jaruzelski': 760,\n",
       " 'regret': 761,\n",
       " '25': 762,\n",
       " 'live': 763,\n",
       " 'changes': 764,\n",
       " 'angeles': 765,\n",
       " 'sorgenti': 766,\n",
       " 'exploded': 767,\n",
       " 'lose': 768,\n",
       " 'pounds': 769,\n",
       " 'benefits': 770,\n",
       " 'street': 771,\n",
       " 'flying': 772,\n",
       " 'aircraft': 773,\n",
       " 'homeless': 774,\n",
       " 'cattle': 775,\n",
       " 'trees': 776,\n",
       " 'el': 777,\n",
       " 'harvard': 778,\n",
       " 'armed': 779,\n",
       " 'radioactive': 780,\n",
       " 'nuclear': 781,\n",
       " 'nelson': 782,\n",
       " 'ctk': 783,\n",
       " 'dramatic': 784,\n",
       " 'june': 785,\n",
       " 'regular': 786,\n",
       " 'unleaded': 787,\n",
       " 'grew': 788,\n",
       " 'hispanics': 789,\n",
       " 'fbis': 790,\n",
       " 'testified': 791,\n",
       " 'petition': 792,\n",
       " 'film': 793,\n",
       " 'student': 794,\n",
       " 'private': 795,\n",
       " 'arrested': 796,\n",
       " 'relatives': 797,\n",
       " 'save': 798,\n",
       " 'troubled': 799,\n",
       " 'guns': 800,\n",
       " 'always': 801,\n",
       " '32': 802,\n",
       " 'marino': 803,\n",
       " 'wounds': 804,\n",
       " 'running': 805,\n",
       " 'glass': 806,\n",
       " 'locked': 807,\n",
       " 'carried': 808,\n",
       " 'adding': 809,\n",
       " 'bureau': 810,\n",
       " '500': 811,\n",
       " 'body': 812,\n",
       " 'ms': 813,\n",
       " 'hour': 814,\n",
       " 'sell': 815,\n",
       " 'bomb': 816,\n",
       " 'assurances': 817,\n",
       " 'run': 818,\n",
       " 'include': 819,\n",
       " 'israeli': 820,\n",
       " 'post': 821,\n",
       " 'close': 822,\n",
       " 'keep': 823,\n",
       " 'sees': 824,\n",
       " 'every': 825,\n",
       " 'attorney': 826,\n",
       " 'meese': 827,\n",
       " 'referred': 828,\n",
       " 'arrangement': 829,\n",
       " 'receive': 830,\n",
       " 'friend': 831,\n",
       " 'indicated': 832,\n",
       " 'able': 833,\n",
       " 'attempt': 834,\n",
       " 'margaret': 835,\n",
       " 'sat': 836,\n",
       " 'give': 837,\n",
       " 'entertainer': 838,\n",
       " 'wanted': 839,\n",
       " 'worked': 840,\n",
       " 'closing': 841,\n",
       " 'wasnt': 842,\n",
       " 'seemed': 843,\n",
       " 'walked': 844,\n",
       " 'collapsed': 845,\n",
       " 'las': 846,\n",
       " 'record': 847,\n",
       " 'systems': 848,\n",
       " 'founded': 849,\n",
       " '1988': 850,\n",
       " 'local': 851,\n",
       " 'william': 852,\n",
       " 'drew': 853,\n",
       " 'launched': 854,\n",
       " 'television': 855,\n",
       " 'india': 856,\n",
       " '43': 857,\n",
       " 'richard': 858,\n",
       " 'actress': 859,\n",
       " 'modern': 860,\n",
       " 'word': 861,\n",
       " 'verse': 862,\n",
       " 'estimated': 863,\n",
       " 'latest': 864,\n",
       " 'perhaps': 865,\n",
       " 'recently': 866,\n",
       " 'takes': 867,\n",
       " 'born': 868,\n",
       " '300': 869,\n",
       " '7': 870,\n",
       " 'charge': 871,\n",
       " 'touch': 872,\n",
       " 'examining': 873,\n",
       " 'bear': 874,\n",
       " 'situation': 875,\n",
       " 'corporations': 876,\n",
       " 'act': 877,\n",
       " 'cautious': 878,\n",
       " 'army': 879,\n",
       " 'believes': 880,\n",
       " 'jesse': 881,\n",
       " 'jacksons': 882,\n",
       " 'despite': 883,\n",
       " 'communications': 884,\n",
       " 'channels': 885,\n",
       " 'tactic': 886,\n",
       " 'buy': 887,\n",
       " 'available': 888,\n",
       " 'traveling': 889,\n",
       " 'california': 890,\n",
       " 'candidates': 891,\n",
       " 'dealing': 892,\n",
       " 'earned': 893,\n",
       " 'plans': 894,\n",
       " 'try': 895,\n",
       " '10000': 896,\n",
       " 'probably': 897,\n",
       " 'evidence': 898,\n",
       " 'jose': 899,\n",
       " 'offices': 900,\n",
       " 'costa': 901,\n",
       " 'step': 902,\n",
       " 'renewed': 903,\n",
       " 'leave': 904,\n",
       " 'remain': 905,\n",
       " 'caiman': 906,\n",
       " 'involving': 907,\n",
       " 'thailand': 908,\n",
       " 'onto': 909,\n",
       " 'cites': 910,\n",
       " 'allowed': 911,\n",
       " 'industry': 912,\n",
       " 'unless': 913,\n",
       " 'improvement': 914,\n",
       " 'organized': 915,\n",
       " 'boost': 916,\n",
       " 'single': 917,\n",
       " 'almost': 918,\n",
       " 'appears': 919,\n",
       " 'fires': 920,\n",
       " 'le': 921,\n",
       " 'pen': 922,\n",
       " 'automobile': 923,\n",
       " 'paris': 924,\n",
       " '1981': 925,\n",
       " '30': 926,\n",
       " 'elections': 927,\n",
       " 'district': 928,\n",
       " 'citizens': 929,\n",
       " 'affected': 930,\n",
       " 'homes': 931,\n",
       " 'southwest': 932,\n",
       " 'newspapers': 933,\n",
       " 'standards': 934,\n",
       " 'tony': 935,\n",
       " 'fair': 936,\n",
       " 'jack': 937,\n",
       " 'failed': 938,\n",
       " 'ever': 939,\n",
       " 'planet': 940,\n",
       " 'dogs': 941,\n",
       " 'anyone': 942,\n",
       " 'huge': 943,\n",
       " 'success': 944,\n",
       " 'democrats': 945,\n",
       " 'known': 946,\n",
       " 'social': 947,\n",
       " 'race': 948,\n",
       " 'popular': 949,\n",
       " 'collective': 950,\n",
       " 'job': 951,\n",
       " 'iraqs': 952,\n",
       " 'concluded': 953,\n",
       " 'mines': 954,\n",
       " 'reserve': 955,\n",
       " 'points': 956,\n",
       " 'eight': 957,\n",
       " 'economists': 958,\n",
       " 'leading': 959,\n",
       " 'truck': 960,\n",
       " 'rates': 961,\n",
       " 'december': 962,\n",
       " 'parts': 963,\n",
       " 'consecutive': 964,\n",
       " 'includes': 965,\n",
       " 'reserves': 966,\n",
       " 'accompanied': 967,\n",
       " 'stronger': 968,\n",
       " '06': 969,\n",
       " 'stood': 970,\n",
       " 'increasingly': 971,\n",
       " 'pets': 972,\n",
       " 'attacks': 973,\n",
       " 'bitten': 974,\n",
       " '39': 975,\n",
       " 'claim': 976,\n",
       " 'buying': 977,\n",
       " 'injuries': 978,\n",
       " 'described': 979,\n",
       " 'arizona': 980,\n",
       " 'population': 981,\n",
       " '03': 982,\n",
       " 'georgia': 983,\n",
       " 'ceremony': 984,\n",
       " 'stop': 985,\n",
       " 'return': 986,\n",
       " 'means': 987,\n",
       " 'longer': 988,\n",
       " 'crystal': 989,\n",
       " 'vocal': 990,\n",
       " 'ford': 991,\n",
       " 'consider': 992,\n",
       " 'normally': 993,\n",
       " 'husband': 994,\n",
       " 'received': 995,\n",
       " 'felt': 996,\n",
       " 'nervous': 997,\n",
       " 'ended': 998,\n",
       " 'referendum': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model_.word2Idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Domains Affect Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg time: 5.7178974151611327e-05\n",
      "torch.Size([1000, 1])\n",
      "avg time: 2.7730941772460937e-05\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "w1 = nn.Parameter(torch.randn(1000, 100).float(), requires_grad=True)\n",
    "w2 = nn.Parameter(torch.randn(1000,  100).float(), requires_grad=True)\n",
    "nIters = 1000\n",
    "negSampleSize = 1000\n",
    "start = time.time()\n",
    "for i in range(1000):\n",
    "    temp = torch.mm(w2[0:negSampleSize], torch.t(w2[0].view(1, -1)))\n",
    "print(\"avg time: {}\".format((time.time() - start)/nIters))\n",
    "print(temp.shape)\n",
    "\n",
    "\n",
    "\n",
    "w1 = w1.data.numpy()\n",
    "w2 = w2.data.numpy()\n",
    "start = time.time()\n",
    "for i in range(1000):\n",
    "    temp = np.matmul(w2[0:negSampleSize], w1[0])\n",
    "print(\"avg time: {}\".format((time.time() - start)/nIters))\n",
    "print(temp.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg time: 1.8839120864868163e-05\n",
      "torch.Size([15, 1])\n"
     ]
    }
   ],
   "source": [
    "w1 = nn.Parameter(torch.randn(1000, 100).float(), requires_grad=True)\n",
    "w2 = nn.Parameter(torch.randn(1000,  100).float(), requires_grad=True)\n",
    "nIters = 1000\n",
    "negSampleSize = 15\n",
    "start = time.time()\n",
    "for i in range(1000):\n",
    "    temp = torch.mm(w2[0:negSampleSize], torch.t(w2[0].view(1, -1)))\n",
    "print(\"avg time: {}\".format((time.time() - start)/nIters))\n",
    "print(temp.shape)\n",
    "\n",
    "\n",
    "\n",
    "# w1 = w1.data.numpy()\n",
    "# w2 = w2.data.numpy()\n",
    "# start = time.time()\n",
    "# for i in range(1000):\n",
    "#     temp = np.matmul(w2[0:negSampleSize], w1[0])\n",
    "# print(\"avg time: {}\".format((time.time() - start)/nIters))\n",
    "# print(temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = w1.data.numpy()\n",
    "w2 = w2.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15,)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(w2[0:15], w1[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['first',\n",
       "  'quit',\n",
       "  'grinnin',\n",
       "  'like',\n",
       "  'idiot',\n",
       "  'indians',\n",
       "  'aint',\n",
       "  'supposed',\n",
       "  'smile',\n",
       "  'like',\n",
       "  'get',\n",
       "  'stoic'],\n",
       " ['like', 'gotta', 'look', 'mean', 'people', 'wont', 'respect'],\n",
       " ['people', 'run', 'dont', 'look', 'mean'],\n",
       " ['gotta',\n",
       "  'look',\n",
       "  'like',\n",
       "  'warrior',\n",
       "  'gotta',\n",
       "  'look',\n",
       "  'like',\n",
       "  'came',\n",
       "  'back',\n",
       "  'killing',\n",
       "  'buffalo'],\n",
       " ['tribe',\n",
       "  'never',\n",
       "  'hunted',\n",
       "  'buffalo',\n",
       "  'fishermenwhat',\n",
       "  'wanna',\n",
       "  'look',\n",
       "  'like',\n",
       "  'came',\n",
       "  'back',\n",
       "  'catching',\n",
       "  'fish'],\n",
       " ['aint',\n",
       "  'dances',\n",
       "  'salmon',\n",
       "  'know',\n",
       "  'thomas',\n",
       "  'gotta',\n",
       "  'look',\n",
       "  'like',\n",
       "  'warrior']]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalTokenizedCorpus_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateObservations(tokenizedCorpus, word2Idx):\n",
    "    window_size = 2\n",
    "    idxPairs = []\n",
    "    # for each sentence\n",
    "    for sentence in tokenizedCorpus:\n",
    "#         indices = [word2Idx[word] for word in sentence]\n",
    "        # for each word, threated as center word\n",
    "        for center_word_pos in range(len(sentence)):\n",
    "            # for each window position\n",
    "            for w in range(-window_size, window_size + 1):\n",
    "                context_word_pos = center_word_pos + w\n",
    "                # make soure not jump out sentence\n",
    "                if context_word_pos < 0 or context_word_pos >= len(sentence) or center_word_pos == context_word_pos:\n",
    "                    continue\n",
    "                idxPairs.append((sentence[center_word_pos], sentence[context_word_pos]))\n",
    "\n",
    "    idxPairs = np.array(idxPairs) # it will be useful to have this as numpy array\n",
    "    return(idxPairs)\n",
    "\n",
    "\n",
    "def generateWordSamplingProb(vocabCount, word2Idx):\n",
    "    wordSampleProbs = [0.0]*len(vocabCount)\n",
    "    numWords = np.sum([count**0.75 for word, count in vocabCount])\n",
    "    for idx in range(len(vocabCount)):\n",
    "        w,c = vocabCount[idx]\n",
    "        wordSampleProbs[word2Idx[w]] = (c**0.75)/(numWords)\n",
    "        \n",
    "        \n",
    "        \n",
    "    wordSampleProbs = []\n",
    "    numWords = np.sum([count for word, count in vocabCount])\n",
    "    for w,c in vocabCount:\n",
    "#         w,c = vocabCount[idx]\n",
    "        wordSampleProbs.extend([word2Idx[w]] * int(((c/numWords)**0.75)/0.001))\n",
    "    return(wordSampleProbs)\n",
    "    \n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocabSize, embedSize, vocabCount, word2Idx):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.vocabSize = vocabSize\n",
    "        self.word2Idx = word2Idx\n",
    "#         self.centerEmbeddings = nn.Embedding(vocab_size, embd_size)\n",
    "#         self.contextEmbeddings = nn.Embedding(vocab_size, embd_size)\n",
    "#         self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
    "        \n",
    "        self.centerEmbeddings = nn.Parameter(torch.randn(vocabSize,\n",
    "                                                     embedSize).float(), requires_grad=True)\n",
    "        self.contextEmbeddings = nn.Parameter(torch.randn(vocabSize,\n",
    "                                                      embedSize).float(), requires_grad=True)\n",
    "        \n",
    "#         initrange = (2.0 / (vocabSize + embedSize)) ** 0.5  # Xavier init\n",
    "        nn.init.xavier_uniform_(self.contextEmbeddings)\n",
    "        nn.init.xavier_uniform_(self.centerEmbeddings)\n",
    "        \n",
    "        self.wordSampleProbs = generateWordSamplingProb(vocabCount, word2Idx)\n",
    "        self.logSigmoid = nn.LogSigmoid()\n",
    "#         self.paramList = nn.ModuleList([self.centerEmbeddings, self.contextEmbeddings] )\n",
    "    def getNegSample(self, k, centerWords):\n",
    "        vocabSizeWithoutUnk = self.vocabSize - 1\n",
    "#         negSample = np.random.choice(vocabSizeWithoutUnk,\n",
    "#                                      size = k, replace = True, p = self.wordSampleProbs)\n",
    "        negSamples = []\n",
    "        for centerWord in centerWords:\n",
    "        negSample = random.sample(self.wordSampleProbs, k)\n",
    "            while self.word2Idx[centerWord] in negSample:\n",
    "    #             negSample = np.random.choice(vocabSizeWithoutUnk,\n",
    "    #                                          size = k, replace = True, p = self.wordSampleProbs)\n",
    "                negSample = random.sample(self.wordSampleProbs, k)\n",
    "                negSamples.append(negSample)\n",
    "        return(negSamples)\n",
    "    def forward(self, center, context, negSampleIndices = None):\n",
    "#         focus = torch.autograd.Variable(torch.LongTensor([0]))\n",
    "#         context = torch.autograd.Variable(torch.LongTensor([0]))\n",
    "#         allEmbeddingIdxs = torch.autograd.Variable(torch.LongTensor([np.arange(0,self.vocabSize)]))\n",
    "\n",
    "\n",
    "#         embedCenter = self.centerEmbeddings(center).view((1, -1))\n",
    "#         embedContext = self.contextEmbeddings(context).view((1, -1))\n",
    "# #         print(allEmbeddingIdxs)\n",
    "#         allContextEmbeddings = self.contextEmbeddings(allEmbeddingIdxs).squeeze()\n",
    "#         num = torch.exp(torch.mm(embedContext, torch.t(embedCenter)))\n",
    "#         denom = torch.exp(torch.mm(allContextEmbeddings, torch.t(embedCenter))).sum()\n",
    "#         logProb = torch.log(num/denom)\n",
    "        embedCenter = self.centerEmbeddings[center].view((1, -1))\n",
    "        embedContext = self.contextEmbeddings[context].view((1, -1))       \n",
    "        if negSampleIndices is not None:\n",
    "#             print(\"hey\")\n",
    "#             posVal = self.logSigmoid(torch.mm(embedContext, torch.t(embedCenter)))\n",
    "#             print(posVal)\n",
    "            posVal = self.logSigmoid(torch.sum(embedContext * embedCenter, dim = 1)).squeeze()\n",
    "#             print(posVal)\n",
    "#             start = time.time()\n",
    "#             for i in range(1000):\n",
    "            negVal = torch.bmm(self.contextEmbeddings[negSampleIndices], embedCenter.unsqueeze(2)).squeeze(2)\n",
    "            negVal = self.logSigmoid(-torch.sum(negVal, dim = 1)).squeeze()\n",
    "#             print(\"avg time: {}\".format((time.time() - start)/100))\n",
    "#             print(torch.mm(self.contextEmbeddings[negSampleIndices], torch.t(embedCenter)).shape)\n",
    "#             1/0\n",
    "            logProb = (posVal + negVal).mean()\n",
    "        else:\n",
    "#             allEmbeddingIdxs = torch.autograd.Variable(torch.LongTensor([np.arange(0,self.vocabSize)]))\n",
    "\n",
    "\n",
    "\n",
    "    #         print(allEmbeddingIdxs)\n",
    "    #         allContextEmbeddings = self.contextEmbeddings(allEmbeddingIdxs).squeeze()\n",
    "            num = torch.exp(torch.mm(embedContext, torch.t(embedCenter)))\n",
    "#             start = time.time()\n",
    "#             for i in range(1000):\n",
    "            denom = torch.exp(torch.mm(self.contextEmbeddings, torch.t(embedCenter))).sum()\n",
    "#             print(\"avg time: {}\".format((time.time() - start)/100))\n",
    "#             print(torch.exp(torch.mm(self.contextEmbeddings, torch.t(embedCenter))).shape)\n",
    "#             1/0\n",
    "            logProb = torch.log(num/denom)\n",
    "#         print(logProb)\n",
    "        return(logProb)\n",
    "\n",
    "\n",
    "def train_skipgram(embeddingSize, trainingData, vocabCount, word2Idx, idx2Word, k, referenceWords):\n",
    "    print(\"training on {} observations\".format(len(trainingData)))\n",
    "    losses = []\n",
    "    loss_fn = nn.MSELoss()\n",
    "    model = SkipGram(vocabSize = len(word2Idx), embedSize = embeddingSize,\n",
    "                     vocabCount = vocabCount, word2Idx = word2Idx)\n",
    "#     print(model)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    listNearestWords(model = model, idx2Word = idx2Word,\n",
    "     referenceWords = referenceWords, topN = 5)\n",
    "    batchSize = 32\n",
    "    for epoch in tqdm_notebook(range(n_epoch), position = 0):\n",
    "#         listNearestWords(model = model, idx2Word = idx2Word,\n",
    "#                  referenceWords = referenceWords, topN = 5)\n",
    "        total_loss = .0\n",
    "        avgLoss = 0.0\n",
    "        iteration = 0\n",
    "        for step in range(0, len(trainingData), batchSize):\n",
    "#         for in_w, out_w in tqdm_notebook(trainingData, position = 1):\n",
    "            endIdx = np.min((i+batchSize), len(trainingData))\n",
    "            myBatch = trainingData[i:(i+batchSize)]\n",
    "            centerWords = [centerword for centerWord, _ in myBatch]\n",
    "            contextWords = [contextWord for _, contextWord in myBatch]\n",
    "            if k is not None:\n",
    "                negSamples = model.getNegSample(k = k, centerWords = centerWords)\n",
    "            else:\n",
    "                negSamples = None\n",
    "#             print(\"neg samples found\")\n",
    "#             print(negSamples)\n",
    "            centerIDs = [word2Idx[idx] for idx in centerWords]#torch.autograd.Variable(torch.LongTensor([word2Idx[in_w]]))\n",
    "            contextIDs = [word2Idx[idx] for idx in contextWords]#torch.autograd.Variable(torch.LongTensor([word2Idx[out_w]]))\n",
    "#             if in_w in word2Idx:\n",
    "#                 in_w_var = word2Idx[in_w]#torch.autograd.Variable(torch.LongTensor([word2Idx[in_w]]))\n",
    "#             else:\n",
    "#                 in_w_var = word2Idx[\"<UNK>\"]\n",
    "#             if out_w in word2Idx:\n",
    "#                 out_w_var = word2Idx[out_w]#torch.autograd.Variable(torch.LongTensor([word2Idx[out_w]]))\n",
    "#             else:\n",
    "#                 out_w_var = word2Idx[\"<UNK>\"]\n",
    "            \n",
    "            model.zero_grad()\n",
    "            log_probs = model(centerIDs, contextIDs, negSampleIndices = negSamples)\n",
    "            loss = -log_probs#loss_fn(log_probs[0], torch.autograd.Variable(torch.Tensor([1])))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.data.numpy()\n",
    "            avgLoss += loss.data.numpy()\n",
    "            iteration += 1\n",
    "            if iteration % 10000 == 0:\n",
    "                avgLoss = total_loss/(iteration)\n",
    "                print(\"avg loss: {}\".format(avgLoss))\n",
    "                avgLoss = 0.0\n",
    "            if iteration % 20000 == 0:\n",
    "                listNearestWords(model = model, idx2Word = idx2Word,\n",
    "                 referenceWords = referenceWords, topN = 5)\n",
    "        losses.append(total_loss)    \n",
    "        print(f'Loss at epoch {epoch}: {total_loss/len(trainingData)}')\n",
    "        listNearestWords(model = model, idx2Word = idx2Word,\n",
    "                     referenceWords = referenceWords, topN = 5)\n",
    "    return(model, losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Computational Methods)",
   "language": "python",
   "name": "compmeth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
