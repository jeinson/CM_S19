{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lab 10: Word Embeddings\n",
    "Thinking of using stuff from here\n",
    "https://gist.github.com/mbednarski/da08eb297304f7a66a3840e857e060a0\n",
    "\n",
    "conda install -c conda-forge tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Janitorial Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCorpus = [\"First of all, quit grinnin’ like an idiot. Indians ain’t supposed to smile like that. Get stoic.\",\n",
    "             \"No. Like this. You gotta look mean, or people won’t respect you.\",\n",
    "              \" people will run all over you if you don’t look mean.\",\n",
    "              \"You gotta look like a warrior. You gotta look like you just came back from killing a buffalo.\",\n",
    "             \"But our tribe never hunted buffalo. We were fishermen.\"\n",
    "             \"What? You wanna look like you just came back from catching a fish?\",\n",
    "             \"This ain’t dances with salmon, you know. Thomas, you gotta look like a warrior.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1767 pub med abstracts\n"
     ]
    }
   ],
   "source": [
    "# Read in pubmed corpus into a text file\n",
    "import glob\n",
    "pubMedDataFolderPath = \"data/pubMed_corpus/\"\n",
    "pubMedDataFiles = glob.glob(pubMedDataFolderPath + \"*.txt\")\n",
    "pubMedCorpus = [\"\"]*len(pubMedDataFiles)\n",
    "for idx, pubMedDataPath in enumerate(pubMedDataFiles):\n",
    "    with open(pubMedDataPath, \"r\") as pubMedFile:\n",
    "        text = pubMedFile.read().strip()\n",
    "        pubMedCorpus[idx] = text\n",
    "print(\"{} pub med abstracts\".format(len(pubMedCorpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2246 ap articles\n"
     ]
    }
   ],
   "source": [
    "# Read in the ap corpus\n",
    "apTextFile = \"data/ap.txt\"\n",
    "apCorpus = []\n",
    "readText = False\n",
    "with open(apTextFile) as apDataFile:\n",
    "    for line in apDataFile:\n",
    "        if readText:\n",
    "            apCorpus.append(line.strip())\n",
    "            readText = False\n",
    "        if line == \"<TEXT>\\n\":\n",
    "            readText = True\n",
    "print(\"{} ap articles\".format(len(apCorpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def removePunctuation(myStr):\n",
    "    excludedCharacters = string.punctuation + \"’\"\n",
    "    newStr = \"\".join(char for char in myStr if char not in excludedCharacters)\n",
    "    return(newStr)\n",
    "def tokenize_corpus(corpus):\n",
    "    tokens = [removePunctuation(x).split() for x in corpus]\n",
    "    return tokens\n",
    "\n",
    "apCorpusTokenized = tokenize_corpus(apCorpus)\n",
    "pubMedCorpusTokenized = tokenize_corpus(pubMedCorpus)\n",
    "testCorpusTokenized = tokenize_corpus(testCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 16/2246 [00:00<00:14, 157.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building ap corpus vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2246/2246 [02:34<00:00, 12.87it/s]\n",
      "  2%|▏         | 39/1767 [00:00<00:04, 382.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building pubMed corpus vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1767/1767 [00:27<00:00, 63.72it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 16090.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building test corpus vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "def extractVocabMappers(tokenizedCorpus):\n",
    "    vocabulary = []\n",
    "    for sentence in tqdm(tokenizedCorpus):\n",
    "        for token in sentence:\n",
    "#             print(token)\n",
    "            if token not in vocabulary:\n",
    "                vocabulary.append(token)\n",
    "\n",
    "    word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "    idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "    return(word2idx, idx2word)\n",
    "\n",
    "# start = time.time()\n",
    "print(\"Building ap corpus vocabulary\")\n",
    "word2Idx_ap, idx2Word_ap = extractVocabMappers(apCorpusTokenized)\n",
    "# print(\"ap data tokenized in {} seconds\".format(time.time() - start))\n",
    "# start = time.time()\n",
    "print(\"Building pubMed corpus vocabulary\")\n",
    "word2Idx_pubMed, idx2Word_pubMed = extractVocabMappers(pubMedCorpusTokenized)\n",
    "# print(\"pubmed data tokenized in {} seconds\".format(time.time() - start))\n",
    "# start = time.time()\n",
    "print(\"Building test corpus vocabulary\")\n",
    "word2Idx_test, idx2Word_test = extractVocabMappers(testCorpusTokenized)\n",
    "# print(\"test data tokenized in {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['First of all, quit grinnin’ like an idiot. Indians ain’t supposed to smile like that. Get stoic.',\n",
       " 'No. Like this. You gotta look mean, or people won’t respect you.',\n",
       " ' people will run all over you if you don’t look mean.',\n",
       " 'You gotta look like a warrior. You gotta look like you just came back from killing a buffalo.',\n",
       " 'But our tribe never hunted buffalo. We were fishermen.What? You wanna look like you just came back from catching a fish?',\n",
       " 'This ain’t dances with salmon, you know. Thomas, you gotta look like a warrior.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testCorpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['First',\n",
       "  'of',\n",
       "  'all',\n",
       "  'quit',\n",
       "  'grinnin',\n",
       "  'like',\n",
       "  'an',\n",
       "  'idiot',\n",
       "  'Indians',\n",
       "  'aint',\n",
       "  'supposed',\n",
       "  'to',\n",
       "  'smile',\n",
       "  'like',\n",
       "  'that',\n",
       "  'Get',\n",
       "  'stoic'],\n",
       " ['No',\n",
       "  'Like',\n",
       "  'this',\n",
       "  'You',\n",
       "  'gotta',\n",
       "  'look',\n",
       "  'mean',\n",
       "  'or',\n",
       "  'people',\n",
       "  'wont',\n",
       "  'respect',\n",
       "  'you'],\n",
       " ['people',\n",
       "  'will',\n",
       "  'run',\n",
       "  'all',\n",
       "  'over',\n",
       "  'you',\n",
       "  'if',\n",
       "  'you',\n",
       "  'dont',\n",
       "  'look',\n",
       "  'mean'],\n",
       " ['You',\n",
       "  'gotta',\n",
       "  'look',\n",
       "  'like',\n",
       "  'a',\n",
       "  'warrior',\n",
       "  'You',\n",
       "  'gotta',\n",
       "  'look',\n",
       "  'like',\n",
       "  'you',\n",
       "  'just',\n",
       "  'came',\n",
       "  'back',\n",
       "  'from',\n",
       "  'killing',\n",
       "  'a',\n",
       "  'buffalo'],\n",
       " ['But',\n",
       "  'our',\n",
       "  'tribe',\n",
       "  'never',\n",
       "  'hunted',\n",
       "  'buffalo',\n",
       "  'We',\n",
       "  'were',\n",
       "  'fishermenWhat',\n",
       "  'You',\n",
       "  'wanna',\n",
       "  'look',\n",
       "  'like',\n",
       "  'you',\n",
       "  'just',\n",
       "  'came',\n",
       "  'back',\n",
       "  'from',\n",
       "  'catching',\n",
       "  'a',\n",
       "  'fish'],\n",
       " ['This',\n",
       "  'aint',\n",
       "  'dances',\n",
       "  'with',\n",
       "  'salmon',\n",
       "  'you',\n",
       "  'know',\n",
       "  'Thomas',\n",
       "  'you',\n",
       "  'gotta',\n",
       "  'look',\n",
       "  'like',\n",
       "  'a',\n",
       "  'warrior']]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testCorpusTokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateObservations(tokenizedCorpus, word2Idx):\n",
    "    window_size = 2\n",
    "    idxPairs = []\n",
    "    # for each sentence\n",
    "    for sentence in tokenizedCorpus:\n",
    "        indices = [word2Idx[word] for word in sentence]\n",
    "        # for each word, threated as center word\n",
    "        for center_word_pos in range(len(sentence)):\n",
    "            # for each window position\n",
    "            for w in range(-window_size, window_size + 1):\n",
    "                context_word_pos = center_word_pos + w\n",
    "                # make soure not jump out sentence\n",
    "                if context_word_pos < 0 or context_word_pos >= len(sentence) or center_word_pos == context_word_pos:\n",
    "                    continue\n",
    "                idxPairs.append((sentence[center_word_pos], sentence[context_word_pos]))\n",
    "\n",
    "    idxPairs = np.array(idxPairs) # it will be useful to have this as numpy array\n",
    "    return(idxPairs)\n",
    "idxPairsTest = generateObservations(tokenizedCorpus = testCorpusTokenized, word2Idx = word2Idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['First', 'of'],\n",
       "       ['First', 'all'],\n",
       "       ['of', 'First'],\n",
       "       ['of', 'all'],\n",
       "       ['of', 'quit'],\n",
       "       ['all', 'First'],\n",
       "       ['all', 'of'],\n",
       "       ['all', 'quit'],\n",
       "       ['all', 'grinnin'],\n",
       "       ['quit', 'of'],\n",
       "       ['quit', 'all'],\n",
       "       ['quit', 'grinnin'],\n",
       "       ['quit', 'like'],\n",
       "       ['grinnin', 'all'],\n",
       "       ['grinnin', 'quit'],\n",
       "       ['grinnin', 'like'],\n",
       "       ['grinnin', 'an'],\n",
       "       ['like', 'quit'],\n",
       "       ['like', 'grinnin'],\n",
       "       ['like', 'an'],\n",
       "       ['like', 'idiot'],\n",
       "       ['an', 'grinnin'],\n",
       "       ['an', 'like'],\n",
       "       ['an', 'idiot'],\n",
       "       ['an', 'Indians'],\n",
       "       ['idiot', 'like'],\n",
       "       ['idiot', 'an'],\n",
       "       ['idiot', 'Indians'],\n",
       "       ['idiot', 'aint'],\n",
       "       ['Indians', 'an'],\n",
       "       ['Indians', 'idiot'],\n",
       "       ['Indians', 'aint'],\n",
       "       ['Indians', 'supposed'],\n",
       "       ['aint', 'idiot'],\n",
       "       ['aint', 'Indians'],\n",
       "       ['aint', 'supposed'],\n",
       "       ['aint', 'to'],\n",
       "       ['supposed', 'Indians'],\n",
       "       ['supposed', 'aint'],\n",
       "       ['supposed', 'to'],\n",
       "       ['supposed', 'smile'],\n",
       "       ['to', 'aint'],\n",
       "       ['to', 'supposed'],\n",
       "       ['to', 'smile'],\n",
       "       ['to', 'like'],\n",
       "       ['smile', 'supposed'],\n",
       "       ['smile', 'to'],\n",
       "       ['smile', 'like'],\n",
       "       ['smile', 'that'],\n",
       "       ['like', 'to'],\n",
       "       ['like', 'smile'],\n",
       "       ['like', 'that'],\n",
       "       ['like', 'Get'],\n",
       "       ['that', 'smile'],\n",
       "       ['that', 'like'],\n",
       "       ['that', 'Get'],\n",
       "       ['that', 'stoic'],\n",
       "       ['Get', 'like'],\n",
       "       ['Get', 'that'],\n",
       "       ['Get', 'stoic'],\n",
       "       ['stoic', 'that'],\n",
       "       ['stoic', 'Get'],\n",
       "       ['No', 'Like'],\n",
       "       ['No', 'this'],\n",
       "       ['Like', 'No'],\n",
       "       ['Like', 'this'],\n",
       "       ['Like', 'You'],\n",
       "       ['this', 'No'],\n",
       "       ['this', 'Like'],\n",
       "       ['this', 'You'],\n",
       "       ['this', 'gotta'],\n",
       "       ['You', 'Like'],\n",
       "       ['You', 'this'],\n",
       "       ['You', 'gotta'],\n",
       "       ['You', 'look'],\n",
       "       ['gotta', 'this'],\n",
       "       ['gotta', 'You'],\n",
       "       ['gotta', 'look'],\n",
       "       ['gotta', 'mean'],\n",
       "       ['look', 'You'],\n",
       "       ['look', 'gotta'],\n",
       "       ['look', 'mean'],\n",
       "       ['look', 'or'],\n",
       "       ['mean', 'gotta'],\n",
       "       ['mean', 'look'],\n",
       "       ['mean', 'or'],\n",
       "       ['mean', 'people'],\n",
       "       ['or', 'look'],\n",
       "       ['or', 'mean'],\n",
       "       ['or', 'people'],\n",
       "       ['or', 'wont'],\n",
       "       ['people', 'mean'],\n",
       "       ['people', 'or'],\n",
       "       ['people', 'wont'],\n",
       "       ['people', 'respect'],\n",
       "       ['wont', 'or'],\n",
       "       ['wont', 'people'],\n",
       "       ['wont', 'respect'],\n",
       "       ['wont', 'you'],\n",
       "       ['respect', 'people'],\n",
       "       ['respect', 'wont'],\n",
       "       ['respect', 'you'],\n",
       "       ['you', 'wont'],\n",
       "       ['you', 'respect'],\n",
       "       ['people', 'will'],\n",
       "       ['people', 'run'],\n",
       "       ['will', 'people'],\n",
       "       ['will', 'run'],\n",
       "       ['will', 'all'],\n",
       "       ['run', 'people'],\n",
       "       ['run', 'will'],\n",
       "       ['run', 'all'],\n",
       "       ['run', 'over'],\n",
       "       ['all', 'will'],\n",
       "       ['all', 'run'],\n",
       "       ['all', 'over'],\n",
       "       ['all', 'you'],\n",
       "       ['over', 'run'],\n",
       "       ['over', 'all'],\n",
       "       ['over', 'you'],\n",
       "       ['over', 'if'],\n",
       "       ['you', 'all'],\n",
       "       ['you', 'over'],\n",
       "       ['you', 'if'],\n",
       "       ['you', 'you'],\n",
       "       ['if', 'over'],\n",
       "       ['if', 'you'],\n",
       "       ['if', 'you'],\n",
       "       ['if', 'dont'],\n",
       "       ['you', 'you'],\n",
       "       ['you', 'if'],\n",
       "       ['you', 'dont'],\n",
       "       ['you', 'look'],\n",
       "       ['dont', 'if'],\n",
       "       ['dont', 'you'],\n",
       "       ['dont', 'look'],\n",
       "       ['dont', 'mean'],\n",
       "       ['look', 'you'],\n",
       "       ['look', 'dont'],\n",
       "       ['look', 'mean'],\n",
       "       ['mean', 'dont'],\n",
       "       ['mean', 'look'],\n",
       "       ['You', 'gotta'],\n",
       "       ['You', 'look'],\n",
       "       ['gotta', 'You'],\n",
       "       ['gotta', 'look'],\n",
       "       ['gotta', 'like'],\n",
       "       ['look', 'You'],\n",
       "       ['look', 'gotta'],\n",
       "       ['look', 'like'],\n",
       "       ['look', 'a'],\n",
       "       ['like', 'gotta'],\n",
       "       ['like', 'look'],\n",
       "       ['like', 'a'],\n",
       "       ['like', 'warrior'],\n",
       "       ['a', 'look'],\n",
       "       ['a', 'like'],\n",
       "       ['a', 'warrior'],\n",
       "       ['a', 'You'],\n",
       "       ['warrior', 'like'],\n",
       "       ['warrior', 'a'],\n",
       "       ['warrior', 'You'],\n",
       "       ['warrior', 'gotta'],\n",
       "       ['You', 'a'],\n",
       "       ['You', 'warrior'],\n",
       "       ['You', 'gotta'],\n",
       "       ['You', 'look'],\n",
       "       ['gotta', 'warrior'],\n",
       "       ['gotta', 'You'],\n",
       "       ['gotta', 'look'],\n",
       "       ['gotta', 'like'],\n",
       "       ['look', 'You'],\n",
       "       ['look', 'gotta'],\n",
       "       ['look', 'like'],\n",
       "       ['look', 'you'],\n",
       "       ['like', 'gotta'],\n",
       "       ['like', 'look'],\n",
       "       ['like', 'you'],\n",
       "       ['like', 'just'],\n",
       "       ['you', 'look'],\n",
       "       ['you', 'like'],\n",
       "       ['you', 'just'],\n",
       "       ['you', 'came'],\n",
       "       ['just', 'like'],\n",
       "       ['just', 'you'],\n",
       "       ['just', 'came'],\n",
       "       ['just', 'back'],\n",
       "       ['came', 'you'],\n",
       "       ['came', 'just'],\n",
       "       ['came', 'back'],\n",
       "       ['came', 'from'],\n",
       "       ['back', 'just'],\n",
       "       ['back', 'came'],\n",
       "       ['back', 'from'],\n",
       "       ['back', 'killing'],\n",
       "       ['from', 'came'],\n",
       "       ['from', 'back'],\n",
       "       ['from', 'killing'],\n",
       "       ['from', 'a'],\n",
       "       ['killing', 'back'],\n",
       "       ['killing', 'from'],\n",
       "       ['killing', 'a'],\n",
       "       ['killing', 'buffalo'],\n",
       "       ['a', 'from'],\n",
       "       ['a', 'killing'],\n",
       "       ['a', 'buffalo'],\n",
       "       ['buffalo', 'killing'],\n",
       "       ['buffalo', 'a'],\n",
       "       ['But', 'our'],\n",
       "       ['But', 'tribe'],\n",
       "       ['our', 'But'],\n",
       "       ['our', 'tribe'],\n",
       "       ['our', 'never'],\n",
       "       ['tribe', 'But'],\n",
       "       ['tribe', 'our'],\n",
       "       ['tribe', 'never'],\n",
       "       ['tribe', 'hunted'],\n",
       "       ['never', 'our'],\n",
       "       ['never', 'tribe'],\n",
       "       ['never', 'hunted'],\n",
       "       ['never', 'buffalo'],\n",
       "       ['hunted', 'tribe'],\n",
       "       ['hunted', 'never'],\n",
       "       ['hunted', 'buffalo'],\n",
       "       ['hunted', 'We'],\n",
       "       ['buffalo', 'never'],\n",
       "       ['buffalo', 'hunted'],\n",
       "       ['buffalo', 'We'],\n",
       "       ['buffalo', 'were'],\n",
       "       ['We', 'hunted'],\n",
       "       ['We', 'buffalo'],\n",
       "       ['We', 'were'],\n",
       "       ['We', 'fishermenWhat'],\n",
       "       ['were', 'buffalo'],\n",
       "       ['were', 'We'],\n",
       "       ['were', 'fishermenWhat'],\n",
       "       ['were', 'You'],\n",
       "       ['fishermenWhat', 'We'],\n",
       "       ['fishermenWhat', 'were'],\n",
       "       ['fishermenWhat', 'You'],\n",
       "       ['fishermenWhat', 'wanna'],\n",
       "       ['You', 'were'],\n",
       "       ['You', 'fishermenWhat'],\n",
       "       ['You', 'wanna'],\n",
       "       ['You', 'look'],\n",
       "       ['wanna', 'fishermenWhat'],\n",
       "       ['wanna', 'You'],\n",
       "       ['wanna', 'look'],\n",
       "       ['wanna', 'like'],\n",
       "       ['look', 'You'],\n",
       "       ['look', 'wanna'],\n",
       "       ['look', 'like'],\n",
       "       ['look', 'you'],\n",
       "       ['like', 'wanna'],\n",
       "       ['like', 'look'],\n",
       "       ['like', 'you'],\n",
       "       ['like', 'just'],\n",
       "       ['you', 'look'],\n",
       "       ['you', 'like'],\n",
       "       ['you', 'just'],\n",
       "       ['you', 'came'],\n",
       "       ['just', 'like'],\n",
       "       ['just', 'you'],\n",
       "       ['just', 'came'],\n",
       "       ['just', 'back'],\n",
       "       ['came', 'you'],\n",
       "       ['came', 'just'],\n",
       "       ['came', 'back'],\n",
       "       ['came', 'from'],\n",
       "       ['back', 'just'],\n",
       "       ['back', 'came'],\n",
       "       ['back', 'from'],\n",
       "       ['back', 'catching'],\n",
       "       ['from', 'came'],\n",
       "       ['from', 'back'],\n",
       "       ['from', 'catching'],\n",
       "       ['from', 'a'],\n",
       "       ['catching', 'back'],\n",
       "       ['catching', 'from'],\n",
       "       ['catching', 'a'],\n",
       "       ['catching', 'fish'],\n",
       "       ['a', 'from'],\n",
       "       ['a', 'catching'],\n",
       "       ['a', 'fish'],\n",
       "       ['fish', 'catching'],\n",
       "       ['fish', 'a'],\n",
       "       ['This', 'aint'],\n",
       "       ['This', 'dances'],\n",
       "       ['aint', 'This'],\n",
       "       ['aint', 'dances'],\n",
       "       ['aint', 'with'],\n",
       "       ['dances', 'This'],\n",
       "       ['dances', 'aint'],\n",
       "       ['dances', 'with'],\n",
       "       ['dances', 'salmon'],\n",
       "       ['with', 'aint'],\n",
       "       ['with', 'dances'],\n",
       "       ['with', 'salmon'],\n",
       "       ['with', 'you'],\n",
       "       ['salmon', 'dances'],\n",
       "       ['salmon', 'with'],\n",
       "       ['salmon', 'you'],\n",
       "       ['salmon', 'know'],\n",
       "       ['you', 'with'],\n",
       "       ['you', 'salmon'],\n",
       "       ['you', 'know'],\n",
       "       ['you', 'Thomas'],\n",
       "       ['know', 'salmon'],\n",
       "       ['know', 'you'],\n",
       "       ['know', 'Thomas'],\n",
       "       ['know', 'you'],\n",
       "       ['Thomas', 'you'],\n",
       "       ['Thomas', 'know'],\n",
       "       ['Thomas', 'you'],\n",
       "       ['Thomas', 'gotta'],\n",
       "       ['you', 'know'],\n",
       "       ['you', 'Thomas'],\n",
       "       ['you', 'gotta'],\n",
       "       ['you', 'look'],\n",
       "       ['gotta', 'Thomas'],\n",
       "       ['gotta', 'you'],\n",
       "       ['gotta', 'look'],\n",
       "       ['gotta', 'like'],\n",
       "       ['look', 'you'],\n",
       "       ['look', 'gotta'],\n",
       "       ['look', 'like'],\n",
       "       ['look', 'a'],\n",
       "       ['like', 'gotta'],\n",
       "       ['like', 'look'],\n",
       "       ['like', 'a'],\n",
       "       ['like', 'warrior'],\n",
       "       ['a', 'look'],\n",
       "       ['a', 'like'],\n",
       "       ['a', 'warrior'],\n",
       "       ['warrior', 'like'],\n",
       "       ['warrior', 'a']], dtype='<U13')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxPairsTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkipGram(\n",
      "  (embeddings): Embedding(58, 5)\n",
      ")\n",
      "Loss at epo 0: 6.003527005513509\n",
      "Loss at epo 10: 3.265365608036518\n",
      "Loss at epo 20: 2.597180875284331\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generateObservations(tokenizedCorpus, word2Idx):\n",
    "    window_size = 2\n",
    "    idxPairs = []\n",
    "    # for each sentence\n",
    "    for sentence in tokenizedCorpus:\n",
    "        indices = [word2Idx[word] for word in sentence]\n",
    "        # for each word, threated as center word\n",
    "        for center_word_pos in range(len(sentence)):\n",
    "            # for each window position\n",
    "            for w in range(-window_size, window_size + 1):\n",
    "                context_word_pos = center_word_pos + w\n",
    "                # make soure not jump out sentence\n",
    "                if context_word_pos < 0 or context_word_pos >= len(sentence) or center_word_pos == context_word_pos:\n",
    "                    continue\n",
    "                idxPairs.append((sentence[center_word_pos], sentence[context_word_pos]))\n",
    "\n",
    "    idxPairs = np.array(idxPairs) # it will be useful to have this as numpy array\n",
    "    return(idxPairs)\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
    "    \n",
    "    def forward(self, focus, context):\n",
    "        embed_focus = self.embeddings(focus).view((1, -1))\n",
    "        embed_ctx = self.embeddings(context).view((1, -1))\n",
    "        score = torch.mm(embed_focus, torch.t(embed_ctx))\n",
    "        log_probs = nn.functional.logsigmoid(score)\n",
    "    \n",
    "        return(log_probs)\n",
    "\n",
    "idxPairsTest = generateObservations(tokenizedCorpus = testCorpusTokenized, word2Idx = word2Idx_test)\n",
    "embd_size = 100\n",
    "learning_rate = 0.001\n",
    "n_epoch = 30\n",
    "\n",
    "\n",
    "def train_skipgram(vocabSize, embeddingSize, trainingData, word2Idx):\n",
    "    losses = []\n",
    "    loss_fn = nn.MSELoss()\n",
    "    model = SkipGram(vocabSize, embeddingSize)\n",
    "    print(model)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        total_loss = .0\n",
    "        for in_w, out_w in trainingData:\n",
    "            in_w_var = torch.autograd.Variable(torch.LongTensor([word2Idx[in_w]]))\n",
    "            out_w_var = torch.autograd.Variable(torch.LongTensor([word2Idx[out_w]]))\n",
    "            \n",
    "            model.zero_grad()\n",
    "            log_probs = model(in_w_var, out_w_var)\n",
    "            loss = loss_fn(log_probs[0], torch.autograd.Variable(torch.Tensor([1])))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.data.numpy()\n",
    "        losses.append(total_loss)\n",
    "        if epoch % 10 == 0:    \n",
    "            print(f'Loss at epo {epoch}: {total_loss/len(trainingData)}')\n",
    "    return(model, losses)\n",
    "    \n",
    "sg_model, sg_losses = train_skipgram(vocabSize = len(word2Idx_test), embeddingSize = 5,\n",
    "                                    trainingData = idxPairsTest, word2Idx = word2Idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Test SkipGram===\n",
      "Accuracy: 79.2% (266/336)\n"
     ]
    }
   ],
   "source": [
    "def test_skipgram(testData, model, word2Idx):\n",
    "    print('====Test SkipGram===')\n",
    "    correct_ct = 0\n",
    "    for in_w, out_w in testData:\n",
    "        in_w_var = torch.autograd.Variable(torch.LongTensor([word2Idx[in_w]]))\n",
    "        out_w_var = torch.autograd.Variable(torch.LongTensor([word2Idx[out_w]]))\n",
    "\n",
    "        model.zero_grad()\n",
    "        log_probs = model(in_w_var, out_w_var)\n",
    "        prob = torch.exp(log_probs)\n",
    "#         print(torch.max(log_probs.data, 1))\n",
    "#         _, predicted = torch.max(log_probs.data, 1)\n",
    "#         predicted = predicted[0]\n",
    "#         print(log_probs.data)\n",
    "        if prob > 0.5:#predicted == 1:\n",
    "            correct_ct += 1\n",
    "\n",
    "    print('Accuracy: {:.1f}% ({}/{})'.format(correct_ct/len(testData)*100, correct_ct, len(testData)))\n",
    "\n",
    "\n",
    "test_skipgram(idxPairsTest, sg_model, word2Idx = word2Idx_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Domains Affect Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Computational Methods)",
   "language": "python",
   "name": "computational_methods"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
