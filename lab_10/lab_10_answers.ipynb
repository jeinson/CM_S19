{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lab 10: Word Embeddings\n",
    "Thinking of using stuff from here\n",
    "https://gist.github.com/mbednarski/da08eb297304f7a66a3840e857e060a0\n",
    "\n",
    "conda install -c conda-forge tqdm\n",
    "conda install -c conda-forge ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Janitorial Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCorpus = [\"First of all, quit grinnin’ like an idiot. Indians ain’t supposed to smile like that. Get stoic.\",\n",
    "             \"No. Like this. You gotta look mean, or people won’t respect you.\",\n",
    "              \" people will run all over you if you don’t look mean.\",\n",
    "              \"You gotta look like a warrior. You gotta look like you just came back from killing a buffalo.\",\n",
    "             \"But our tribe never hunted buffalo. We were fishermen.\"\n",
    "             \"What? You wanna look like you just came back from catching a fish?\",\n",
    "             \"This ain’t dances with salmon, you know. Thomas, you gotta look like a warrior.\"]\n",
    "maxDocs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 pub med abstracts\n"
     ]
    }
   ],
   "source": [
    "# Read in pubmed corpus into a text file\n",
    "\n",
    "import glob\n",
    "pubMedDataFolderPath = \"data/pubMed_corpus/\"\n",
    "pubMedDataFiles = glob.glob(pubMedDataFolderPath + \"*.txt\")\n",
    "pubMedCorpus = [\"\"]*len(pubMedDataFiles)\n",
    "for idx, pubMedDataPath in enumerate(pubMedDataFiles):\n",
    "    with open(pubMedDataPath, \"r\") as pubMedFile:\n",
    "        text = pubMedFile.read().strip()\n",
    "        pubMedCorpus[idx] = text\n",
    "pubMedCorpus = pubMedCorpus[0:maxDocs]\n",
    "print(\"{} pub med abstracts\".format(len(pubMedCorpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 ap articles\n"
     ]
    }
   ],
   "source": [
    "# Read in the ap corpus\n",
    "apTextFile = \"data/ap.txt\"\n",
    "apCorpus = []\n",
    "readText = False\n",
    "with open(apTextFile) as apDataFile:\n",
    "    for line in apDataFile:\n",
    "        if readText:\n",
    "            apCorpus.append(line.strip())\n",
    "            readText = False\n",
    "        if line == \"<TEXT>\\n\":\n",
    "            readText = True\n",
    "apCorpus = apCorpus[0:maxDocs]\n",
    "print(\"{} ap articles\".format(len(apCorpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def removePunctuation(myStr):\n",
    "    excludedCharacters = string.punctuation + \"’\"\n",
    "    newStr = \"\".join(char for char in myStr if char not in excludedCharacters)\n",
    "    return(newStr)\n",
    "def tokenize_corpus(corpus):\n",
    "    tokens = [removePunctuation(x).split() for x in corpus]\n",
    "    return tokens\n",
    "\n",
    "apCorpusTokenized = tokenize_corpus(apCorpus)\n",
    "pubMedCorpusTokenized = tokenize_corpus(pubMedCorpus)\n",
    "testCorpusTokenized = tokenize_corpus(testCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building ap corpus vocabulary\n",
      "Vocab size: 5168\n",
      "ap data tokenized in 0.011765718460083008 seconds\n",
      "\n",
      "Building pubMed corpus vocabulary\n",
      "Vocab size: 2847\n",
      "pubmed data tokenized in 0.0056684017181396484 seconds\n",
      "\n",
      "Building test corpus vocabulary\n",
      "Vocab size: 59\n",
      "test data tokenized in 0.0002300739288330078 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from collections import Counter\n",
    "\n",
    "maxVocabSize = 10000\n",
    "\n",
    "def extractVocabMappers(tokenizedCorpus, vocabSizeMax = None):\n",
    "    UNK = \"<UNK>\"\n",
    "    flattenedCorpus = [item for sublist in tokenizedCorpus for item in sublist]\n",
    "    wordCounts = Counter(flattenedCorpus)\n",
    "    wordCounts = wordCounts.most_common(vocabSizeMax)\n",
    "    vocabulary = [word for word, count in wordCounts]\n",
    "    \n",
    "    # below is more readable but significantly slower code\n",
    "    if False:\n",
    "        vocabulary = []\n",
    "        for sentence in tqdm(tokenizedCorpus):\n",
    "            for token in sentence:\n",
    "                if token not in vocabulary:\n",
    "                    vocabulary.append(token)\n",
    "    vocabulary.append(UNK)\n",
    "    print(\"Vocab size: {}\".format(len(vocabulary)))\n",
    "    word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "    idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "    newTokenizedCorpus = []# all words missing from vocab replaced with <UNK>\n",
    "    for doc in tokenizedCorpus:\n",
    "        newDoc = [word if word in word2idx else UNK for word in doc]\n",
    "        newTokenizedCorpus.append(newDoc)\n",
    "    return(word2idx, idx2word, wordCounts, newTokenizedCorpus)\n",
    "\n",
    "start = time.time()\n",
    "print(\"Building ap corpus vocabulary\")\n",
    "word2Idx_ap, idx2Word_ap, vocabCount_ap, finalTokenizedCorpus_ap = extractVocabMappers(apCorpusTokenized,\n",
    "                                                                                       vocabSizeMax = maxVocabSize)\n",
    "print(\"ap data tokenized in {} seconds\\n\".format(time.time() - start))\n",
    "start = time.time()\n",
    "print(\"Building pubMed corpus vocabulary\")\n",
    "word2Idx_pubMed, idx2Word_pubMed, vocabCount_pubMed, finalTokenizedCorpus_pubMed = extractVocabMappers(pubMedCorpusTokenized,\n",
    "                                                                                                       vocabSizeMax = maxVocabSize)\n",
    "print(\"pubmed data tokenized in {} seconds\\n\".format(time.time() - start))\n",
    "start = time.time()\n",
    "print(\"Building test corpus vocabulary\")\n",
    "word2Idx_test, idx2Word_test, vocabCount_test, finalTokenizedCorpus_test = extractVocabMappers(testCorpusTokenized,\n",
    "                                                                                               vocabSizeMax = maxVocabSize)\n",
    "print(\"test data tokenized in {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateObservations(tokenizedCorpus, word2Idx):\n",
    "    window_size = 1\n",
    "    idxPairs = []\n",
    "    # for each sentence\n",
    "    for sentence in tokenizedCorpus:\n",
    "#         indices = [word2Idx[word] for word in sentence]\n",
    "        # for each word, threated as center word\n",
    "        for center_word_pos in range(len(sentence)):\n",
    "            # for each window position\n",
    "            for w in range(-window_size, window_size + 1):\n",
    "                context_word_pos = center_word_pos + w\n",
    "                # make soure not jump out sentence\n",
    "                if context_word_pos < 0 or context_word_pos >= len(sentence) or center_word_pos == context_word_pos:\n",
    "                    continue\n",
    "                idxPairs.append((sentence[center_word_pos], sentence[context_word_pos]))\n",
    "\n",
    "    idxPairs = np.array(idxPairs) # it will be useful to have this as numpy array\n",
    "    return(idxPairs)\n",
    "\n",
    "\n",
    "def generateWordSamplingProb(vocabCount, word2Idx):\n",
    "    wordSampleProbs = [0.0]*len(vocabCount)\n",
    "    numWords = np.sum([count**0.75 for word, count in vocabCount])\n",
    "    for idx in range(len(vocabCount)):\n",
    "        w,c = vocabCount[idx]\n",
    "        wordSampleProbs[word2Idx[w]] = (c**0.75)/(numWords)\n",
    "        \n",
    "        \n",
    "        \n",
    "    wordSampleProbs = []\n",
    "    numWords = np.sum([count for word, count in vocabCount])\n",
    "    for w,c in vocabCount:\n",
    "#         w,c = vocabCount[idx]\n",
    "        wordSampleProbs.extend([word2Idx[w]] * int(((c/numWords)**0.75)/0.001))\n",
    "    return(wordSampleProbs)\n",
    "    \n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocabSize, embedSize, vocabCount, word2Idx):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.vocabSize = vocabSize\n",
    "        self.word2Idx = word2Idx\n",
    "#         self.centerEmbeddings = nn.Embedding(vocab_size, embd_size)\n",
    "#         self.contextEmbeddings = nn.Embedding(vocab_size, embd_size)\n",
    "#         self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
    "        \n",
    "        self.centerEmbeddings = nn.Parameter(torch.randn(vocabSize,\n",
    "                                                     embedSize).float(), requires_grad=True)\n",
    "        self.contextEmbeddings = nn.Parameter(torch.randn(vocabSize,\n",
    "                                                      embedSize).float(), requires_grad=True)\n",
    "        self.wordSampleProbs = generateWordSamplingProb(vocabCount, word2Idx)\n",
    "        self.logSigmoid = nn.LogSigmoid()\n",
    "#         self.paramList = nn.ModuleList([self.centerEmbeddings, self.contextEmbeddings] )\n",
    "    def getNegSample(self, k, centerWord):\n",
    "        vocabSizeWithoutUnk = self.vocabSize - 1\n",
    "#         negSample = np.random.choice(vocabSizeWithoutUnk,\n",
    "#                                      size = k, replace = True, p = self.wordSampleProbs)\n",
    "        negSample = random.sample(self.wordSampleProbs, k)\n",
    "        while self.word2Idx[centerWord] in negSample:\n",
    "#             negSample = np.random.choice(vocabSizeWithoutUnk,\n",
    "#                                          size = k, replace = True, p = self.wordSampleProbs)\n",
    "            negSample = random.sample(self.wordSampleProbs, k)\n",
    "        return(negSample)\n",
    "    def forward(self, center, context, negSampleIndices = None):\n",
    "#         focus = torch.autograd.Variable(torch.LongTensor([0]))\n",
    "#         context = torch.autograd.Variable(torch.LongTensor([0]))\n",
    "#         allEmbeddingIdxs = torch.autograd.Variable(torch.LongTensor([np.arange(0,self.vocabSize)]))\n",
    "\n",
    "\n",
    "#         embedCenter = self.centerEmbeddings(center).view((1, -1))\n",
    "#         embedContext = self.contextEmbeddings(context).view((1, -1))\n",
    "# #         print(allEmbeddingIdxs)\n",
    "#         allContextEmbeddings = self.contextEmbeddings(allEmbeddingIdxs).squeeze()\n",
    "#         num = torch.exp(torch.mm(embedContext, torch.t(embedCenter)))\n",
    "#         denom = torch.exp(torch.mm(allContextEmbeddings, torch.t(embedCenter))).sum()\n",
    "#         logProb = torch.log(num/denom)\n",
    "        embedCenter = self.centerEmbeddings[center].view((1, -1))\n",
    "        embedContext = self.contextEmbeddings[context].view((1, -1))       \n",
    "        if negSampleIndices is not None:\n",
    "#             print(\"hey\")\n",
    "            posVal = self.logSigmoid (torch.mm(embedContext, torch.t(embedCenter)))\n",
    "            negVal = torch.mm(self.contextEmbeddings[negSampleIndices], torch.t(embedCenter))\n",
    "            negVal = self.logSigmoid (-torch.sum(negVal))\n",
    "            logProb = -(posVal + negVal)\n",
    "        else:\n",
    "#             allEmbeddingIdxs = torch.autograd.Variable(torch.LongTensor([np.arange(0,self.vocabSize)]))\n",
    "\n",
    "\n",
    "\n",
    "    #         print(allEmbeddingIdxs)\n",
    "    #         allContextEmbeddings = self.contextEmbeddings(allEmbeddingIdxs).squeeze()\n",
    "            num = torch.exp(torch.mm(embedContext, torch.t(embedCenter)))\n",
    "            denom = torch.exp(torch.mm(self.contextEmbeddings, torch.t(embedCenter))).sum()\n",
    "            logProb = torch.log(num/denom)\n",
    "    \n",
    "        return(logProb)\n",
    "\n",
    "\n",
    "def train_skipgram(embeddingSize, trainingData, vocabCount, word2Idx, k):\n",
    "    print(\"training on {} observations\".format(len(trainingData)))\n",
    "    losses = []\n",
    "    loss_fn = nn.MSELoss()\n",
    "    model = SkipGram(vocabSize = len(word2Idx), embedSize = embeddingSize,\n",
    "                     vocabCount = vocabCount, word2Idx = word2Idx)\n",
    "#     print(model)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in tqdm_notebook(range(n_epoch), position = 0):\n",
    "#         print(\"entered epoch\")\n",
    "        total_loss = .0\n",
    "        for in_w, out_w in tqdm_notebook(trainingData, position = 1):\n",
    "            if k is not None:\n",
    "                negSamples = model.getNegSample(k = k, centerWord = in_w)\n",
    "            else:\n",
    "                negSamples = None\n",
    "#             print(\"neg samples found\")\n",
    "#             print(negSamples)\n",
    "            in_w_var = word2Idx[in_w]#torch.autograd.Variable(torch.LongTensor([word2Idx[in_w]]))\n",
    "            out_w_var = word2Idx[out_w]#torch.autograd.Variable(torch.LongTensor([word2Idx[out_w]]))\n",
    "#             if in_w in word2Idx:\n",
    "#                 in_w_var = word2Idx[in_w]#torch.autograd.Variable(torch.LongTensor([word2Idx[in_w]]))\n",
    "#             else:\n",
    "#                 in_w_var = word2Idx[\"<UNK>\"]\n",
    "#             if out_w in word2Idx:\n",
    "#                 out_w_var = word2Idx[out_w]#torch.autograd.Variable(torch.LongTensor([word2Idx[out_w]]))\n",
    "#             else:\n",
    "#                 out_w_var = word2Idx[\"<UNK>\"]\n",
    "            \n",
    "            model.zero_grad()\n",
    "            log_probs = model(in_w_var, out_w_var, negSampleIndices = negSamples)\n",
    "            loss = -log_probs[0]#loss_fn(log_probs[0], torch.autograd.Variable(torch.Tensor([1])))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.data.numpy()\n",
    "        losses.append(total_loss)\n",
    "        if epoch % 10 == 0:    \n",
    "            print(f'Loss at epoch {epoch}: {total_loss/len(trainingData)}')\n",
    "    return(model, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec92b01baac541409ba1694031d0065e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=60), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ac3cf7073f1458aad4135606bd126c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0: [2.925287]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "322e395308754744825aad21254b5239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9c78c8f66649ca9e51f5eb7f9ab7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f0399458d864b37bf2d815788080d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7220f8dd79684f3b92e757a05c95a075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb93c6c20854b93b7c6190022bdce13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6384bf26d64b12bb7b814305594229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c816a18cf3d842818d89f46b1c5ccaf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692ff7b7e5344e7bb22d8788db6ea62a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54fc030f25e84389bbc4b980a90e87ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9dca5b1e4d044fba585549bfb214967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 10: [2.6161726]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "714072b5d2314cf083460ce74bc9ad87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40154ba6ea5641688e0093d77fd562cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b79be86feb4082ac03d8319cfb6947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96706cde0a1447869a4ba485fd590d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05bca56dca3f4b5dab9509af012ca08f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00dc49fe05574823aa1031a1f6563b78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d88670204949b1899762bbc7f0be3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae9d453f02040e4aeab3c01c89a70a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a090b59903694731a7f2d9b6530c3384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55b19d24e604672bf965f28abc4b9f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 20: [2.5666747]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e376ac7aad4370a7474b1ea6d23550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "192bc2c7b7e84823a351c6757fe41d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a2692e4c944b6fac200a84e7216d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1a2e1dac1346c2b6a1071fc4ebd5e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa387f795d24ad5a41ed641bc9f7fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2376f2e2287462dbe671698e4d08955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be197742890040ffae3a310ebbbc8dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5276708d8834ba499b30d9821253305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226370b9ffa744c282682d392b0970e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51503e2c8b8740b39a0fb8b1f76fecf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 30: [2.4666207]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "037df085f2ff4144aa50e04848840b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eecb012ff21543eba6ab2852a1e9dcad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05db8c4416444cc1bad8bfa1323fc6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720d607726134d2c880938bc237ace15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee4570a670a4ecd90fd2daed65a46ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ea395cc2704424968695dd34177b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b44013d004043f7916c59dfae77a319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5841de1c48014d3faa340da90da6cb5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6dc92eddf04d919603e328fc340f15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04be550a424d44b1ad2395bf62175ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 40: [2.2420926]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d2e38c0363417592a09143ba7da7c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee050ec060b4e1e95231ad479154d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04487b12df374b89a6d27b5fd3607a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "250dc647c5c04c18a00a711e4eefe499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aebe915261346d68a972911cc5c99db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80472c48147d4d1494b91cbe8c7fea1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0363f75117774951a2f48fb6de424831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce28b3a5ecc4b03b33a249beac848bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04239c85b1b34e109c6f18ee5580b10a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf972dc370a499c94edf80fc8704ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 50: [2.0216222]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336f8bb63bc146b1afc8fcf147f72af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "337ebe27238b40a0a77419c9a75764cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ed6f53a3434aa88c25e549ff5dbad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5f5d7c64214d0cb0da76a7759f6718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b4298e164a41f682bb6dc8d57e9ec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf811c1896fa47be9a864deb090981b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076e7a58c11d418b97085e9a24938de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05382004d1b74297ba4bbac6867d2394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab81f7091a24499be873eafd42dbfcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=336), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embd_size = 100\n",
    "learning_rate = 0.001\n",
    "n_epoch = 60\n",
    "idxPairsTest = generateObservations(tokenizedCorpus = finalTokenizedCorpus_test, word2Idx = word2Idx_test)\n",
    "sg_model, sg_losses = train_skipgram(embeddingSize = 5, trainingData = idxPairsTest, vocabCount = vocabCount_test,\n",
    "                                     word2Idx = word2Idx_test, k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on 39788 observations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12fe9376ee9c46568ae9578d783d030b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f0e91864e34b9f94a4f83420826b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=39788), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0: [-26.419334]\n"
     ]
    }
   ],
   "source": [
    "embeddingSize = 100\n",
    "learning_rate = 0.001\n",
    "n_epoch = 1\n",
    "idxPairsAP = generateObservations(tokenizedCorpus = finalTokenizedCorpus_ap, word2Idx = word2Idx_ap)\n",
    "sg_model, sg_losses = train_skipgram(embeddingSize = embeddingSize, trainingData = idxPairsAP, vocabCount = vocabCount_ap,\n",
    "                                     word2Idx = word2Idx_ap, k = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128.45454545454547"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1413/11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on 3627632 observations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da1a61de8d414beca9f7676607d44e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "064b4b21a2bb4045aba563b6d4a4ab73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3627632), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-b37290375457>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# idxPairsAP = generateObservations(tokenizedCorpus = finalTokenizedCorpus_ap, word2Idx = word2Idx_ap)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m sg_model, sg_losses = train_skipgram(embeddingSize = embeddingSize, trainingData = idxPairsAP, vocabCount = vocabCount_ap,\n\u001b[0;32m----> 6\u001b[0;31m                                      word2Idx = word2Idx_ap, k = 5)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-36c626829b33>\u001b[0m in \u001b[0;36mtrain_skipgram\u001b[0;34m(embeddingSize, trainingData, vocabCount, word2Idx, k)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#loss_fn(log_probs[0], torch.autograd.Variable(torch.Tensor([1])))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/compMeth/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/compMeth/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embeddingSize = 100\n",
    "learning_rate = 0.001\n",
    "n_epoch = 1\n",
    "# idxPairsAP = generateObservations(tokenizedCorpus = finalTokenizedCorpus_ap, word2Idx = word2Idx_ap)\n",
    "sg_model, sg_losses = train_skipgram(embeddingSize = embeddingSize, trainingData = idxPairsAP, vocabCount = vocabCount_ap,\n",
    "                                     word2Idx = word2Idx_ap, k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1705/11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on 3627632 observations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e077f8dcd24471da040254ce78d7487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f1a55a288c04fa3b68f19412b0c428d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3627632), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-7f3571d22c4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# idxPairsAP = generateObservations(tokenizedCorpus = finalTokenizedCorpus_ap, word2Idx = word2Idx_ap)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m sg_model, sg_losses = train_skipgram(embeddingSize = embeddingSize, trainingData = idxPairsAP, vocabCount = vocabCount_ap,\n\u001b[0;32m----> 6\u001b[0;31m                                      word2Idx = word2Idx_ap, k = None)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-36c626829b33>\u001b[0m in \u001b[0;36mtrain_skipgram\u001b[0;34m(embeddingSize, trainingData, vocabCount, word2Idx, k)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/compMeth/lib/python3.6/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embeddingSize = 100\n",
    "learning_rate = 0.001\n",
    "n_epoch = 1\n",
    "# idxPairsAP = generateObservations(tokenizedCorpus = finalTokenizedCorpus_ap, word2Idx = word2Idx_ap)\n",
    "sg_model, sg_losses = train_skipgram(embeddingSize = embeddingSize, trainingData = idxPairsAP, vocabCount = vocabCount_ap,\n",
    "                                     word2Idx = word2Idx_ap, k = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on 1467190 observations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9508a018c6d44212a1db51754573d78e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca31dc770aa846f48bc0832f57e1e4f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1467190), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-cb41106bb10d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0midxPairsPubMed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerateObservations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizedCorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinalTokenizedCorpus_pubMed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2Idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2Idx_pubMed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m sg_model, sg_losses = train_skipgram(embeddingSize = embeddingSize, trainingData = idxPairsPubMed, vocabCount = vocabCount_pubMed,\n\u001b[0;32m----> 6\u001b[0;31m                                      word2Idx = word2Idx_pubMed, k = 20)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-36c626829b33>\u001b[0m in \u001b[0;36mtrain_skipgram\u001b[0;34m(embeddingSize, trainingData, vocabCount, word2Idx, k)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#loss_fn(log_probs[0], torch.autograd.Variable(torch.Tensor([1])))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/compMeth/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/compMeth/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embeddingSize = 100\n",
    "learning_rate = 0.001\n",
    "n_epoch = 1\n",
    "idxPairsPubMed = generateObservations(tokenizedCorpus = finalTokenizedCorpus_pubMed, word2Idx = word2Idx_pubMed)\n",
    "sg_model, sg_losses = train_skipgram(embeddingSize = embeddingSize, trainingData = idxPairsPubMed, vocabCount = vocabCount_pubMed,\n",
    "                                     word2Idx = word2Idx_pubMed, k = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dbcb806ca584b07a0554fd710693ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e4cdd7abd849bbb2ad18fb65bd478c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3627632), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0: [2.2236288]\n"
     ]
    }
   ],
   "source": [
    "embeddingSize = 100\n",
    "learning_rate = 0.001\n",
    "n_epoch = 1\n",
    "# idxPairsAP = generateObservations(tokenizedCorpus = finalTokenizedCorpus_ap, word2Idx = word2Idx_ap)\n",
    "sg_model, sg_losses = train_skipgram(embeddingSize = embeddingSize, trainingData = idxPairsAP, vocabCount = vocabCount_ap,\n",
    "                                     word2Idx = word2Idx_ap, k = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d916bc764dc4def95b3e1f1af958416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1482cbee5c2b406487e76d7609ca3b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3627632), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0: [nan]\n"
     ]
    }
   ],
   "source": [
    "embeddingSize = 100\n",
    "learning_rate = 0.001\n",
    "n_epoch = 1\n",
    "# idxPairsAP = generateObservations(tokenizedCorpus = finalTokenizedCorpus_ap, word2Idx = word2Idx_ap)\n",
    "sg_model, sg_losses = train_skipgram(embeddingSize = embeddingSize, trainingData = idxPairsAP, vocabCount = vocabCount_ap,\n",
    "                                     word2Idx = word2Idx_ap, k = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0,\n",
       " 'of': 1,\n",
       " 'to': 2,\n",
       " 'and': 3,\n",
       " 'a': 4,\n",
       " 'in': 5,\n",
       " 'said': 6,\n",
       " 'for': 7,\n",
       " 'The': 8,\n",
       " 'that': 9,\n",
       " 'was': 10,\n",
       " 'on': 11,\n",
       " 'is': 12,\n",
       " 'with': 13,\n",
       " 'by': 14,\n",
       " 'at': 15,\n",
       " 'he': 16,\n",
       " 'from': 17,\n",
       " 'as': 18,\n",
       " 'be': 19,\n",
       " 'were': 20,\n",
       " 'have': 21,\n",
       " 'it': 22,\n",
       " 'his': 23,\n",
       " 'an': 24,\n",
       " 'has': 25,\n",
       " 'not': 26,\n",
       " 'are': 27,\n",
       " 'who': 28,\n",
       " 'had': 29,\n",
       " 'will': 30,\n",
       " 'would': 31,\n",
       " 'about': 32,\n",
       " 'but': 33,\n",
       " 'been': 34,\n",
       " 'they': 35,\n",
       " 'its': 36,\n",
       " 'I': 37,\n",
       " 'their': 38,\n",
       " 'percent': 39,\n",
       " 'which': 40,\n",
       " 'or': 41,\n",
       " 'this': 42,\n",
       " 'after': 43,\n",
       " 'He': 44,\n",
       " 'more': 45,\n",
       " 'up': 46,\n",
       " 'people': 47,\n",
       " 'million': 48,\n",
       " 'US': 49,\n",
       " 'also': 50,\n",
       " 'one': 51,\n",
       " 'In': 52,\n",
       " 'other': 53,\n",
       " 'than': 54,\n",
       " 'year': 55,\n",
       " 'two': 56,\n",
       " 'when': 57,\n",
       " 'government': 58,\n",
       " 'A': 59,\n",
       " 'years': 60,\n",
       " 'last': 61,\n",
       " 'But': 62,\n",
       " 'no': 63,\n",
       " 'out': 64,\n",
       " 'all': 65,\n",
       " 'we': 66,\n",
       " 'It': 67,\n",
       " 'could': 68,\n",
       " 'over': 69,\n",
       " 'new': 70,\n",
       " 'into': 71,\n",
       " 'first': 72,\n",
       " 'because': 73,\n",
       " 'Soviet': 74,\n",
       " 'some': 75,\n",
       " 'them': 76,\n",
       " 'she': 77,\n",
       " 'New': 78,\n",
       " 'her': 79,\n",
       " 'United': 80,\n",
       " 'there': 81,\n",
       " 'Bush': 82,\n",
       " 'if': 83,\n",
       " 'time': 84,\n",
       " 'officials': 85,\n",
       " 'before': 86,\n",
       " 'We': 87,\n",
       " 'billion': 88,\n",
       " 'can': 89,\n",
       " 'only': 90,\n",
       " 'police': 91,\n",
       " 'told': 92,\n",
       " 'against': 93,\n",
       " 'state': 94,\n",
       " 'three': 95,\n",
       " 'down': 96,\n",
       " 'American': 97,\n",
       " 'him': 98,\n",
       " 'States': 99,\n",
       " 'today': 100,\n",
       " 'any': 101,\n",
       " 'since': 102,\n",
       " 'president': 103,\n",
       " 'Thursday': 104,\n",
       " 'what': 105,\n",
       " 'President': 106,\n",
       " 'during': 107,\n",
       " 'where': 108,\n",
       " 'most': 109,\n",
       " 'Tuesday': 110,\n",
       " 'week': 111,\n",
       " 'made': 112,\n",
       " 'you': 113,\n",
       " 'Wednesday': 114,\n",
       " 'Monday': 115,\n",
       " 'Friday': 116,\n",
       " 'say': 117,\n",
       " 'did': 118,\n",
       " 'do': 119,\n",
       " 'They': 120,\n",
       " 'under': 121,\n",
       " 'company': 122,\n",
       " 'York': 123,\n",
       " 'so': 124,\n",
       " 'our': 125,\n",
       " 'being': 126,\n",
       " 'between': 127,\n",
       " 'report': 128,\n",
       " 'market': 129,\n",
       " 'through': 130,\n",
       " 'those': 131,\n",
       " 'now': 132,\n",
       " 'just': 133,\n",
       " 'day': 134,\n",
       " 'military': 135,\n",
       " 'such': 136,\n",
       " 'off': 137,\n",
       " 'House': 138,\n",
       " 'federal': 139,\n",
       " 'should': 140,\n",
       " 'many': 141,\n",
       " 'members': 142,\n",
       " 'reported': 143,\n",
       " 'including': 144,\n",
       " 'political': 145,\n",
       " 'group': 146,\n",
       " 'former': 147,\n",
       " 'make': 148,\n",
       " 'going': 149,\n",
       " 'while': 150,\n",
       " 'spokesman': 151,\n",
       " 'get': 152,\n",
       " 'news': 153,\n",
       " 'home': 154,\n",
       " 'dont': 155,\n",
       " '10': 156,\n",
       " 'think': 157,\n",
       " 'like': 158,\n",
       " 'National': 159,\n",
       " 'says': 160,\n",
       " 'work': 161,\n",
       " 'West': 162,\n",
       " 'back': 163,\n",
       " 'South': 164,\n",
       " 'next': 165,\n",
       " 'country': 166,\n",
       " 'official': 167,\n",
       " 'take': 168,\n",
       " '1': 169,\n",
       " 'then': 170,\n",
       " 'still': 171,\n",
       " 'way': 172,\n",
       " 'Department': 173,\n",
       " 'prices': 174,\n",
       " 'called': 175,\n",
       " 'Dukakis': 176,\n",
       " 'found': 177,\n",
       " 'much': 178,\n",
       " 'month': 179,\n",
       " 'four': 180,\n",
       " 'Congress': 181,\n",
       " 'very': 182,\n",
       " 'money': 183,\n",
       " 'days': 184,\n",
       " 'court': 185,\n",
       " 'may': 186,\n",
       " 'campaign': 187,\n",
       " 'months': 188,\n",
       " 'case': 189,\n",
       " 'part': 190,\n",
       " 'meeting': 191,\n",
       " 'five': 192,\n",
       " 'used': 193,\n",
       " 'office': 194,\n",
       " 'program': 195,\n",
       " 'support': 196,\n",
       " 'economic': 197,\n",
       " 'several': 198,\n",
       " 'late': 199,\n",
       " 'Its': 200,\n",
       " 'law': 201,\n",
       " 'plan': 202,\n",
       " 'help': 203,\n",
       " 'trade': 204,\n",
       " 'ago': 205,\n",
       " 'go': 206,\n",
       " 'later': 207,\n",
       " 'public': 208,\n",
       " 'There': 209,\n",
       " 'Mrs': 210,\n",
       " 'expected': 211,\n",
       " 'killed': 212,\n",
       " 'least': 213,\n",
       " 'began': 214,\n",
       " 'workers': 215,\n",
       " 'John': 216,\n",
       " 'how': 217,\n",
       " 'nations': 218,\n",
       " 'Washington': 219,\n",
       " 'early': 220,\n",
       " 'end': 221,\n",
       " 'major': 222,\n",
       " 'saying': 223,\n",
       " 'If': 224,\n",
       " 'came': 225,\n",
       " 'use': 226,\n",
       " 'business': 227,\n",
       " 'until': 228,\n",
       " 'another': 229,\n",
       " 'both': 230,\n",
       " 'Reagan': 231,\n",
       " 'oil': 232,\n",
       " 'agreement': 233,\n",
       " 'chief': 234,\n",
       " 'This': 235,\n",
       " 'want': 236,\n",
       " 'miles': 237,\n",
       " 'even': 238,\n",
       " 'night': 239,\n",
       " 'took': 240,\n",
       " 'system': 241,\n",
       " 'among': 242,\n",
       " 'good': 243,\n",
       " 'Union': 244,\n",
       " 'East': 245,\n",
       " 'Senate': 246,\n",
       " 'asked': 247,\n",
       " '15': 248,\n",
       " 'same': 249,\n",
       " 'according': 250,\n",
       " 'city': 251,\n",
       " 'administration': 252,\n",
       " '20': 253,\n",
       " 'Sunday': 254,\n",
       " 'At': 255,\n",
       " 'whether': 256,\n",
       " 'statement': 257,\n",
       " 'party': 258,\n",
       " 'my': 259,\n",
       " 'national': 260,\n",
       " 'without': 261,\n",
       " '30': 262,\n",
       " '1987': 263,\n",
       " 'man': 264,\n",
       " 'foreign': 265,\n",
       " 'earlier': 266,\n",
       " 'number': 267,\n",
       " 'held': 268,\n",
       " 'talks': 269,\n",
       " 'left': 270,\n",
       " 'higher': 271,\n",
       " 'family': 272,\n",
       " 'Inc': 273,\n",
       " 'children': 274,\n",
       " 'know': 275,\n",
       " 'defense': 276,\n",
       " 'own': 277,\n",
       " 'well': 278,\n",
       " 'On': 279,\n",
       " 'leader': 280,\n",
       " 'must': 281,\n",
       " 'announced': 282,\n",
       " 'And': 283,\n",
       " 'leaders': 284,\n",
       " 'death': 285,\n",
       " 'decision': 286,\n",
       " 'set': 287,\n",
       " 'bill': 288,\n",
       " 'see': 289,\n",
       " 'sales': 290,\n",
       " 'put': 291,\n",
       " 'war': 292,\n",
       " 'past': 293,\n",
       " 'long': 294,\n",
       " 'drug': 295,\n",
       " 'Democratic': 296,\n",
       " 'dollar': 297,\n",
       " 'interest': 298,\n",
       " 'Gorbachev': 299,\n",
       " 'world': 300,\n",
       " 'She': 301,\n",
       " '2': 302,\n",
       " 'director': 303,\n",
       " 'high': 304,\n",
       " 'April': 305,\n",
       " 'might': 306,\n",
       " 'charges': 307,\n",
       " 'six': 308,\n",
       " 'rose': 309,\n",
       " 'vote': 310,\n",
       " 'each': 311,\n",
       " 'That': 312,\n",
       " 'area': 313,\n",
       " 'rights': 314,\n",
       " 'states': 315,\n",
       " 'Republican': 316,\n",
       " 'died': 317,\n",
       " 'these': 318,\n",
       " 'pay': 319,\n",
       " 'around': 320,\n",
       " 'power': 321,\n",
       " 'second': 322,\n",
       " 'issue': 323,\n",
       " 'price': 324,\n",
       " 'countries': 325,\n",
       " 'Saturday': 326,\n",
       " '1986': 327,\n",
       " 'control': 328,\n",
       " 'weeks': 329,\n",
       " 'Corp': 330,\n",
       " 'trial': 331,\n",
       " 'too': 332,\n",
       " 'here': 333,\n",
       " 'economy': 334,\n",
       " 'few': 335,\n",
       " 'never': 336,\n",
       " 'come': 337,\n",
       " 'Co': 338,\n",
       " 'us': 339,\n",
       " 'life': 340,\n",
       " 'got': 341,\n",
       " 'troops': 342,\n",
       " 'cents': 343,\n",
       " 'men': 344,\n",
       " 'give': 345,\n",
       " '1988': 346,\n",
       " 'authorities': 347,\n",
       " '12': 348,\n",
       " 'Committee': 349,\n",
       " 'March': 350,\n",
       " 'No': 351,\n",
       " 'show': 352,\n",
       " 'agency': 353,\n",
       " 'close': 354,\n",
       " 'chairman': 355,\n",
       " 'June': 356,\n",
       " 'students': 357,\n",
       " 'State': 358,\n",
       " 'North': 359,\n",
       " 'less': 360,\n",
       " 'force': 361,\n",
       " 'prison': 362,\n",
       " 'German': 363,\n",
       " 'City': 364,\n",
       " 'Party': 365,\n",
       " 'Ms': 366,\n",
       " 'rates': 367,\n",
       " 'problems': 368,\n",
       " 'trading': 369,\n",
       " 'rate': 370,\n",
       " 'recent': 371,\n",
       " 'increase': 372,\n",
       " 'security': 373,\n",
       " 'released': 374,\n",
       " 'lower': 375,\n",
       " 'Police': 376,\n",
       " 'fire': 377,\n",
       " 'budget': 378,\n",
       " 'me': 379,\n",
       " 'forces': 380,\n",
       " 'stock': 381,\n",
       " 'right': 382,\n",
       " 'hours': 383,\n",
       " 'near': 384,\n",
       " 'Court': 385,\n",
       " 'May': 386,\n",
       " 'aid': 387,\n",
       " 'issues': 388,\n",
       " 'reporters': 389,\n",
       " 'taken': 390,\n",
       " 'Sen': 391,\n",
       " '1989': 392,\n",
       " 'school': 393,\n",
       " 'went': 394,\n",
       " 'away': 395,\n",
       " 'Im': 396,\n",
       " 'board': 397,\n",
       " 'policy': 398,\n",
       " 'fell': 399,\n",
       " 'White': 400,\n",
       " 'One': 401,\n",
       " 'women': 402,\n",
       " 'presidential': 403,\n",
       " 'groups': 404,\n",
       " '3': 405,\n",
       " 'conference': 406,\n",
       " 'share': 407,\n",
       " 'July': 408,\n",
       " 'condition': 409,\n",
       " 'received': 410,\n",
       " 'Germany': 411,\n",
       " 'Iraq': 412,\n",
       " 'far': 413,\n",
       " '25': 414,\n",
       " 'whose': 415,\n",
       " 'tax': 416,\n",
       " 'top': 417,\n",
       " 'added': 418,\n",
       " 'vice': 419,\n",
       " 'record': 420,\n",
       " 'black': 421,\n",
       " 'head': 422,\n",
       " 'California': 423,\n",
       " 'As': 424,\n",
       " 'industry': 425,\n",
       " 'America': 426,\n",
       " 'Some': 427,\n",
       " 'agreed': 428,\n",
       " 'service': 429,\n",
       " 'order': 430,\n",
       " 'general': 431,\n",
       " 'little': 432,\n",
       " 'University': 433,\n",
       " 'George': 434,\n",
       " 'others': 435,\n",
       " 'offer': 436,\n",
       " 'attorney': 437,\n",
       " 'employees': 438,\n",
       " '11': 439,\n",
       " 'information': 440,\n",
       " 'General': 441,\n",
       " 'air': 442,\n",
       " 'Minister': 443,\n",
       " 'largest': 444,\n",
       " 'food': 445,\n",
       " '5': 446,\n",
       " 'capital': 447,\n",
       " 'reports': 448,\n",
       " 'When': 449,\n",
       " 'Robert': 450,\n",
       " 'known': 451,\n",
       " 'Air': 452,\n",
       " 'James': 453,\n",
       " 'Europe': 454,\n",
       " 'place': 455,\n",
       " 'wife': 456,\n",
       " 'San': 457,\n",
       " 'Rep': 458,\n",
       " 'didnt': 459,\n",
       " 'Jackson': 460,\n",
       " 'companies': 461,\n",
       " 'attack': 462,\n",
       " 'opposition': 463,\n",
       " 'outside': 464,\n",
       " 'am': 465,\n",
       " 'given': 466,\n",
       " 'change': 467,\n",
       " 'already': 468,\n",
       " 'job': 469,\n",
       " 'points': 470,\n",
       " '100': 471,\n",
       " 'private': 472,\n",
       " 'across': 473,\n",
       " 'Japan': 474,\n",
       " 'television': 475,\n",
       " 'Communist': 476,\n",
       " 'water': 477,\n",
       " 'An': 478,\n",
       " 'enough': 479,\n",
       " 'plans': 480,\n",
       " 'average': 481,\n",
       " 'cost': 482,\n",
       " 'move': 483,\n",
       " 'lost': 484,\n",
       " 'need': 485,\n",
       " 'believe': 486,\n",
       " 'investigation': 487,\n",
       " 'member': 488,\n",
       " 'scheduled': 489,\n",
       " 'accused': 490,\n",
       " 'nearly': 491,\n",
       " 'election': 492,\n",
       " 'cut': 493,\n",
       " 'Texas': 494,\n",
       " 'committee': 495,\n",
       " 'return': 496,\n",
       " 'Israel': 497,\n",
       " 'total': 498,\n",
       " 'newspaper': 499,\n",
       " 'trying': 500,\n",
       " 'included': 501,\n",
       " 'arrested': 502,\n",
       " 'small': 503,\n",
       " 'along': 504,\n",
       " 'union': 505,\n",
       " 'action': 506,\n",
       " 'shot': 507,\n",
       " 'sent': 508,\n",
       " 'international': 509,\n",
       " 'gave': 510,\n",
       " 'building': 511,\n",
       " 'include': 512,\n",
       " 'World': 513,\n",
       " 'County': 514,\n",
       " 'continue': 515,\n",
       " 'peace': 516,\n",
       " 'interview': 517,\n",
       " 'human': 518,\n",
       " 'local': 519,\n",
       " 'Americans': 520,\n",
       " 'won': 521,\n",
       " 'possible': 522,\n",
       " 'half': 523,\n",
       " 'executive': 524,\n",
       " 'keep': 525,\n",
       " 'become': 526,\n",
       " 'Michael': 527,\n",
       " 'index': 528,\n",
       " 'However': 529,\n",
       " 'refused': 530,\n",
       " 'financial': 531,\n",
       " 'does': 532,\n",
       " 'After': 533,\n",
       " 'Africa': 534,\n",
       " 'again': 535,\n",
       " 'London': 536,\n",
       " 'times': 537,\n",
       " 'Japanese': 538,\n",
       " 'led': 539,\n",
       " 'seven': 540,\n",
       " 'health': 541,\n",
       " 'met': 542,\n",
       " 'judge': 543,\n",
       " 'point': 544,\n",
       " 'working': 545,\n",
       " '50': 546,\n",
       " 'deal': 547,\n",
       " 'morning': 548,\n",
       " 'contract': 549,\n",
       " '13': 550,\n",
       " 'every': 551,\n",
       " 'soldiers': 552,\n",
       " 'For': 553,\n",
       " 'Bank': 554,\n",
       " 'best': 555,\n",
       " 'filed': 556,\n",
       " 'strong': 557,\n",
       " 'plant': 558,\n",
       " 'call': 559,\n",
       " 'face': 560,\n",
       " 'charged': 561,\n",
       " 'making': 562,\n",
       " 'closed': 563,\n",
       " 'British': 564,\n",
       " 'open': 565,\n",
       " 'important': 566,\n",
       " 'leave': 567,\n",
       " 'elections': 568,\n",
       " 'real': 569,\n",
       " 'special': 570,\n",
       " 'yen': 571,\n",
       " 'eight': 572,\n",
       " 'future': 573,\n",
       " 'per': 574,\n",
       " 'planned': 575,\n",
       " 'problem': 576,\n",
       " 'bank': 577,\n",
       " 'Kuwait': 578,\n",
       " 'army': 579,\n",
       " 'further': 580,\n",
       " 'study': 581,\n",
       " '40': 582,\n",
       " 'Iraqi': 583,\n",
       " 'Exchange': 584,\n",
       " 'likely': 585,\n",
       " '4': 586,\n",
       " 'production': 587,\n",
       " 'your': 588,\n",
       " 'proposal': 589,\n",
       " 'proposed': 590,\n",
       " 'town': 591,\n",
       " 'something': 592,\n",
       " 'spending': 593,\n",
       " '18': 594,\n",
       " '16': 595,\n",
       " 'Service': 596,\n",
       " 'free': 597,\n",
       " 'lot': 598,\n",
       " 'wanted': 599,\n",
       " 'taking': 600,\n",
       " 'International': 601,\n",
       " 'Stock': 602,\n",
       " 'nation': 603,\n",
       " 'hospital': 604,\n",
       " '1985': 605,\n",
       " 'His': 606,\n",
       " 'allowed': 607,\n",
       " 'involved': 608,\n",
       " 'toward': 609,\n",
       " 'better': 610,\n",
       " 'final': 611,\n",
       " 'find': 612,\n",
       " 'allow': 613,\n",
       " 'central': 614,\n",
       " '6': 615,\n",
       " 'thousands': 616,\n",
       " 'bid': 617,\n",
       " 'officers': 618,\n",
       " 'Federal': 619,\n",
       " 'sold': 620,\n",
       " 'became': 621,\n",
       " 'legislation': 622,\n",
       " 'release': 623,\n",
       " 'jobs': 624,\n",
       " 'done': 625,\n",
       " 'session': 626,\n",
       " 'once': 627,\n",
       " 'failed': 628,\n",
       " '17': 629,\n",
       " 'seen': 630,\n",
       " 'things': 631,\n",
       " 'shares': 632,\n",
       " 'name': 633,\n",
       " 'minister': 634,\n",
       " 'ordered': 635,\n",
       " 'District': 636,\n",
       " 'house': 637,\n",
       " 'Eastern': 638,\n",
       " 'victims': 639,\n",
       " 'You': 640,\n",
       " 'Moscow': 641,\n",
       " 'based': 642,\n",
       " 'continued': 643,\n",
       " 'large': 644,\n",
       " 'deficit': 645,\n",
       " 'Richard': 646,\n",
       " 'officer': 647,\n",
       " 'meet': 648,\n",
       " 'radio': 649,\n",
       " 'strike': 650,\n",
       " 'white': 651,\n",
       " 'third': 652,\n",
       " 'Democrats': 653,\n",
       " 'community': 654,\n",
       " 'violence': 655,\n",
       " 'big': 656,\n",
       " 'senior': 657,\n",
       " 'run': 658,\n",
       " 'provide': 659,\n",
       " 'approved': 660,\n",
       " 'comment': 661,\n",
       " 'effort': 662,\n",
       " 'situation': 663,\n",
       " 'paid': 664,\n",
       " 'areas': 665,\n",
       " 'quoted': 666,\n",
       " 'War': 667,\n",
       " 'previous': 668,\n",
       " 'cases': 669,\n",
       " 'plane': 670,\n",
       " 'William': 671,\n",
       " 'News': 672,\n",
       " 'hit': 673,\n",
       " 'annual': 674,\n",
       " 'programs': 675,\n",
       " '8': 676,\n",
       " 'behind': 677,\n",
       " 'sources': 678,\n",
       " 'car': 679,\n",
       " 'following': 680,\n",
       " 'brought': 681,\n",
       " 'level': 682,\n",
       " 'calls': 683,\n",
       " 'Bushs': 684,\n",
       " 'side': 685,\n",
       " 'Supreme': 686,\n",
       " 'estimated': 687,\n",
       " 'value': 688,\n",
       " 'markets': 689,\n",
       " 'European': 690,\n",
       " 'convicted': 691,\n",
       " 'within': 692,\n",
       " 'Association': 693,\n",
       " 'Los': 694,\n",
       " 'Iran': 695,\n",
       " 'summit': 696,\n",
       " 'running': 697,\n",
       " 'position': 698,\n",
       " 'recently': 699,\n",
       " 'negotiations': 700,\n",
       " 'look': 701,\n",
       " 'main': 702,\n",
       " 'reached': 703,\n",
       " 'telephone': 704,\n",
       " 'worked': 705,\n",
       " 'evidence': 706,\n",
       " 'speech': 707,\n",
       " 'January': 708,\n",
       " 'Council': 709,\n",
       " 'dollars': 710,\n",
       " 'futures': 711,\n",
       " 'appeared': 712,\n",
       " 'able': 713,\n",
       " 'fall': 714,\n",
       " 'period': 715,\n",
       " 'current': 716,\n",
       " 'computer': 717,\n",
       " 'products': 718,\n",
       " '14': 719,\n",
       " 'About': 720,\n",
       " 'probably': 721,\n",
       " 'November': 722,\n",
       " 'declined': 723,\n",
       " 'rebels': 724,\n",
       " 'issued': 725,\n",
       " 'really': 726,\n",
       " 'full': 727,\n",
       " 'Saudi': 728,\n",
       " 'Defense': 729,\n",
       " '60': 730,\n",
       " 'compared': 731,\n",
       " 'great': 732,\n",
       " 'cause': 733,\n",
       " 'hope': 734,\n",
       " 'African': 735,\n",
       " 'hearing': 736,\n",
       " 'however': 737,\n",
       " 'farmers': 738,\n",
       " 'changes': 739,\n",
       " 'injured': 740,\n",
       " 'ruling': 741,\n",
       " 'Justice': 742,\n",
       " 'center': 743,\n",
       " 'medical': 744,\n",
       " 'thought': 745,\n",
       " 'woman': 746,\n",
       " 'Secretary': 747,\n",
       " 'efforts': 748,\n",
       " 'Board': 749,\n",
       " 'lead': 750,\n",
       " 'Aug': 751,\n",
       " 'signed': 752,\n",
       " 'weapons': 753,\n",
       " 'visit': 754,\n",
       " 'Angeles': 755,\n",
       " 'secretary': 756,\n",
       " 'Center': 757,\n",
       " 'spent': 758,\n",
       " 'anonymity': 759,\n",
       " 'effect': 760,\n",
       " 'thats': 761,\n",
       " 'ended': 762,\n",
       " 'income': 763,\n",
       " 'taxes': 764,\n",
       " 'August': 765,\n",
       " 'Israeli': 766,\n",
       " 'remain': 767,\n",
       " 'summer': 768,\n",
       " 'showed': 769,\n",
       " 'St': 770,\n",
       " 'David': 771,\n",
       " 'measure': 772,\n",
       " 'anything': 773,\n",
       " 'tried': 774,\n",
       " 'clear': 775,\n",
       " 'cash': 776,\n",
       " 'December': 777,\n",
       " 'increased': 778,\n",
       " 'immediately': 779,\n",
       " 'often': 780,\n",
       " 'services': 781,\n",
       " 'governments': 782,\n",
       " 'Chicago': 783,\n",
       " 'forced': 784,\n",
       " 'charge': 785,\n",
       " 'southern': 786,\n",
       " 'II': 787,\n",
       " 'son': 788,\n",
       " 'Jr': 789,\n",
       " 'September': 790,\n",
       " 'Oct': 791,\n",
       " 'legal': 792,\n",
       " 'ever': 793,\n",
       " 'All': 794,\n",
       " 'Other': 795,\n",
       " 'stocks': 796,\n",
       " 'jury': 797,\n",
       " 'primary': 798,\n",
       " 'believed': 799,\n",
       " 'moved': 800,\n",
       " 'buy': 801,\n",
       " 'homes': 802,\n",
       " 'banks': 803,\n",
       " 'considered': 804,\n",
       " 'Soviets': 805,\n",
       " '1990': 806,\n",
       " 'seeking': 807,\n",
       " 'spoke': 808,\n",
       " 'living': 809,\n",
       " 'French': 810,\n",
       " 'Judge': 811,\n",
       " 'rather': 812,\n",
       " 'serious': 813,\n",
       " 'February': 814,\n",
       " 'damage': 815,\n",
       " 'killing': 816,\n",
       " 'firm': 817,\n",
       " '21': 818,\n",
       " 'Foreign': 819,\n",
       " 'himself': 820,\n",
       " 'Most': 821,\n",
       " 'similar': 822,\n",
       " 'fighting': 823,\n",
       " 'doesnt': 824,\n",
       " 'questions': 825,\n",
       " 'flight': 826,\n",
       " 'line': 827,\n",
       " 'Paul': 828,\n",
       " 'murder': 829,\n",
       " '24': 830,\n",
       " 'operations': 831,\n",
       " 'opened': 832,\n",
       " 'equipment': 833,\n",
       " 'hold': 834,\n",
       " 'different': 835,\n",
       " 'returned': 836,\n",
       " 'parties': 837,\n",
       " 'live': 838,\n",
       " 'cant': 839,\n",
       " 'civil': 840,\n",
       " 'decided': 841,\n",
       " '22': 842,\n",
       " 'denied': 843,\n",
       " 'investors': 844,\n",
       " 'needed': 845,\n",
       " 'caused': 846,\n",
       " 'Two': 847,\n",
       " 'looking': 848,\n",
       " 'pm': 849,\n",
       " 'department': 850,\n",
       " 'followed': 851,\n",
       " 'result': 852,\n",
       " 'relations': 853,\n",
       " 'AIDS': 854,\n",
       " 'almost': 855,\n",
       " 'October': 856,\n",
       " 'space': 857,\n",
       " 'ground': 858,\n",
       " 'Jones': 859,\n",
       " 'parents': 860,\n",
       " 'person': 861,\n",
       " '1984': 862,\n",
       " 'residents': 863,\n",
       " 'test': 864,\n",
       " 'process': 865,\n",
       " 'bring': 866,\n",
       " 'cent': 867,\n",
       " 'fair': 868,\n",
       " 'Nov': 869,\n",
       " 'identified': 870,\n",
       " 'offered': 871,\n",
       " 'sell': 872,\n",
       " 'Army': 873,\n",
       " 'spokeswoman': 874,\n",
       " 'labor': 875,\n",
       " 'together': 876,\n",
       " 'leading': 877,\n",
       " 'north': 878,\n",
       " 'care': 879,\n",
       " 'Were': 880,\n",
       " 'Baker': 881,\n",
       " 'investment': 882,\n",
       " 'Dec': 883,\n",
       " 'Jan': 884,\n",
       " 'old': 885,\n",
       " 'Gov': 886,\n",
       " 'nuclear': 887,\n",
       " 'guilty': 888,\n",
       " 'role': 889,\n",
       " 'funds': 890,\n",
       " '7': 891,\n",
       " 'using': 892,\n",
       " 'young': 893,\n",
       " 'quarter': 894,\n",
       " '19': 895,\n",
       " 'claims': 896,\n",
       " 'raised': 897,\n",
       " 'alleged': 898,\n",
       " 'getting': 899,\n",
       " 'started': 900,\n",
       " 'accident': 901,\n",
       " 'de': 902,\n",
       " 'start': 903,\n",
       " 'independent': 904,\n",
       " 'costs': 905,\n",
       " 'source': 906,\n",
       " 'panel': 907,\n",
       " 'analysts': 908,\n",
       " 'cars': 909,\n",
       " 'Force': 910,\n",
       " 'staff': 911,\n",
       " 'inflation': 912,\n",
       " 'remains': 913,\n",
       " 'growth': 914,\n",
       " 'team': 915,\n",
       " 'rule': 916,\n",
       " 'history': 917,\n",
       " 'While': 918,\n",
       " 'press': 919,\n",
       " 'demand': 920,\n",
       " 'conditions': 921,\n",
       " 'noted': 922,\n",
       " 'Many': 923,\n",
       " 'win': 924,\n",
       " 'northern': 925,\n",
       " 'wants': 926,\n",
       " 'nothing': 927,\n",
       " 'S': 928,\n",
       " 'soon': 929,\n",
       " 'turned': 930,\n",
       " 'Security': 931,\n",
       " 'thing': 932,\n",
       " 'Central': 933,\n",
       " 'candidates': 934,\n",
       " 'Pentagon': 935,\n",
       " 'results': 936,\n",
       " 'Gulf': 937,\n",
       " 'research': 938,\n",
       " 'safety': 939,\n",
       " 'figures': 940,\n",
       " 'Western': 941,\n",
       " 'orders': 942,\n",
       " 'although': 943,\n",
       " 'movement': 944,\n",
       " 'aircraft': 945,\n",
       " 'claimed': 946,\n",
       " 'UN': 947,\n",
       " 'land': 948,\n",
       " 'body': 949,\n",
       " 'border': 950,\n",
       " 'candidate': 951,\n",
       " 'governor': 952,\n",
       " 'stop': 953,\n",
       " 'themselves': 954,\n",
       " 'doing': 955,\n",
       " 'Last': 956,\n",
       " 'dropped': 957,\n",
       " 'pressure': 958,\n",
       " 'Sept': 959,\n",
       " 'question': 960,\n",
       " 'request': 961,\n",
       " 'Attorney': 962,\n",
       " 'Panama': 963,\n",
       " 'yet': 964,\n",
       " 'documents': 965,\n",
       " 'race': 966,\n",
       " 'cities': 967,\n",
       " 'addition': 968,\n",
       " 'named': 969,\n",
       " 'Labor': 970,\n",
       " 'saw': 971,\n",
       " 'date': 972,\n",
       " 'estate': 973,\n",
       " 'kind': 974,\n",
       " 'Street': 975,\n",
       " 'companys': 976,\n",
       " 'China': 977,\n",
       " 'emergency': 978,\n",
       " 'drugs': 979,\n",
       " 'church': 980,\n",
       " 'fired': 981,\n",
       " '31': 982,\n",
       " 'Wall': 983,\n",
       " 'having': 984,\n",
       " 'crisis': 985,\n",
       " 'tons': 986,\n",
       " 'wrote': 987,\n",
       " '28': 988,\n",
       " 'letter': 989,\n",
       " 'why': 990,\n",
       " 'try': 991,\n",
       " 'voters': 992,\n",
       " 'heart': 993,\n",
       " 'sale': 994,\n",
       " 'debt': 995,\n",
       " 'loss': 996,\n",
       " 'decline': 997,\n",
       " 'trip': 998,\n",
       " 'begin': 999}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2Idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SkipGram(20, 5)\n",
    "focus = torch.autograd.Variable(torch.LongTensor([0]))\n",
    "context = torch.autograd.Variable(torch.LongTensor([0]))\n",
    "allEmbeddingIdxs = torch.autograd.Variable(torch.LongTensor([np.arange(0,20)]))\n",
    "\n",
    "\n",
    "embedCenter = model.centerEmbeddings(focus).view((1, -1))\n",
    "embedContext = model.contextEmbeddings(context).view((1, -1))\n",
    "allContextEmbeddings = model.contextEmbeddings(allEmbeddingIdxs).squeeze()\n",
    "num = torch.exp(torch.mm(embedContext, torch.t(embedCenter)))\n",
    "denom = torch.exp(torch.mm(allContextEmbeddings, torch.t(embedCenter))).sum()\n",
    "logProb = torch.log(num/denom)\n",
    "# score = torch.mm(embed_focus, torch.t(embed_ctx))\n",
    "# log_probs = nn.functional.logsigmoid(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.3745]], grad_fn=<LogBackward>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0443]], grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(logProb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2262]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(model.contextEmbeddings[0].view((1, -1)), torch.t(embedCenter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5118]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(model.contextEmbeddings[1].view((1, -1)), torch.t(embedCenter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8398, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denom.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4778, -0.8661, -0.2023, -0.4160, -0.7439], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0176,  1.2786, -0.6968,  0.0286, -0.3026],\n",
       "        [-0.7574, -0.6157, -1.5389,  0.2034,  0.7011],\n",
       "        [-1.8946,  0.6623,  1.1087,  1.1338,  0.4926],\n",
       "        [-0.5137, -0.0341,  0.4722, -2.4054,  1.2151],\n",
       "        [ 1.8809,  0.3533, -0.1564, -0.0763, -0.0259],\n",
       "        [-0.1247,  0.1701, -0.1384,  0.6218, -0.7557],\n",
       "        [ 2.1279,  0.4135, -0.1488, -1.1001,  0.6704],\n",
       "        [ 1.2196,  0.0604,  0.3532, -0.1608,  0.6684],\n",
       "        [ 0.8554, -2.7541,  0.9763, -1.4335, -0.6975],\n",
       "        [-0.2119,  0.2934,  0.6183,  2.2487, -0.9169],\n",
       "        [ 1.0163,  0.9303, -1.9464,  2.3819, -0.9266],\n",
       "        [-0.5092, -0.3674, -0.6601,  0.6410,  0.2617],\n",
       "        [-0.3888, -1.4945,  0.2670,  0.3349, -0.4055],\n",
       "        [ 1.7100,  0.1870,  1.2487, -0.8660, -0.8817],\n",
       "        [ 0.2530, -1.1636, -0.4466,  0.7912, -2.3919],\n",
       "        [-0.9038, -0.8937,  0.4035, -0.8282,  0.9962],\n",
       "        [-0.3595,  0.0266,  1.8298, -1.8399,  1.0233],\n",
       "        [ 0.0946, -0.3587,  1.8775,  1.1186, -0.6393],\n",
       "        [ 0.6724,  0.8932, -0.2031,  0.0671, -0.2851],\n",
       "        [ 0.8563, -0.4952, -0.8984, -0.1976, -1.9564]], requires_grad=True)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.centerEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 5])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allContext = torch.autograd.Variable(torch.LongTensor(np.arange(0, 20)))\n",
    "model.contextEmbeddings(context).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Test SkipGram===\n",
      "Accuracy: 80.1% (269/336)\n"
     ]
    }
   ],
   "source": [
    "def test_skipgram(testData, model, word2Idx):\n",
    "    print('====Test SkipGram===')\n",
    "    correct_ct = 0\n",
    "    for in_w, out_w in testData:\n",
    "        in_w_var = torch.autograd.Variable(torch.LongTensor([word2Idx[in_w]]))\n",
    "        out_w_var = torch.autograd.Variable(torch.LongTensor([word2Idx[out_w]]))\n",
    "\n",
    "        model.zero_grad()\n",
    "        log_probs = model(in_w_var, out_w_var)\n",
    "        prob = torch.exp(log_probs)\n",
    "#         print(torch.max(log_probs.data, 1))\n",
    "#         _, predicted = torch.max(log_probs.data, 1)\n",
    "#         predicted = predicted[0]\n",
    "#         print(log_probs.data)\n",
    "        if prob > 0.5:#predicted == 1:\n",
    "            correct_ct += 1\n",
    "\n",
    "    print('Accuracy: {:.1f}% ({}/{})'.format(correct_ct/len(testData)*100, correct_ct, len(testData)))\n",
    "\n",
    "\n",
    "test_skipgram(idxPairsTest, sg_model, word2Idx = word2Idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_skipgram(testData, model, word2Idx):\n",
    "    print('====Test SkipGram===')\n",
    "    correct_ct = 0\n",
    "    for in_w, out_w in testData:\n",
    "        in_w_var = torch.autograd.Variable(torch.LongTensor([word2Idx[in_w]]))\n",
    "        out_w_var = torch.autograd.Variable(torch.LongTensor([word2Idx[out_w]]))\n",
    "\n",
    "        model.zero_grad()\n",
    "        log_probs = model(in_w_var, out_w_var)\n",
    "        prob = torch.exp(log_probs)\n",
    "#         print(torch.max(log_probs.data, 1))\n",
    "        _, predicted = torch.max(log_probs.data, 1)\n",
    "#         predicted = predicted[0]\n",
    "#         print(log_probs.data)\n",
    "        if prob > 0.5:#predicted == 1:\n",
    "            correct_ct += 1\n",
    "\n",
    "    print('Accuracy: {:.1f}% ({}/{})'.format(correct_ct/len(testData)*100, correct_ct, len(testData)))\n",
    "\n",
    "\n",
    "test_skipgram(idxPairsTest, sg_model, word2Idx = word2Idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-2.1355]), tensor([0]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(log_probs.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "max() received an invalid combination of arguments - got (int, int), but expected one of:\n * (Tensor input)\n * (Tensor input, Tensor other, Tensor out)\n * (Tensor input, int dim, bool keepdim, tuple of Tensors out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-2cf7e100665b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: max() received an invalid combination of arguments - got (int, int), but expected one of:\n * (Tensor input)\n * (Tensor input, Tensor other, Tensor out)\n * (Tensor input, int dim, bool keepdim, tuple of Tensors out)\n"
     ]
    }
   ],
   "source": [
    "torch.max(log_probs.data, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Domains Affect Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Computational Methods)",
   "language": "python",
   "name": "compmeth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
