{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lab 10: Word Embeddings\n",
    "\n",
    "## Introduction\n",
    "In this lab you'll learn how [Skip-Gram](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) is implemented. The skip-gram model works by training a single layer neural network to predict the surrounding words given a center word. The goal is to have a network that learns which words are more likely to appear in the context of a given word. The model is trained using word pairs given a center word, and the context words that appear within a fixed window around the word. The example below shows the word pairs created using different center words and a window size of 2\n",
    "\n",
    "![trainingWordPairs](http://mccormickml.com/assets/word2vec/training_data.png)\n",
    "\n",
    "\n",
    "[Cool](https://www.youtube.com/watch?v=A8q8PXoJwVk). So how does this model actually work? The model has two main moving parts, a set of weights representing the center word and context word embeddings. Each matrix has separate weights and $\\in R^{v, e}$ where v is the size of the vocabulary and e is the embedding dimension (a hyperparameter you choose).\n",
    "\n",
    "The model learns to minimize the following function.\n",
    "\n",
    "$$L = log(\\sigma(v_{c_o}v_{c_c}^{T})) + \\sum_{c_o,c_c \\in \\bar{D}} log(\\sigma(-v_{c_o}v_{c_c}^{T}))$$\n",
    "\n",
    "Where $c_o$ and $c_c$ \n",
    "\n",
    "## Installs\n",
    "tqdm is a nice wrapper for loops to check your progress as you go\n",
    "\n",
    "conda install -c conda-forge tqdm\n",
    "\n",
    "ipywidgets makes tqdm look pretty\n",
    "\n",
    "conda install -c conda-forge ipywidgets\n",
    "\n",
    "Tokenization and NLP toolkit\n",
    "\n",
    "conda install -c anaconda nltk \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Janitorial Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCorpus = [\"First of all, quit grinnin’ like an idiot. Indians ain’t supposed to smile like that. Get stoic.\",\n",
    "             \"No. Like this. You gotta look mean, or people won’t respect you.\",\n",
    "              \" people will run all over you if you don’t look mean.\",\n",
    "              \"You gotta look like a warrior. You gotta look like you just came back from killing a buffalo.\",\n",
    "             \"But our tribe never hunted buffalo. We were fishermen.\"\n",
    "             \"What? You wanna look like you just came back from catching a fish?\",\n",
    "             \"This ain’t dances with salmon, you know. Thomas, you gotta look like a warrior.\"]\n",
    "maxDocs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ob2285/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 pub med abstracts\n"
     ]
    }
   ],
   "source": [
    "# Read in pubmed corpus into a text file\n",
    "\n",
    "import glob\n",
    "pubMedDataFolderPath = \"data/pubMed_corpus/\"\n",
    "pubMedDataFiles = glob.glob(pubMedDataFolderPath + \"*.txt\")\n",
    "pubMedCorpus = [\"\"]*len(pubMedDataFiles)\n",
    "for idx, pubMedDataPath in enumerate(pubMedDataFiles):\n",
    "    with open(pubMedDataPath, \"r\") as pubMedFile:\n",
    "        text = pubMedFile.read().strip()\n",
    "        pubMedCorpus[idx] = text\n",
    "pubMedCorpus = pubMedCorpus[0:maxDocs]\n",
    "print(\"{} pub med abstracts\".format(len(pubMedCorpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 ap articles\n"
     ]
    }
   ],
   "source": [
    "# Read in the ap corpus\n",
    "apTextFile = \"data/ap.txt\"\n",
    "apCorpus = []\n",
    "readText = False\n",
    "with open(apTextFile) as apDataFile:\n",
    "    for line in apDataFile:\n",
    "        if readText:\n",
    "            apCorpus.append(line.strip())\n",
    "            readText = False\n",
    "        if line == \"<TEXT>\\n\":\n",
    "            readText = True\n",
    "apCorpus = apCorpus[0:maxDocs]\n",
    "print(\"{} ap articles\".format(len(apCorpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ob2285/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ob2285/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "def removePunctuation(myStr):\n",
    "    excludedCharacters = string.punctuation + \"’\"\n",
    "    newStr = \"\".join(char for char in myStr if char not in excludedCharacters)\n",
    "    return(newStr)\n",
    "def removeStopWords(tokenList):\n",
    "    newTokenList = [tok for tok in tokenList if tok not in stopwords.words('english')]\n",
    "    return(newTokenList)\n",
    "def cleanDocStr(docStr):\n",
    "    docStr = docStr.lower()\n",
    "    docStr = removePunctuation(docStr)\n",
    "    docStr = re.sub('\\d', '%d%', docStr)\n",
    "    docStrTokenized = nltk.tokenize.word_tokenize(docStr)\n",
    "    myStopWords = set(stopwords.words('english'))\n",
    "    docStrTokenized = [tok for tok in docStrTokenized if tok not in myStopWords]\n",
    "    return(docStrTokenized)\n",
    "def tokenize_corpus(corpus):\n",
    "    tokens = [cleanDocStr(x) for x in corpus]\n",
    "    return tokens\n",
    "\n",
    "apCorpusTokenized = tokenize_corpus(apCorpus)\n",
    "pubMedCorpusTokenized = tokenize_corpus(pubMedCorpus)\n",
    "testCorpusTokenized = tokenize_corpus(testCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building ap corpus vocabulary\n",
      "Vocab size: 6511\n",
      "ap data tokenized in 0.12973356246948242 seconds\n",
      "\n",
      "Building pubMed corpus vocabulary\n",
      "Vocab size: 3529\n",
      "pubmed data tokenized in 0.06275820732116699 seconds\n",
      "\n",
      "Building test corpus vocabulary\n",
      "Vocab size: 38\n",
      "test data tokenized in 0.00023603439331054688 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from collections import Counter\n",
    "\n",
    "minVocabOccurence = 5\n",
    "\n",
    "def extractVocabMappers(tokenizedCorpus, vocabSizeMax = None, minVocabOccurence = 0):\n",
    "    UNK = \"<UNK>\"\n",
    "    flattenedCorpus = [item for sublist in tokenizedCorpus for item in sublist]\n",
    "    wordCounts = Counter(flattenedCorpus).most_common()\n",
    "    wordCounts = [(w, c) for w,c in wordCounts if c > minVocabOccurence]\n",
    "#     wordCounts = wordCounts.most_common(vocabSizeMax)\n",
    "    vocabulary = [word for word, count in wordCounts]\n",
    "    \n",
    "    # below is more readable but significantly slower code\n",
    "    if False:\n",
    "        vocabulary = []\n",
    "        for sentence in tqdm(tokenizedCorpus):\n",
    "            for token in sentence:\n",
    "                if token not in vocabulary:\n",
    "                    vocabulary.append(token)\n",
    "    vocabulary.append(UNK)\n",
    "    print(\"Vocab size: {}\".format(len(vocabulary)))\n",
    "    word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "    idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "    newTokenizedCorpus = []# all words missing from vocab replaced with <UNK>\n",
    "    for doc in tokenizedCorpus:\n",
    "        newDoc = [word if word in word2idx else UNK for word in doc]\n",
    "        newTokenizedCorpus.append(newDoc)\n",
    "    return(word2idx, idx2word, wordCounts, newTokenizedCorpus)\n",
    "\n",
    "start = time.time()\n",
    "print(\"Building ap corpus vocabulary\")\n",
    "word2Idx_ap, idx2Word_ap, vocabCount_ap, finalTokenizedCorpus_ap = extractVocabMappers(apCorpusTokenized,\n",
    "#                                                                                        vocabSizeMax = maxVocabSize,\n",
    "                                                                                      minVocabOccurence = minVocabOccurence)\n",
    "print(\"ap data tokenized in {} seconds\\n\".format(time.time() - start))\n",
    "start = time.time()\n",
    "print(\"Building pubMed corpus vocabulary\")\n",
    "word2Idx_pubMed, idx2Word_pubMed, vocabCount_pubMed, finalTokenizedCorpus_pubMed = extractVocabMappers(pubMedCorpusTokenized,\n",
    "#                                                                                                        vocabSizeMax = maxVocabSize,\n",
    "                                                                                                      minVocabOccurence = minVocabOccurence)\n",
    "print(\"pubmed data tokenized in {} seconds\\n\".format(time.time() - start))\n",
    "start = time.time()\n",
    "print(\"Building test corpus vocabulary\")\n",
    "word2Idx_test, idx2Word_test, vocabCount_test, finalTokenizedCorpus_test = extractVocabMappers(testCorpusTokenized,\n",
    "#                                                                                                vocabSizeMax = maxVocabSize,\n",
    "                                                                                              minVocabOccurence = 0)\n",
    "print(\"test data tokenized in {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### BATCH VERSION ######\n",
    "\n",
    "\n",
    "def generateObservations(tokenizedCorpus, word2Idx):\n",
    "    \"\"\"\n",
    "    Decription: Iterates through every token in the corpus and creates a (center, context)\n",
    "        pair for each context word in the window on either side of the center word.\n",
    "    Input:\n",
    "        tokenizedCorpus (list(list(str))): A list where each index is a document from the corpus.\n",
    "            Each document is further tokenized into a list of tokens. \n",
    "            [doc1, doc2,...] where doc1 = [tok1, tok2, ...]\n",
    "        word2Idx\n",
    "    Output:\n",
    "        idxPairs (lsit(tuples)): A list of tuples where each tuple is a (center, context word)\n",
    "    \"\"\"\n",
    "    window_size = 5\n",
    "    idxPairs = []\n",
    "    for sentence in tokenizedCorpus:\n",
    "        for center_word_pos in range(len(sentence)):\n",
    "            # for each window position\n",
    "            for w in range(-window_size, window_size + 1):\n",
    "                context_word_pos = center_word_pos + w\n",
    "                # make sure not jump out sentence\n",
    "                if context_word_pos < 0 or context_word_pos >= len(sentence) or center_word_pos == context_word_pos:\n",
    "                    continue\n",
    "                idxPairs.append((sentence[center_word_pos], sentence[context_word_pos]))\n",
    "\n",
    "    idxPairs = np.array(idxPairs)\n",
    "    return(idxPairs)\n",
    "\n",
    "\n",
    "def generateWordSamplingProb(vocabCount, word2Idx):\n",
    "    \"\"\"\n",
    "    Decription: Generates a unigram table to sample data from. The unigram table\n",
    "        should contains the index of every vocab index multiple times. The number\n",
    "        of times an element appears is dictated by its sample probability. The unigram\n",
    "        table can the be sampled. \n",
    "    Input:\n",
    "        vocabCount (list(tuples)): A list of tuples mapping each vocab to its count in the\n",
    "            corpus\n",
    "        word2Idx (dict): A dictionary mapping words to their integer IDs\n",
    "    Output:\n",
    "        wordSampleProbs (list(int)): A list of integers as described above. For example\n",
    "        in a 3 word vocabulary it might look something like [0,0,1,1,1,1,1,1,1,2].\n",
    "        Sampling from the previous example will mean that 0 is sampled 2/10 times,\n",
    "        1 is sampled 7/10 times, and 2 is sampled 1/10 times.\n",
    "    \"\"\"\n",
    "    \n",
    "#     wordSampleProbs = [0.0]*len(vocabCount)\n",
    "#     numWords = np.sum([count**0.75 for word, count in vocabCount])\n",
    "#     for idx in range(len(vocabCount)):\n",
    "#         w,c = vocabCount[idx]\n",
    "#         wordSampleProbs[word2Idx[w]] = (c**0.75)/(numWords)\n",
    "    wordSampleProbs = []\n",
    "    numWords = np.sum([count for word, count in vocabCount])\n",
    "    for w,c in vocabCount:\n",
    "        wordSampleProbs.extend([word2Idx[w]] * int(((c/numWords)**0.75)/0.001))\n",
    "    return(wordSampleProbs)\n",
    "    \n",
    "class SkipGram(nn.Module):\n",
    "    \"\"\"\n",
    "    Decription: Instantiates and implements the forward pass of the skip gram\n",
    "        algorithm with negative sampling.\n",
    "    Input:\n",
    "        vocabSize (int): Number of words to create embeddings for\n",
    "        embedSize (int): Dimension of word embeddings\n",
    "        word2Idx (dict): A dictionary mapping words to their integer IDs\n",
    "    Output:\n",
    "    \"\"\"\n",
    "    def __init__(self, vocabSize, embedSize, vocabCount, word2Idx):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.vocabSize = vocabSize\n",
    "        self.word2Idx = word2Idx\n",
    "        self.centerEmbeddings = nn.Parameter(torch.randn(vocabSize,\n",
    "                                                     embedSize).float(), requires_grad=True)\n",
    "        self.contextEmbeddings = nn.Parameter(torch.randn(vocabSize,\n",
    "                                                      embedSize).float(), requires_grad=True)\n",
    "        \n",
    "#         initrange = (2.0 / (vocabSize + embedSize)) ** 0.5  # Xavier init\n",
    "        nn.init.xavier_uniform_(self.contextEmbeddings)\n",
    "#         self.output_emb.weight.data.uniform_(-0, 0)\n",
    "        nn.init.xavier_uniform_(self.centerEmbeddings)\n",
    "#         nn.init.uniform_(self.centerEmbeddings, -0,0)\n",
    "        \n",
    "        self.wordSampleProbs = generateWordSamplingProb(vocabCount, word2Idx)\n",
    "        self.logSigmoid = nn.LogSigmoid()\n",
    "    def getNegSample(self, k, centerWords):\n",
    "        \"\"\"\n",
    "        Decription: Randomly selects negative samples from the vocabulary. USes\n",
    "            self.wordSampleProbs in order to sample words. \n",
    "        Input:\n",
    "            k (int): Number of negative samples to select\n",
    "            centerWords (list(str)): A list of the string center words. There should\n",
    "                be batchSize of these.\n",
    "        Output:\n",
    "            negSamples (list(numpyArray)): A list of numpy arrays where each numpy array\n",
    "                contains the indices of negative samples. There are batchSize numpy arrays\n",
    "        \"\"\"\n",
    "        vocabSizeWithoutUnk = self.vocabSize - 1\n",
    "        negSamples = []\n",
    "        for centerWord in centerWords:\n",
    "            negSample = random.sample(self.wordSampleProbs, k)\n",
    "            while self.word2Idx[centerWord] in negSample:\n",
    "                negSample = random.sample(self.wordSampleProbs, k)\n",
    "            negSamples.append(negSample)\n",
    "        return(negSamples)\n",
    "    def forward(self, center, context, negSampleIndices):\n",
    "        \"\"\"\n",
    "        Decription Forward pass for the skipgram model. \n",
    "        Input:\n",
    "            center (list(int)): A list of word integer IDs indicating all\n",
    "                batchSize center words. Matches one to one with context\n",
    "            context (list(int)): A list of word integer IDs indicating all\n",
    "                batchSize context words. Matches one to one with center\n",
    "            negSampleIndices (list(numpyArray)): A list of numpy arrays where\n",
    "                each numpy array contains the indices of negative samples.\n",
    "                There are batchSize numpy arrays. Returned by getNegSample()\n",
    "        Output:\n",
    "            logProb (tensor): The loss over the entire batch.\n",
    "        \"\"\"\n",
    "        embedCenter = self.centerEmbeddings[center]#.view((1, -1))\n",
    "        embedContext = self.contextEmbeddings[context]#.view((1, -1))       \n",
    "        posVal = self.logSigmoid(torch.sum(embedContext * embedCenter, dim = 1)).squeeze()\n",
    "        negSampleIndices = torch.autograd.Variable(torch.LongTensor(negSampleIndices))\n",
    "#         negVal = torch.bmm(self.contextEmbeddings[negSampleIndices], embedCenter.unsqueeze(2)).squeeze(2)\n",
    "#         negVal = self.logSigmoid(-torch.sum(negVal, dim = 1)).squeeze()\n",
    "        negVal = torch.bmm(self.contextEmbeddings[negSampleIndices], embedCenter.unsqueeze(2)).squeeze()\n",
    "        negVal = torch.sum(self.logSigmoid(-negVal), dim = 1)\n",
    "#         print(negVal.shape)\n",
    "#         1/0\n",
    "        logProb = -(posVal + negVal).mean()\n",
    "        return(logProb)\n",
    "\n",
    "\n",
    "def train_skipgram(embeddingSize, trainingData, vocabCount, word2Idx, idx2Word,\n",
    "                   k, referenceWords, batchSize = 1024):\n",
    "    \"\"\"\n",
    "    Decription: Instantiates and trains a skipgam model. The forward pass of the skipgram mode\n",
    "        handles the forward pass so all you have to do here is handle the loss, and\n",
    "        updating the weights.\n",
    "    Input:\n",
    "        embeddingSize (int): Size of each word embedding\n",
    "        trainingData (list(tuples)): A list of tuples generated by generateWordSamplingProb()\n",
    "            where each tuple is a center and context word\n",
    "        vocabCount (list(tuples)): A list of tuples mapping each vocab to its count in the\n",
    "            corpus\n",
    "        word2Idx (dict): A dictionary mapping each word to its integer ID\n",
    "        idx2Word (dict): A dictionary mapping each integer ID to its word\n",
    "        k (int): Dictates the number of sampls used during negative sampling\n",
    "        referenceWords (list(str)): A list of words to compare word embeddings for\n",
    "        batchSize (int): The number of (center, context) words to run through each forward\n",
    "            pass of the skipgram model.\n",
    "    Output:\n",
    "        model (SkipGram): The final trained SkipGram model\n",
    "    \"\"\"\n",
    "    print(\"training on {} observations\".format(len(trainingData)))\n",
    "    loss_fn = nn.MSELoss()\n",
    "    model = SkipGram(vocabSize = len(word2Idx), embedSize = embeddingSize,\n",
    "                     vocabCount = vocabCount, word2Idx = word2Idx)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    listNearestWords(model = model, idx2Word = idx2Word,\n",
    "     referenceWords = referenceWords, topN = 5)\n",
    "    #         listNearestWords(model = model, idx2Word = idx2Word,\n",
    "#                  referenceWords = referenceWords, topN = 5)\n",
    "    for epoch in tqdm_notebook(range(n_epoch), position = 0):\n",
    "        total_loss = .0\n",
    "        avgLoss = 0.0\n",
    "        iteration = 0\n",
    "        for step in tqdm_notebook(range(0, len(trainingData), batchSize), position = 1):\n",
    "            endIdx = np.min([(step+batchSize), len(trainingData)])\n",
    "            myBatch = trainingData[step:(step+batchSize)]\n",
    "            centerWords = [elem[0] for elem in myBatch]\n",
    "            contextWords = [elem[1] for elem in myBatch]\n",
    "            negSamples = model.getNegSample(k = k, centerWords = centerWords)\n",
    "            centerIDs = [word2Idx[idx] for idx in centerWords]\n",
    "            contextIDs = [word2Idx[idx] for idx in contextWords]\n",
    "            model.zero_grad()\n",
    "            loss = model(centerIDs, contextIDs, negSampleIndices = negSamples)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.data.numpy()\n",
    "            avgLoss += loss.data.numpy()\n",
    "            iteration += 1\n",
    "            if iteration % 500 == 0:\n",
    "                avgLoss = avgLoss/(500)\n",
    "                print(\"avg loss: {}\".format(avgLoss))\n",
    "#                 avgLoss = 0.0\n",
    "#             if iteration % 2000 == 0:\n",
    "#                 listNearestWords(model = model, idx2Word = idx2Word,\n",
    "#                  referenceWords = referenceWords, topN = 5)\n",
    "        print(\"Loss at epoch {}: {}\".format(epoch, total_loss/iteration))\n",
    "        if epoch % 1 == 0 and epoch != 0:\n",
    "            listNearestWords(model = model, idx2Word = idx2Word,\n",
    "                         referenceWords = referenceWords, topN = 5)\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "def listNearestWords(model, idx2Word, referenceWords, topN):\n",
    "    \"\"\"\n",
    "    Decription: Lists the topN closes words by cosine distance to each word in referenceWords\n",
    "    Input:\n",
    "        model (SkipGram): The final trained SkipGram model\n",
    "        idx2Word (dict): A dictionary mapping each integer ID to its word\n",
    "        referenceWords (list(str)): A list of words in the vocabulary of the model\n",
    "        topN (int): The number of closest words to print\n",
    "    Output:\n",
    "        None: Just prints\n",
    "    \"\"\"\n",
    "    assert len(idx2Word) == len(model.word2Idx), \"Possibly passed in two different vocabularies\"\n",
    "    embeddings = model.centerEmbeddings.data.numpy()\n",
    "    distMat = cdist(embeddings, embeddings, metric = \"cosine\")\n",
    "    for word in referenceWords:\n",
    "        wordIdx = model.word2Idx[word]\n",
    "        closestIndices = np.argsort(distMat[wordIdx,:])[0:topN]\n",
    "        closestWords = [(idx2Word[idx], distMat[wordIdx, idx]) for idx in closestIndices]\n",
    "        for elem in closestWords:\n",
    "            print(elem)\n",
    "        print(\"*\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# embd_size = 100\n",
    "# learning_rate = 0.001\n",
    "# n_epoch = 60\n",
    "# idxPairsTest = generateObservations(tokenizedCorpus = finalTokenizedCorpus_test, word2Idx = word2Idx_test)\n",
    "# sg_model, sg_losses = train_skipgram(embeddingSize = 5, trainingData = idxPairsTest, vocabCount = vocabCount_test,\n",
    "#                                      word2Idx = word2Idx_test, k = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on 2958526 observations\n",
      "('bush', 0.0)\n",
      "('expects', 0.6522620280364388)\n",
      "('eliminating', 0.6529257515186815)\n",
      "('print', 0.6627602020945023)\n",
      "('benefits', 0.6694153864346877)\n",
      "**************************************************\n",
      "\n",
      "('soviet', 0.0)\n",
      "('major', 0.6058653020184896)\n",
      "('truly', 0.633406660396931)\n",
      "('choir', 0.6366973768105313)\n",
      "('faster', 0.6377779374912153)\n",
      "**************************************************\n",
      "\n",
      "('president', 0.0)\n",
      "('say', 0.5558501684833579)\n",
      "('energy', 0.6587206105332978)\n",
      "('fla', 0.6671704308702158)\n",
      "('palestinians', 0.6765234901711763)\n",
      "**************************************************\n",
      "\n",
      "('military', 0.0)\n",
      "('intensive', 0.6590037380660443)\n",
      "('pace', 0.6629391373375562)\n",
      "('dinner', 0.6719848680174947)\n",
      "('supreme', 0.6730046375896891)\n",
      "**************************************************\n",
      "\n",
      "('american', 2.220446049250313e-16)\n",
      "('depressed', 0.5944903728251656)\n",
      "('conservative', 0.6536432475183498)\n",
      "('wedding', 0.6826734932418118)\n",
      "('rule', 0.6890044658155274)\n",
      "**************************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e9e576d093a49f3b9dcfcf797f26093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=60), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe099b67b09476d81679768653bb67e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2890), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddingSize = 100\n",
    "learning_rate = 0.1\n",
    "n_epoch = 60\n",
    "idxPairsAP = generateObservations(tokenizedCorpus = finalTokenizedCorpus_ap, word2Idx = word2Idx_ap)\n",
    "sg_model_ap, sg_losses_ap = train_skipgram(embeddingSize = embeddingSize, trainingData = idxPairsAP,\n",
    "                                     vocabCount = vocabCount_ap,\n",
    "                                     word2Idx = word2Idx_ap, idx2Word = idx2Word_ap, k = 20,\n",
    "                                          referenceWords = [\"bush\", \"soviet\", \"president\", \"military\", \"american\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embeddingSize = 50\n",
    "learning_rate = 0.001\n",
    "n_epoch = 3\n",
    "idxPairsPubMed = generateObservations(tokenizedCorpus = finalTokenizedCorpus_pubMed, word2Idx = word2Idx_pubMed)\n",
    "sg_model_pubMed, sg_losses_pubMed = train_skipgram(embeddingSize = embeddingSize, trainingData = idxPairsPubMed,\n",
    "                                     vocabCount = vocabCount_pubMed,\n",
    "                                     word2Idx = word2Idx_pubMed, idx2Word = idx2Word_pubMed, k = 15,\n",
    "                                                  referenceWords = [\"clinical\", \"obesity\", \"microbial\", \"microbiome\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Domains Affect Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg time: 5.7178974151611327e-05\n",
      "torch.Size([1000, 1])\n",
      "avg time: 2.7730941772460937e-05\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "w1 = nn.Parameter(torch.randn(1000, 100).float(), requires_grad=True)\n",
    "w2 = nn.Parameter(torch.randn(1000,  100).float(), requires_grad=True)\n",
    "nIters = 1000\n",
    "negSampleSize = 1000\n",
    "start = time.time()\n",
    "for i in range(1000):\n",
    "    temp = torch.mm(w2[0:negSampleSize], torch.t(w2[0].view(1, -1)))\n",
    "print(\"avg time: {}\".format((time.time() - start)/nIters))\n",
    "print(temp.shape)\n",
    "\n",
    "\n",
    "\n",
    "w1 = w1.data.numpy()\n",
    "w2 = w2.data.numpy()\n",
    "start = time.time()\n",
    "for i in range(1000):\n",
    "    temp = np.matmul(w2[0:negSampleSize], w1[0])\n",
    "print(\"avg time: {}\".format((time.time() - start)/nIters))\n",
    "print(temp.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg time: 1.8839120864868163e-05\n",
      "torch.Size([15, 1])\n"
     ]
    }
   ],
   "source": [
    "w1 = nn.Parameter(torch.randn(1000, 100).float(), requires_grad=True)\n",
    "w2 = nn.Parameter(torch.randn(1000,  100).float(), requires_grad=True)\n",
    "nIters = 1000\n",
    "negSampleSize = 15\n",
    "start = time.time()\n",
    "for i in range(1000):\n",
    "    temp = torch.mm(w2[0:negSampleSize], torch.t(w2[0].view(1, -1)))\n",
    "print(\"avg time: {}\".format((time.time() - start)/nIters))\n",
    "print(temp.shape)\n",
    "\n",
    "\n",
    "\n",
    "# w1 = w1.data.numpy()\n",
    "# w2 = w2.data.numpy()\n",
    "# start = time.time()\n",
    "# for i in range(1000):\n",
    "#     temp = np.matmul(w2[0:negSampleSize], w1[0])\n",
    "# print(\"avg time: {}\".format((time.time() - start)/nIters))\n",
    "# print(temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = w1.data.numpy()\n",
    "w2 = w2.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15,)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(w2[0:15], w1[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['first',\n",
       "  'quit',\n",
       "  'grinnin',\n",
       "  'like',\n",
       "  'idiot',\n",
       "  'indians',\n",
       "  'aint',\n",
       "  'supposed',\n",
       "  'smile',\n",
       "  'like',\n",
       "  'get',\n",
       "  'stoic'],\n",
       " ['like', 'gotta', 'look', 'mean', 'people', 'wont', 'respect'],\n",
       " ['people', 'run', 'dont', 'look', 'mean'],\n",
       " ['gotta',\n",
       "  'look',\n",
       "  'like',\n",
       "  'warrior',\n",
       "  'gotta',\n",
       "  'look',\n",
       "  'like',\n",
       "  'came',\n",
       "  'back',\n",
       "  'killing',\n",
       "  'buffalo'],\n",
       " ['tribe',\n",
       "  'never',\n",
       "  'hunted',\n",
       "  'buffalo',\n",
       "  'fishermenwhat',\n",
       "  'wanna',\n",
       "  'look',\n",
       "  'like',\n",
       "  'came',\n",
       "  'back',\n",
       "  'catching',\n",
       "  'fish'],\n",
       " ['aint',\n",
       "  'dances',\n",
       "  'salmon',\n",
       "  'know',\n",
       "  'thomas',\n",
       "  'gotta',\n",
       "  'look',\n",
       "  'like',\n",
       "  'warrior']]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalTokenizedCorpus_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Computational Methods)",
   "language": "python",
   "name": "compmeth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
