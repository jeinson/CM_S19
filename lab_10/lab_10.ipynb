{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lab 10: Word Embeddings\n",
    "![itIs30MinutesBeforeLabAndThisIsTheBestICanDo](https://github.com/crowegian/memes/blob/master/iHaveHitALowPoint.png?raw=true)\n",
    "\n",
    "## Introduction\n",
    "In this lab you'll learn how [Skip-Gram](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) is implemented. The skip-gram model works by training a single layer neural network to predict the surrounding words given a center word. The goal is to have a network that learns which words are more likely to appear in the context of a given word. The model is trained using word pairs given a center word, and the context words that appear within a fixed window around the word. The example below shows the word pairs created using different center words and a window size of 2\n",
    "\n",
    "![trainingWordPairs](http://mccormickml.com/assets/word2vec/training_data.png)\n",
    "\n",
    "\n",
    "[Cool](https://www.youtube.com/watch?v=A8q8PXoJwVk). So how does this model actually work? The model has two main moving parts, a set of weights representing the center word and context word embeddings or $V$ and $V^{\\prime}$. Each matrix has separate weights and $\\in R^{v, e}$ where v is the size of the vocabulary and e is the embedding dimension (a hyperparameter you choose).\n",
    "\n",
    "The model learns to minimize the following function.\n",
    "\n",
    "$$L = log(\\sigma(v^{\\prime}_{c_o}v_{c_e}^{T})) + \\sum_{c_o,c_e \\in \\bar{D}} log(\\sigma(-v^{\\prime}_{c_o}v_{c_e}^{T}))$$\n",
    "\n",
    "Where $c_o$ and $c_e$ are the context and center words respectively, $v$ and $v^{\\prime}$ represent the center and context embeddings respectively and $\\bar{D}$ is the set of word pairs where $c_o$ are the negatively sampled context embeddings.\n",
    "\n",
    "## Negative Sampling\n",
    "Please refer to [this](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/) tutorial to understand more about negative sampling. You don't have to build the unigram table but you'll need to know how it's used.\n",
    "\n",
    "## Data\n",
    "There are two datasets extracted for you. One is from the ap news data, and the other is a pull from pubmed. We'll train two sets of word embeddings and compare them at the end. There is also a test corpus which you can use for debugging and getting the model to run. Extra points ($\\leq 0$) if you can [guess](www.google.com) where the corpus comes from.\n",
    "\n",
    "## Installs\n",
    "tqdm is a nice wrapper for loops to check your progress as you go\n",
    "\n",
    "conda install -c conda-forge tqdm\n",
    "\n",
    "ipywidgets makes tqdm look pretty\n",
    "\n",
    "conda install -c conda-forge ipywidgets\n",
    "\n",
    "Tokenization and NLP toolkit\n",
    "\n",
    "conda install -c anaconda nltk \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Janitorial Work\n",
    "All of the data cleaning is handled for you. But please familiarize yourself with the objects created by extractVocabMappers as you'll be using these in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCorpus = [\"First of all, quit grinnin’ like an idiot. Indians ain’t supposed to smile like that. Get stoic.\",\n",
    "             \"No. Like this. You gotta look mean, or people won’t respect you.\",\n",
    "              \" people will run all over you if you don’t look mean.\",\n",
    "              \"You gotta look like a warrior. You gotta look like you just came back from killing a buffalo.\",\n",
    "             \"But our tribe never hunted buffalo. We were fishermen.\"\n",
    "             \"What? You wanna look like you just came back from catching a fish?\",\n",
    "             \"This ain’t dances with salmon, you know. Thomas, you gotta look like a warrior.\"]\n",
    "\n",
    "# NOTE: reduce this number if you can't get things to run quickly.\n",
    "maxDocs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ob2285/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 pub med abstracts\n"
     ]
    }
   ],
   "source": [
    "# Read in pubmed corpus into a text file\n",
    "\n",
    "import glob\n",
    "pubMedDataFolderPath = \"data/pubMed_corpus/\"\n",
    "pubMedDataFiles = glob.glob(pubMedDataFolderPath + \"*.txt\")\n",
    "pubMedCorpus = [\"\"]*len(pubMedDataFiles)\n",
    "for idx, pubMedDataPath in enumerate(pubMedDataFiles):\n",
    "    with open(pubMedDataPath, \"r\") as pubMedFile:\n",
    "        text = pubMedFile.read().strip()\n",
    "        pubMedCorpus[idx] = text\n",
    "pubMedCorpus = pubMedCorpus[0:maxDocs]\n",
    "print(\"{} pub med abstracts\".format(len(pubMedCorpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 ap articles\n"
     ]
    }
   ],
   "source": [
    "# Read in the ap corpus\n",
    "apTextFile = \"data/ap.txt\"\n",
    "apCorpus = []\n",
    "readText = False\n",
    "with open(apTextFile) as apDataFile:\n",
    "    for line in apDataFile:\n",
    "        if readText:\n",
    "            apCorpus.append(line.strip())\n",
    "            readText = False\n",
    "        if line == \"<TEXT>\\n\":\n",
    "            readText = True\n",
    "apCorpus = apCorpus[0:maxDocs]\n",
    "print(\"{} ap articles\".format(len(apCorpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ob2285/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ob2285/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "def removePunctuation(myStr):\n",
    "    excludedCharacters = string.punctuation + \"’\" + \"\\%\"\n",
    "    newStr = \"\".join(char for char in myStr if char not in excludedCharacters)\n",
    "    return(newStr)\n",
    "def removeStopWords(tokenList):\n",
    "    newTokenList = [tok for tok in tokenList if tok not in stopwords.words('english')]\n",
    "    return(newTokenList)\n",
    "def cleanDocStr(docStr):\n",
    "    docStr = docStr.lower()\n",
    "    docStr = removePunctuation(docStr)\n",
    "    docStr = re.sub('\\d', '%d%', docStr)\n",
    "    docStrTokenized = nltk.tokenize.word_tokenize(docStr)\n",
    "    myStopWords = set(stopwords.words('english'))\n",
    "    docStrTokenized = [tok for tok in docStrTokenized if tok not in myStopWords]\n",
    "    return(docStrTokenized)\n",
    "def tokenize_corpus(corpus):\n",
    "    tokens = [cleanDocStr(x) for x in corpus]\n",
    "    return tokens\n",
    "\n",
    "apCorpusTokenized = tokenize_corpus(apCorpus)\n",
    "pubMedCorpusTokenized = tokenize_corpus(pubMedCorpus)\n",
    "testCorpusTokenized = tokenize_corpus(testCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building ap corpus vocabulary\n",
      "Vocab size: 6510\n",
      "ap data tokenized in 0.09615182876586914 seconds\n",
      "\n",
      "Building pubMed corpus vocabulary\n",
      "Vocab size: 3528\n",
      "pubmed data tokenized in 0.09422731399536133 seconds\n",
      "\n",
      "Building test corpus vocabulary\n",
      "Vocab size: 37\n",
      "test data tokenized in 0.00021076202392578125 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from collections import Counter\n",
    "\n",
    "minVocabOccurence = 5\n",
    "\n",
    "def extractVocabMappers(tokenizedCorpus, minVocabOccurence = 0):\n",
    "    \"\"\"\n",
    "    Decription: \n",
    "    Input:\n",
    "        tokenizedCorpus (list(list(str))): A list where each index is a document from the corpus.\n",
    "            Each document is further tokenized into a list of tokens. \n",
    "            [doc1, doc2,...] where doc1 = [tok1, tok2, ...]\n",
    "        minVocabOccurence (int): Minimum number of times a word needs to show up to be considered\n",
    "            for the vocabulary\n",
    "    Output:\n",
    "        word2Idx (dict): A dictionary mapping each word to its integer ID\n",
    "        idx2Word (dict): A dictionary mapping each integer ID to its word\n",
    "        wordCounts (list(tuples)): A list of tuples mapping each vocab to its count in the\n",
    "            corpus\n",
    "        newTokenizedCorpus (list(list(str))): Same as tokenized corpus but out of vocabulary terms are\n",
    "            mapped to <UNK>\n",
    "        \n",
    "    \"\"\"\n",
    "    UNK = \"<UNK>\"\n",
    "    flattenedCorpus = [item for sublist in tokenizedCorpus for item in sublist]\n",
    "    wordCounts = Counter(flattenedCorpus).most_common()\n",
    "    wordCounts = [(w, c) for w,c in wordCounts if c > minVocabOccurence]\n",
    "#     wordCounts = wordCounts.most_common(vocabSizeMax)\n",
    "    vocabulary = [word for word, count in wordCounts]\n",
    "    \n",
    "    # below is more readable but significantly slower code\n",
    "    if False:\n",
    "        vocabulary = []\n",
    "        for sentence in tqdm(tokenizedCorpus):\n",
    "            for token in sentence:\n",
    "                if token not in vocabulary:\n",
    "                    vocabulary.append(token)\n",
    "#     vocabulary.append(UNK)\n",
    "    print(\"Vocab size: {}\".format(len(vocabulary)))\n",
    "    word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "    idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "    newTokenizedCorpus = []# all words missing from vocab replaced with <UNK>\n",
    "    # JK Im removing them\n",
    "    for doc in tokenizedCorpus:\n",
    "        newDoc = [word for word in doc if word in word2idx]# remove UNK from corpus\n",
    "#         newDoc = [word if word in word2idx else UNK for word in doc]\n",
    "        newTokenizedCorpus.append(newDoc)\n",
    "    return(word2idx, idx2word, wordCounts, newTokenizedCorpus)\n",
    "\n",
    "start = time.time()\n",
    "print(\"Building ap corpus vocabulary\")\n",
    "word2Idx_ap, idx2Word_ap, vocabCount_ap, finalTokenizedCorpus_ap = extractVocabMappers(apCorpusTokenized,\n",
    "                                                                                      minVocabOccurence = minVocabOccurence)\n",
    "print(\"ap data tokenized in {} seconds\\n\".format(time.time() - start))\n",
    "start = time.time()\n",
    "print(\"Building pubMed corpus vocabulary\")\n",
    "word2Idx_pubMed, idx2Word_pubMed, vocabCount_pubMed, finalTokenizedCorpus_pubMed = extractVocabMappers(pubMedCorpusTokenized,\n",
    "                                                                                                      minVocabOccurence = minVocabOccurence)\n",
    "print(\"pubmed data tokenized in {} seconds\\n\".format(time.time() - start))\n",
    "start = time.time()\n",
    "print(\"Building test corpus vocabulary\")\n",
    "word2Idx_test, idx2Word_test, vocabCount_test, finalTokenizedCorpus_test = extractVocabMappers(testCorpusTokenized,\n",
    "                                                                                              minVocabOccurence = 0)\n",
    "print(\"test data tokenized in {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateObservations(tokenizedCorpus, word2Idx):\n",
    "    \"\"\"\n",
    "    Decription: Iterates through every token in the corpus and creates a (center, context)\n",
    "        pair for each context word in the window on either side of the center word. Please\n",
    "        refer to the first figure to understand how word pairs are created\n",
    "    Input:\n",
    "        tokenizedCorpus (list(list(str))): A list where each index is a document from the corpus.\n",
    "            Each document is further tokenized into a list of tokens. \n",
    "            [doc1, doc2,...] where doc1 = [tok1, tok2, ...]\n",
    "        word2Idx (dict): A dictionary mapping words to their integer IDs\n",
    "    Output:\n",
    "        idxPairs (list(tuples)): A list of tuples where each tuple is a (center, context word)\n",
    "        For example, idxPairs could look like [(w1, w2), (w1,w3), ...] where w1 and w2 are\n",
    "        strings of tokens from the corpus.\n",
    "    \"\"\"\n",
    "    window_size = 5\n",
    "    idxPairs = []\n",
    "    for sentence in tokenizedCorpus:\n",
    "        for center_word_pos in range(len(sentence)):\n",
    "            # Your code here\n",
    "            # Populate idxPairs. Be sure not to jump outside the sentence bounds with your window.\n",
    "\n",
    "            # End your code\n",
    "    idxPairs = np.array(idxPairs)\n",
    "    return(idxPairs)\n",
    "\n",
    "\n",
    "def generateWordSamplingUnigramTable(vocabCount, word2Idx):\n",
    "    \"\"\"\n",
    "    Decription: Generates a unigram table to sample data from. The unigram table\n",
    "        should contains the index of every vocab index multiple times. The number\n",
    "        of times an element appears is dictated by its sample probability. The unigram\n",
    "        table can the be sampled. \n",
    "    Input:\n",
    "        vocabCount (list(tuples)): A list of tuples mapping each vocab to its count in the\n",
    "            corpus\n",
    "        word2Idx (dict): A dictionary mapping words to their integer IDs\n",
    "    Output:\n",
    "        unigram_table (list(int)): A list of integers as described above. For example\n",
    "        in a 3 word vocabulary it might look something like [0,0,1,1,1,1,1,1,1,2].\n",
    "        Sampling from the previous example will mean that 0 is sampled 2/10 times,\n",
    "        1 is sampled 7/10 times, and 2 is sampled 1/10 times.\n",
    "    \"\"\"\n",
    "    unigram_table = []\n",
    "#     numWords = np.sum([count for word, count in vocabCount])\n",
    "    numWords = np.sum([count**0.75 for word, count in vocabCount])\n",
    "    tableLength = 10000\n",
    "    for w,c in vocabCount:\n",
    "        unigram_table.extend([word2Idx[w]] * int((((c**0.75)/numWords))*tableLength))\n",
    "#         unigram_table.extend([word2Idx[w]] * int(((c/numWords)**0.75)/0.001))\n",
    "    return(unigram_table)\n",
    "    \n",
    "class SkipGram(nn.Module):\n",
    "    \"\"\"\n",
    "    Decription: Instantiates and implements the forward pass of the skip gram\n",
    "        algorithm with negative sampling.\n",
    "    Input:\n",
    "        vocabSize (int): Number of words to create embeddings for\n",
    "        embedSize (int): Dimension of word embeddings\n",
    "        word2Idx (dict): A dictionary mapping words to their integer IDs\n",
    "    Output:\n",
    "    \"\"\"\n",
    "    def __init__(self, vocabSize, embedSize, vocabCount, word2Idx):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.vocabSize = vocabSize\n",
    "        self.word2Idx = word2Idx\n",
    "        # Your code here\n",
    "        # Init the center and context embedding matrices. These are learnable parameters\n",
    "        self.centerEmbeddings = \n",
    "        self.contextEmbeddings = \n",
    "        # End your code\n",
    "        # Init the embeddings however you like, but this init worked well for me.\n",
    "        nn.init.xavier_uniform_(self.contextEmbeddings)\n",
    "        nn.init.xavier_uniform_(self.centerEmbeddings)\n",
    "        \n",
    "        self.unigram_table = generateWordSamplingUnigramTable(vocabCount, word2Idx)\n",
    "        self.logSigmoid = nn.LogSigmoid()\n",
    "    def getNegSample(self, k, centerWords):\n",
    "        \"\"\"\n",
    "        Decription: Randomly selects negative samples from the vocabulary. USes\n",
    "            self.unigram_table in order to sample words. \n",
    "        Input:\n",
    "            k (int): Number of negative samples to select\n",
    "            centerWords (list(str)): A list of the string center words. There should\n",
    "                be batchSize of these.\n",
    "        Output:\n",
    "            negSamples (list(numpyArray)): A list of numpy arrays where each numpy array\n",
    "                contains the indices of negative samples. There are batchSize numpy arrays\n",
    "        \"\"\"\n",
    "        negSamples = []\n",
    "        for centerWord in centerWords:\n",
    "            # Your code here\n",
    "            # Using self.unigram_table sample indices to use as your negative samples\n",
    "            # Be sure that for each center word you return negative samples, which\n",
    "            # don't contain the center word. Should't happen often but just ot be sure.\n",
    "            \n",
    "            # self.unigram_table don't forget to use this.\n",
    "\n",
    "        # End your code\n",
    "        return(negSamples)\n",
    "    def forward(self, center, context, negSampleIndices):\n",
    "        \"\"\"\n",
    "        Decription Forward pass for the skipgram model. \n",
    "        Input:\n",
    "            center (list(int)): A list of word integer IDs indicating all\n",
    "                batchSize center words. Matches one to one with context\n",
    "            context (list(int)): A list of word integer IDs indicating all\n",
    "                batchSize context words. Matches one to one with center\n",
    "            negSampleIndices (list(numpyArray)): A list of numpy arrays where\n",
    "                each numpy array contains the indices of negative samples.\n",
    "                There are batchSize numpy arrays. Returned by getNegSample()\n",
    "        Output:\n",
    "            logProb (tensor): The loss over the entire batch.\n",
    "        \"\"\"\n",
    "        # Your Code\n",
    "        # implement a forward pass of the model. Be sure to allow for varying batch sizes\n",
    "\n",
    "        # End your code\n",
    "        return(negLogProb)\n",
    "\n",
    "\n",
    "def train_skipgram(embeddingSize, trainingData, vocabCount, word2Idx, idx2Word,\n",
    "                   k, referenceWords, batchSize = 1024):\n",
    "    \"\"\"\n",
    "    Decription: Instantiates and trains a skipgam model. The forward pass of the skipgram mode\n",
    "        handles the forward pass so all you have to do here is handle the loss, and\n",
    "        updating the weights.\n",
    "    Input:\n",
    "        embeddingSize (int): Size of each word embedding\n",
    "        trainingData (list(tuples)): A list of tuples generated by generateWordSamplingUnigramTable()\n",
    "            where each tuple is a center and context word\n",
    "        vocabCount (list(tuples)): A list of tuples mapping each vocab to its count in the\n",
    "            corpus\n",
    "        word2Idx (dict): A dictionary mapping each word to its integer ID\n",
    "        idx2Word (dict): A dictionary mapping each integer ID to its word\n",
    "        k (int): Dictates the number of sampls used during negative sampling\n",
    "        referenceWords (list(str)): A list of words to compare word embeddings for\n",
    "        batchSize (int): The number of (center, context) words to run through each forward\n",
    "            pass of the skipgram model.\n",
    "    Output:\n",
    "        model (SkipGram): The final trained SkipGram model\n",
    "    \"\"\"\n",
    "    print(\"training on {} observations\".format(len(trainingData)))\n",
    "    model = SkipGram(vocabSize = len(word2Idx), embedSize = embeddingSize,\n",
    "                     vocabCount = vocabCount, word2Idx = word2Idx)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    listNearestWords(model = model, idx2Word = idx2Word,\n",
    "     referenceWords = referenceWords, topN = 5)\n",
    "    #         listNearestWords(model = model, idx2Word = idx2Word,\n",
    "#                  referenceWords = referenceWords, topN = 5)\n",
    "    for epoch in tqdm_notebook(range(n_epoch), position = 0):\n",
    "        total_loss = .0\n",
    "        avgLoss = 0.0\n",
    "        iteration = 0\n",
    "        for step in tqdm_notebook(range(0, len(trainingData), batchSize), position = 1):\n",
    "            endIdx = np.min([(step+batchSize), len(trainingData)])\n",
    "            myBatch = trainingData[step:(step+batchSize)]\n",
    "            centerWords = [elem[0] for elem in myBatch]\n",
    "            contextWords = [elem[1] for elem in myBatch]\n",
    "            negSamples = model.getNegSample(k = k, centerWords = centerWords)\n",
    "            centerIDs = [word2Idx[idx] for idx in centerWords]\n",
    "            contextIDs = [word2Idx[idx] for idx in contextWords]\n",
    "            model.zero_grad()\n",
    "            loss = model(centerIDs, contextIDs, negSampleIndices = negSamples)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.data.numpy()\n",
    "            avgLoss += loss.data.numpy()\n",
    "            iteration += 1\n",
    "            if iteration % 500 == 0:\n",
    "                avgLoss = avgLoss/(500)\n",
    "                print(\"avg loss: {}\".format(avgLoss))\n",
    "        print(\"Loss at epoch {}: {}\".format(epoch, total_loss/iteration))\n",
    "        if epoch % 1 == 0:\n",
    "            listNearestWords(model = model, idx2Word = idx2Word,\n",
    "                         referenceWords = referenceWords, topN = 5)\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "def listNearestWords(model, idx2Word, referenceWords, topN):\n",
    "    \"\"\"\n",
    "    Decription: Lists the topN closes words by cosine distance to each word in referenceWords\n",
    "    Input:\n",
    "        model (SkipGram): The final trained SkipGram model\n",
    "        idx2Word (dict): A dictionary mapping each integer ID to its word\n",
    "        referenceWords (list(str)): A list of words in the vocabulary of the model\n",
    "        topN (int): The number of closest words to print\n",
    "    Output:\n",
    "        None: Just prints\n",
    "    \"\"\"\n",
    "    assert len(idx2Word) == len(model.word2Idx), \"Possibly passed in two different vocabularies\"\n",
    "    embeddings = model.centerEmbeddings.data.numpy()\n",
    "    distMat = cdist(embeddings, embeddings, metric = \"cosine\")\n",
    "    # Your code here\n",
    "    # print the topN closest words to each word in referenceWords\n",
    "    for word in referenceWords:\n",
    "\n",
    "    # End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Your Code\n",
    "Using the test corpus below go ahead and test your code to make sure things run, and your loss is decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embd_size = 100\n",
    "learning_rate = 0.001\n",
    "n_epoch = 60\n",
    "idxPairsTest = generateObservations(tokenizedCorpus = finalTokenizedCorpus_test, word2Idx = word2Idx_test)\n",
    "sg_model = train_skipgram(embeddingSize = 5, trainingData = idxPairsTest, vocabCount = vocabCount_test,\n",
    "                                     word2Idx = word2Idx_test, idx2Word = idx2Word_test, k = 10,\n",
    "                                    referenceWords = [\"thomas\", \"salmon\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a skip-gram model\n",
    "Now we're ready to train our skip-gram model on two different corpora. Play with these hyper-parameters to and check your word embeddings regularly to make sure they're learning the right thing. **For the reference words provided, and more if you like, argue that the closest word embeddings make sense. Why might these word embeddings be so close? Consider the context in the corpus. I'll base your grade for this section mostly on your argument.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on 2605610 observations\n",
      "('bush', 0.0)\n",
      "('shares', 0.5194150914349732)\n",
      "('poland', 0.5218423178999131)\n",
      "('estimate', 0.5441517338645673)\n",
      "('widely', 0.5513883726368334)\n",
      "**************************************************\n",
      "\n",
      "('soviet', 1.1102230246251565e-16)\n",
      "('criminal', 0.46760505696415045)\n",
      "('born', 0.4898914966899621)\n",
      "('persistent', 0.500935605592492)\n",
      "('sentenced', 0.5165142317527203)\n",
      "**************************************************\n",
      "\n",
      "('president', 1.1102230246251565e-16)\n",
      "('prestige', 0.4515375190452422)\n",
      "('deicing', 0.49984655865450345)\n",
      "('christians', 0.5155236051739778)\n",
      "('legal', 0.5356843649976819)\n",
      "**************************************************\n",
      "\n",
      "('economy', 0.0)\n",
      "('monets', 0.5346631316121371)\n",
      "('robinson', 0.5358051657353012)\n",
      "('certain', 0.5434795723617019)\n",
      "('columbus', 0.5480942726486525)\n",
      "**************************************************\n",
      "\n",
      "('american', 0.0)\n",
      "('die', 0.49618172889851486)\n",
      "('german', 0.5058398432846687)\n",
      "('walked', 0.5250782122010302)\n",
      "('sharing', 0.5372364260175952)\n",
      "**************************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5351ed262b4a4824b1c8bec0525b34f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07c41397608c4556bf2de3dfbbf1ac24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2545), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 14.418619115829468\n",
      "avg loss: 13.099470156589508\n",
      "avg loss: 11.887569530523141\n",
      "avg loss: 11.060079142061307\n",
      "avg loss: 11.051930679886754\n",
      "Loss at epoch 0: 12.2664336380181\n",
      "('bush', 0.0)\n",
      "('government', 0.016436052964986403)\n",
      "('one', 0.022464927321183326)\n",
      "('two', 0.023364495530326934)\n",
      "('us', 0.024297377591558478)\n",
      "**************************************************\n",
      "\n",
      "('soviet', 0.0)\n",
      "('also', 0.016949834118185847)\n",
      "('us', 0.018801936313878387)\n",
      "('would', 0.021414450920200445)\n",
      "('two', 0.022375601991986627)\n",
      "**************************************************\n",
      "\n",
      "('president', 0.0)\n",
      "('one', 0.011309800148615712)\n",
      "('year', 0.013361777755761084)\n",
      "('two', 0.013461328693379349)\n",
      "('would', 0.013482899247442504)\n",
      "**************************************************\n",
      "\n",
      "('economy', 0.0)\n",
      "('news', 0.1221998164462742)\n",
      "('united', 0.13160612208791556)\n",
      "('one', 0.1325020814429606)\n",
      "('bush', 0.1373247260042273)\n",
      "**************************************************\n",
      "\n",
      "('american', 0.0)\n",
      "('president', 0.018977035233529005)\n",
      "('one', 0.019758484884318528)\n",
      "('years', 0.02142941720262692)\n",
      "('united', 0.023956911926282687)\n",
      "**************************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba0c081e9de04bf5be37e2640604b211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2545), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 10.843738045454025\n",
      "avg loss: 10.595060351662159\n",
      "avg loss: 10.131587166792496\n",
      "avg loss: 9.44532161120416\n",
      "avg loss: 9.41494197974928\n",
      "Loss at epoch 1: 10.06158934011909\n",
      "('bush', 0.0)\n",
      "('many', 0.004564515499829724)\n",
      "('united', 0.0048288900530799594)\n",
      "('could', 0.004861329448970442)\n",
      "('news', 0.004901564683698512)\n",
      "**************************************************\n",
      "\n",
      "('soviet', 0.0)\n",
      "('states', 0.003799948786777696)\n",
      "('time', 0.003918875168575475)\n",
      "('officials', 0.004143978014025662)\n",
      "('government', 0.00418413757657099)\n",
      "**************************************************\n",
      "\n",
      "('president', 0.0)\n",
      "('two', 0.002568132564815917)\n",
      "('also', 0.002768243360620315)\n",
      "('us', 0.0028542633394230688)\n",
      "('one', 0.002939002352398745)\n",
      "**************************************************\n",
      "\n",
      "('economy', 0.0)\n",
      "('might', 0.020907855464797342)\n",
      "('come', 0.021087863944941332)\n",
      "('know', 0.02137706134443562)\n",
      "('director', 0.021431731527101316)\n",
      "**************************************************\n",
      "\n",
      "('american', 0.0)\n",
      "('president', 0.0031007045554297674)\n",
      "('police', 0.003937291889692718)\n",
      "('one', 0.004217805067095681)\n",
      "('also', 0.004322177390797655)\n",
      "**************************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d11f9d4f195541eebdc03ab8347e4a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2545), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 9.141483906269073\n",
      "avg loss: 8.870717495448112\n",
      "avg loss: 8.381149161517156\n",
      "avg loss: 7.760508625246405\n",
      "avg loss: 7.699830366310552\n",
      "Loss at epoch 2: 8.347920382655206\n",
      "('bush', 1.1102230246251565e-16)\n",
      "('many', 0.002238930462963973)\n",
      "('news', 0.0025811540833399205)\n",
      "('federal', 0.0025909907976920943)\n",
      "('house', 0.0026683929547705043)\n",
      "**************************************************\n",
      "\n",
      "('soviet', 1.1102230246251565e-16)\n",
      "('states', 0.0020294505469018453)\n",
      "('time', 0.002068412481474491)\n",
      "('officials', 0.0021661296134071195)\n",
      "('also', 0.0022847743454394998)\n",
      "**************************************************\n",
      "\n",
      "('president', 0.0)\n",
      "('two', 0.0015512300660099898)\n",
      "('one', 0.0015515476147457408)\n",
      "('also', 0.001640716637153261)\n",
      "('first', 0.0016932215615717006)\n",
      "**************************************************\n",
      "\n",
      "('economy', 2.220446049250313e-16)\n",
      "('might', 0.007834931139029777)\n",
      "('come', 0.007916209916335903)\n",
      "('know', 0.008077843561111964)\n",
      "('director', 0.008246080951527679)\n",
      "**************************************************\n",
      "\n",
      "('american', 0.0)\n",
      "('president', 0.0016942704836404587)\n",
      "('police', 0.0020286116228047657)\n",
      "('states', 0.0022648536392596563)\n",
      "('one', 0.002276492922200002)\n",
      "**************************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7db28ed4f6425fa8cb86d9a642b3d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2545), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 7.46271546626091\n",
      "avg loss: 7.253077980676174\n",
      "avg loss: 6.853389161341982\n"
     ]
    }
   ],
   "source": [
    "embeddingSize = 50\n",
    "learning_rate = 0.1\n",
    "n_epoch = 10\n",
    "idxPairsAP = generateObservations(tokenizedCorpus = finalTokenizedCorpus_ap, word2Idx = word2Idx_ap)\n",
    "sg_model_ap = train_skipgram(embeddingSize = embeddingSize, trainingData = idxPairsAP,\n",
    "                                     vocabCount = vocabCount_ap,\n",
    "                                     word2Idx = word2Idx_ap, idx2Word = idx2Word_ap, k = 20,\n",
    "                                          referenceWords = [\"bush\", \"soviet\", \"president\", \"economy\", \"american\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingSize = 50\n",
    "learning_rate = 0.1\n",
    "n_epoch = 10\n",
    "idxPairsPubMed = generateObservations(tokenizedCorpus = finalTokenizedCorpus_pubMed, word2Idx = word2Idx_pubMed)\n",
    "sg_model_pubMed = train_skipgram(embeddingSize = embeddingSize, trainingData = idxPairsPubMed,\n",
    "                                     vocabCount = vocabCount_pubMed,\n",
    "                                     word2Idx = word2Idx_pubMed, idx2Word = idx2Word_pubMed, k = 20,\n",
    "                                                  referenceWords = [\"clinical\", \"obesity\", \"microbial\", \"microbiome\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Domains Affect Word Embeddings\n",
    "Choose two words that appear in both the pubmed and ap vocabularies and compare the closest embeddings to both words in the pubmed and ap embeddings[.](https://www.youtube.com/watch?v=Tr-WrGcexlY) **Why might the two words you chose have different representations? How might this affect downstream NLP tasks?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Computational Methods)",
   "language": "python",
   "name": "compmeth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
