{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![giraffe](https://raw.githubusercontent.com/jeinson/jeinson.github.io/master/images/giraffe.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Feed Forward Neural Networks\n",
    "\n",
    "In this lab activity, we will experiment with a simple yet powerful tool called a Feedforward Neural Network, implemented in (you guessed it) pytorch. This network is more complicated than linear and logistic regression that we've seen earlier, but it is a simpler form of neural network because information is only passed forward, i.e. nodes to not form a cycle. (spoiler alert, this will come later) A canonical FFNN is composed of layers of linear nodes called *perceptrons*, which take the value 1 or -1, according to some activation function, dependent on the previous layer. The parameters of a FFNN are learned exactly the same way we learned the parameters for linear and logistic regression, by gradient descent using **back propagation**\n",
    "\n",
    "![nn_image](https://upload.wikimedia.org/wikipedia/en/5/54/Feed_forward_neural_net.gif)\n",
    "\n",
    "As it's a right of passage for beginner machine learners, we will start with the MNIST dataset, which consists of scanned images of handwritten digits. The neural net will classify each image as being a 0 through 9. (And hopefully better job than Patrick at classifying bubble buddies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the Training Data\n",
    "\n",
    "Fortunately MNIST comes in the box with the pytorch package, and has been formatted for input into a neural network. Wrangling your data is half the battle, and it's already done for you :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root='./data',\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Making the dataset more manageable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch sizes and iterations**\n",
    "\n",
    "Using the entire thing to train the model is unadvisable because it 1) would require a ton of RAM and 2.) could result in unstable training, as errors would accumlate very quickly. Therefore, we divide the training data into smaller batches, and feed it through the network iteratively. \n",
    "\n",
    "How big is the training dataset? If we want a batch size of 100 images, how many iterations do we need to have so that the network sees each image at least once?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Epochs**\n",
    "\n",
    "The formal definition of \"epoch\" is one complete pass of all the data through the model. How many iterations do we need to get to 5 epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your code here (if you need help multiplying) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bringing it all together**\n",
    "\n",
    "Now let's specify the batch size, number of iterations, and the number ofr epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters =  # Your code here\n",
    "n_epochs = #### your code here (but it should be 5 cuz I said so) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define `train_loader` and `test_loader` as one of the objects in the [torch.utils.data](https://pytorch.org/docs/stable/data.html) module. These things will load images from your dataset objects in the batch size you specify. This is one of the flagship features of Pytorch, since loading huge datasets can eat up a ton of memory. Read more about data loaders and how to use them [here](https://github.com/utkuozbulak/pytorch-custom-dataset-examples). Note you should shuffle your training data, but no need to shuffle the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create a Model Class\n",
    "\n",
    "A feed forward neural net looks remarkably similar to the linear and logistic regression networks we played with for the past two weeks. The only difference is an extra linear and non-linear layer. \n",
    "![ExtraLayer](https://www.deeplearningwizard.com/deep_learning/practical_pytorch/images/logistic_regression_comparison_nn5.png)\n",
    "\n",
    "The syntax for defining a neural net in pytorch should be familiar to you by now, but remember to check the API here if you get stuck\n",
    "\n",
    "In this net, we set the dimensions of each layer as parameters that are specified when defining a model `nn.Module` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetwork, self).__init__()\n",
    "        \n",
    "        # Linear Function\n",
    "        #### your code here ####\n",
    "        \n",
    "        # Non-linearity (Use a sigmoid for now)\n",
    "        #### your code here ####\n",
    "        \n",
    "        # Another linear function (the readout)\n",
    "        #### your code here ####\n",
    "    \n",
    "    # Here's where we connect all the layers together\n",
    "    def forward(self, x):\n",
    "        # Linear function\n",
    "        #### your code here ####\n",
    "        \n",
    "        # Non-linearity \n",
    "        #### your code here ####\n",
    "        \n",
    "        # The final linear function\n",
    "        #### your code here ####\n",
    "        \n",
    "        # aaaaannnddd spit out the output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Instantiate the Model Class\n",
    "\n",
    "Our input size is determined by the size of the image (numbers ranging from 0 to 9) which has a width of 28 pixels and a height of 28 pixels. Hence the size of our input is 784 (28 x 28).\n",
    "\n",
    "Our output size is what we are trying to predict. When we pass an image to our model, it will try to predict if it's 0, 1, 2, 3, 4, 5, 6, 7, 8, or 9. How many possible digits are there?\n",
    "\n",
    "Now the tricky part is in determining our hidden layer size, that is the size of our first linear layer prior to the non-linear layer. This can be any number; a larger number implies a bigger model with more parameters. Intuitively we think a bigger model equates to a better model, but a bigger model requires more training samples to learn and converge to a good model (also called curse of dimensionality). Hence, it is wise to pick the model size for the problem at hand. Because it is a simple problem of recognizing digits, we typically would not need a big model to achieve state-of-the-art results.\n",
    "\n",
    "On the flipside, too small of a hidden size would mean there would be insufficient model capacity to predict competently. In layman terms, too small of a capacity implies a smaller brain capacity so no matter how many training samples you give it, it has a maximum capacity in terms of its predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = # Your code here\n",
    "hidden_dim =  # Your code here (Try tinkering with this if stuff goes wrong)\n",
    "output_dim =  # Your code here\n",
    "\n",
    "model = #### your code here ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have a GPU, this is where you can use it!\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Instantiate a Loss Class\n",
    "\n",
    "For classification problems, we use cross entropy as a loss function. Don't worry too much about what it actually does. Just define the appropriate object from the `torch.nn` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Instantitate an Optimizer Class\n",
    "\n",
    "In terms of updating your parameters, we still follow the basic premace of $\\theta_{t+1} = \\theta_{t} - \\eta \\cdot \\nabla_{\\theta_t}$, where $\\theta$ are the model parameters, $\\eta$ is the learning rate and $\\nabla_\\theta$ is the gradient. For now we're gonna stick with stochastic gradient descent for the optimizer. It's pretty much the duct tape of ML. As for a learning rate, start with 0.1, but feel free to change it up later if you need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = #### your code here ###\n",
    "optimizer =  #### your code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![T_time](https://media1.tenor.com/images/b04545d91916611c2c2cd61cc3854d2d/tenor.gif)\n",
    "\n",
    "<center>Time Out!</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a few minutes to understand what's going on at this point. First off, how many parameters does each layer of the network have? What values to the parameters take when initializing the model? Do you see how at the core, we're really just doing matrix the same operations as in regression? The diagram below shows the interaction amongst our input $X$ and out linear layers' parameters $A_1$, $B_1$, $A_2$, and $B_2$ to reach the final size of 10 x 1.\n",
    "\n",
    "![net_image](https://www.deeplearningwizard.com/deep_learning/practical_pytorch/images/nn1_params3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Train the Model\n",
    "\n",
    "This may be the most confusing part, especially accessing data from your training data loader object. To get a feel for how it works, make a plot of the first 25 digits in the first batch of MNIST images, and label the x axis with the true label. Make sure to use the `train_loader` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#### your code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training follows the same process as we've seen previously \n",
    "1. Convert inputs to tensors with gradient accumulation capabilities\n",
    "2. Zero the gradients\n",
    "3. Get a new output given the current parameters\n",
    "4. Calculate the loss \n",
    "5. Get new gradients w.r.t. the model parameters\n",
    "6. Update the parameters using the gradients, in a direction toward the optimal solution\n",
    "7. *REPEAT!!*\n",
    "\n",
    "Now fill in the missing code to train the model. Depending on how powerful your CPU (or GPU) is, training may take a few minutes. For debugging, try reducing the number of epochs or batch size, just to make sure stuff is working properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = 0\n",
    "for epochs in range(n_epochs):\n",
    "    \n",
    "    # This will load images 100 at a time, as specified when you defined this thing\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        # Flatten the images to a torch tensor of 1x(28*28) vectors\n",
    "        # (try the .view method)\n",
    "        # And make sure they accumulate gradients!\n",
    "        #### your code here ####\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        #### your code here ####\n",
    "        \n",
    "        # Forward pass to get the output/logits\n",
    "        #### your code here ####\n",
    "        \n",
    "        # Calculate Loss\n",
    "        #### your code here ####\n",
    "        \n",
    "        # Get gradients w.r.t. the parameters\n",
    "        #### your code here ####\n",
    "        \n",
    "        # Update the parameters\n",
    "        #### your code here ####\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        # Every 500 iterations, check up on how the model is doing, \n",
    "        # by printing the loss and the training accuracy on the held out data. \n",
    "        # Accuracy = (number of images correctly identified) / (total number of images)\n",
    "        #### your code here ####\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images with gradient accumulation capabilities\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "\n",
    "                # Get the total number of labels\n",
    "\n",
    "                # Calculate the total correct predictions\n",
    "\n",
    "            accuracy = #### your code here ####\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice work! You trained a neural net! If you got a training accuracy of over 90%, you can reward yourself with a picture of my cat helping me write this lab...\n",
    "\n",
    "<center><form method=\"get\" action=\"https://raw.githubusercontent.com/jeinson/jeinson.github.io/master/images/20190220_095041.jpg\">\n",
    "    <button type=\"submit\">Click here for LULz</button>\n",
    "</form></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus challenge\n",
    "\n",
    "From the last round of training, select the images that the neural net failed to classify correctly and plot them. Why do you think the net was unable to correctly identify these digits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Citation*:\n",
    "This lab was developed from www.deeplearningwizard.com. With some googling, I'm sure you can find it, but if you copy-paste the answers you won't really learn anything!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Computational Methods)",
   "language": "python",
   "name": "cm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
