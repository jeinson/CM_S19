{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What students should learn\n",
    "1. How to build a basic CNN with guard rails.\n",
    "2. How to take into account dimensions when performing pooling\n",
    "3. Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                            train=False, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download = True)\n",
    "# train_dataset = dsets.MNIST(root='./data', \n",
    "#                             train=True, \n",
    "#                             transform=transforms.Compose([\n",
    "#                                 transforms.RandomRotation(degrees = 10),\n",
    "#                                 transforms.ToTensor(),\n",
    "#                                 ]),\n",
    "#                             download=True)\n",
    "\n",
    "# test_dataset = dsets.MNIST(root='./data', \n",
    "#                             train=False, \n",
    "#                             transform=transforms.Compose([\n",
    "#                                 transforms.RandomRotation(degrees = 10),\n",
    "#                                 transforms.ToTensor(),\n",
    "#                                 ]),\n",
    "#                             download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.train_data.size())\n",
    "print(train_dataset.train_labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 28, 28])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset.test_data.size())\n",
    "print(test_dataset.test_labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.modules.ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, in_channels_list = [1, 16], out_channels_list = [16, 32],\n",
    "                 kernel_size_list = [5, 5], stride_list = [1, 1],\n",
    "                 padding_list = [0,0], kernel_pool_list = [2, 2],\n",
    "                 pooling_list = [nn.MaxPool2d, nn.MaxPool2d], activations_list = [nn.ReLU(), nn.ReLU()]):\n",
    "        localArgs = locals().items()\n",
    "        super(CNNModel, self).__init__()\n",
    "        argLens = set()\n",
    "        ignoredArgs = ['self', \"__class__\"]\n",
    "        for argName, arg in localArgs:\n",
    "            if argName not in ignoredArgs:\n",
    "                argLens.add(len(arg))\n",
    "        assert len(argLens) == 1, (\"mismatch in lengths of arguments.\"\n",
    "                                   \"All params for each layer must be specified\")\n",
    "#         listLengths = np.array([len()])\n",
    "        # Convolution 1\n",
    "        modules = list()\n",
    "        for layerIdx in range(0, argLens.pop()):\n",
    "            modules.append(nn.Conv2d(in_channels = in_channels_list[layerIdx],\n",
    "                                 out_channels = out_channels_list[layerIdx],\n",
    "                                 kernel_size = kernel_size_list[layerIdx],\n",
    "                                 stride = stride_list[layerIdx],\n",
    "                                 padding = padding_list[layerIdx]))\n",
    "            modules.append(activations_list[layerIdx])\n",
    "            modules.append(pooling_list[layerIdx](kernel_size = kernel_pool_list[layerIdx]))\n",
    "        self.convolutions = nn.Sequential(*modules)\n",
    "        self.finalLayer = nn.Linear(32 * 4 * 4, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.convolutions(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.finalLayer(out)\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 5, 5])\n",
      "torch.Size([16])\n",
      "torch.Size([32, 16, 5, 5])\n",
      "torch.Size([32])\n",
      "torch.Size([10, 512])\n",
      "torch.Size([10])\n",
      "<generator object Module.parameters at 0x7f92bddcde08>\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "for modelParam in model.parameters():\n",
    "    print(modelParam.size())\n",
    "print(model.parameters())\n",
    "\n",
    "print(len(list(model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 50. Loss: 2.2498464584350586. Accuracy: 18\n",
      "Iteration: 100. Loss: 2.132849931716919. Accuracy: 58\n",
      "Iteration: 150. Loss: 1.7954744100570679. Accuracy: 66\n",
      "Iteration: 200. Loss: 1.2130354642868042. Accuracy: 77\n",
      "Iteration: 250. Loss: 0.7746879458427429. Accuracy: 83\n",
      "Iteration: 300. Loss: 0.5422929525375366. Accuracy: 85\n",
      "Iteration: 350. Loss: 0.6932182312011719. Accuracy: 86\n",
      "Iteration: 400. Loss: 0.37876656651496887. Accuracy: 87\n",
      "Iteration: 450. Loss: 0.5879152417182922. Accuracy: 86\n",
      "Iteration: 500. Loss: 0.5827994346618652. Accuracy: 88\n",
      "Iteration: 550. Loss: 0.23624227941036224. Accuracy: 90\n",
      "Iteration: 600. Loss: 0.31086039543151855. Accuracy: 90\n",
      "Iteration: 650. Loss: 0.351461261510849. Accuracy: 90\n",
      "Iteration: 700. Loss: 0.3782237768173218. Accuracy: 91\n",
      "Iteration: 750. Loss: 0.3811475336551666. Accuracy: 91\n",
      "Iteration: 800. Loss: 0.3364792764186859. Accuracy: 91\n",
      "Iteration: 850. Loss: 0.41134724020957947. Accuracy: 92\n",
      "Iteration: 900. Loss: 0.26772287487983704. Accuracy: 93\n",
      "Iteration: 950. Loss: 0.3538384735584259. Accuracy: 91\n",
      "Iteration: 1000. Loss: 0.23763488233089447. Accuracy: 92\n",
      "Iteration: 1050. Loss: 0.25607672333717346. Accuracy: 93\n",
      "Iteration: 1100. Loss: 0.15069063007831573. Accuracy: 93\n",
      "Iteration: 1150. Loss: 0.2935296893119812. Accuracy: 93\n",
      "Iteration: 1200. Loss: 0.20739907026290894. Accuracy: 94\n",
      "Iteration: 1250. Loss: 0.36801034212112427. Accuracy: 93\n",
      "Iteration: 1300. Loss: 0.17183330655097961. Accuracy: 94\n",
      "Iteration: 1350. Loss: 0.14234408736228943. Accuracy: 94\n",
      "Iteration: 1400. Loss: 0.1674802452325821. Accuracy: 94\n",
      "Iteration: 1450. Loss: 0.17291595041751862. Accuracy: 94\n",
      "Iteration: 1500. Loss: 0.16440795361995697. Accuracy: 94\n",
      "Iteration: 1550. Loss: 0.2237321138381958. Accuracy: 94\n",
      "Iteration: 1600. Loss: 0.17678043246269226. Accuracy: 95\n",
      "Iteration: 1650. Loss: 0.22482715547084808. Accuracy: 95\n",
      "Iteration: 1700. Loss: 0.21372488141059875. Accuracy: 95\n",
      "Iteration: 1750. Loss: 0.24522992968559265. Accuracy: 95\n",
      "Iteration: 1800. Loss: 0.11313488334417343. Accuracy: 95\n",
      "Iteration: 1850. Loss: 0.18335206806659698. Accuracy: 95\n",
      "Iteration: 1900. Loss: 0.13504911959171295. Accuracy: 95\n",
      "Iteration: 1950. Loss: 0.16814737021923065. Accuracy: 95\n",
      "Iteration: 2000. Loss: 0.19234676659107208. Accuracy: 95\n",
      "Iteration: 2050. Loss: 0.18560455739498138. Accuracy: 95\n",
      "Iteration: 2100. Loss: 0.332667738199234. Accuracy: 95\n",
      "Iteration: 2150. Loss: 0.19015061855316162. Accuracy: 95\n",
      "Iteration: 2200. Loss: 0.19299346208572388. Accuracy: 95\n",
      "Iteration: 2250. Loss: 0.16976356506347656. Accuracy: 95\n",
      "Iteration: 2300. Loss: 0.16262802481651306. Accuracy: 96\n",
      "Iteration: 2350. Loss: 0.1290479153394699. Accuracy: 96\n",
      "Iteration: 2400. Loss: 0.11490020900964737. Accuracy: 95\n",
      "Iteration: 2450. Loss: 0.19346210360527039. Accuracy: 96\n",
      "Iteration: 2500. Loss: 0.11460298299789429. Accuracy: 96\n",
      "Iteration: 2550. Loss: 0.14423947036266327. Accuracy: 96\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images\n",
    "        images = images.requires_grad_()\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 50 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images\n",
    "                images = images.requires_grad_()\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Computational Methods)",
   "language": "python",
   "name": "computational_methods"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
