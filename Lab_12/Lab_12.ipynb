{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/crowegian/memes/blob/master/spideyPoint.jpg?raw=true\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/crowegian/memes/blob/master/spideyAdmire.jpg?raw=true\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/crowegian/memes/blob/master/itsAlwaysSunnySpidey.jpg?raw=true\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble models, Confidence Intervals and the Bootstrap\n",
    "In class we learned about the bootstrap. In today's lab we'll learn how to implement the bootstrap to 1) build a ensemble model (a model built by aggregating the results of a bunch of models) almost from scratch, and assess how  sure we can be that a given machine learning model is from a simple majority classifier model. After this lab you should have a deeper understanding of how to use bootstrapping to build ensebmle models, and confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import random\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling in Data\n",
    "You can read more about the dataset [here](https://scikit-learn.org/stable/datasets/index.html#breast-cancer-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breastCancer = datasets.load_breast_cancer()\n",
    "X_all = breastCancer.data\n",
    "randomNumGen = np.random.RandomState(seed=23)\n",
    "E = randomNumGen.normal(size=X_all.shape, loc = 0.0, scale = 1)\n",
    "# Add noisy data to the informative features for make the task harder\n",
    "X_all += E\n",
    "y_all = breastCancer.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size = 0.25, random_state = 23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Building an ensemble model\n",
    "In this section you'll implement an ensemble model. Don't worry, you'll have ample access to scikit-learn libraries, but you have to implement your own functions to generate bootstrap samples, and fit multiple models.\n",
    "\n",
    "### Ensemble Models\n",
    "An ensemble model is a model that leverages many individual models to reduce the overall model's variance. This is achieved by training multiple models (we'll be using logistic regression) and using either averaging, or majority voting to combine predictions into one single prediction. Ideally this is done with multiple datasets, but in practice gathering data can be difficult. So in order to build multiple models we use the bootstrap to generate many datasets from our single dataset. **Why might we not want to train each model on the same dataset and average results?**\n",
    "\n",
    "\n",
    "### Things to do\n",
    "You'll be building a model that will work like many other sklearn models. In order to do this you'll need to provide fit, predict and score functions. You will also need to implement a function to generate data using the bootstrap.\n",
    "\n",
    "### Things to worry about\n",
    "1. What happens when you call fit on a model multiple times?\n",
    "2. Are you sampling with replacement during bootstrapping?\n",
    "3. Inevitable heat death of the universe\n",
    "4. What's for dinner?\n",
    "\n",
    "### Deliverables\n",
    "1. Fill in the ensemble model code below and using the comparison code below train it for 1,2,3,...,10 learners.\n",
    "2. Answer the above bolded question\n",
    "#### Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateBootStrapData(X, y):\n",
    "    \"\"\"\n",
    "    Description: Implements the bootstrap using your own functions. Do not use any functionality\n",
    "        from sklearn.\n",
    "    Input:\n",
    "        X (numpy matrix): A numpy matrix of shape nxm\n",
    "        y (numpy array): A numpy array of length n\n",
    "    Output:\n",
    "        X_bs (numpy matrix): Same shape as X, with rows sampled with replacement from X\n",
    "        y_bs (numpy array): Similar to X_bs but based off of y\n",
    "    \"\"\"\n",
    "    bootStrapSampleIdxs = np.array(random.choices(np.arange(X.shape[0]), k = X.shape[0]))\n",
    "    X_bs = X[bootStrapSampleIdxs]\n",
    "    y_bs = y[bootStrapSampleIdxs]\n",
    "    return(X_bs, y_bs)\n",
    "\n",
    "\n",
    "class ensembleClassifier(BaseEstimator, ClassifierMixin):  \n",
    "    \"\"\"\n",
    "    We're going to implement an ensemble classifier using a simple sklearn base estimator class.\n",
    "    There's a lot more to implementing an estimator in sklearn, but we'll only implement three methods\n",
    "    in addition to the init method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nEstimators):\n",
    "        \"\"\"\n",
    "        Description: Called when initializing the classifier. Don't do any heavy lifting here. Just store\n",
    "        information that your model will need when it's called. The init method should never learn the model,\n",
    "        that should be done in the fit method.\n",
    "        Note: It's s a good idea to name your objects with the same name as the arguments specified to\n",
    "        init the method.\n",
    "        Input:\n",
    "            nEstimators (int): The number of estimators to be fit for the enesmble method.\n",
    "        Output:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Store objects that you'll need for later.\n",
    "        # Your code here\n",
    "\n",
    "        # End your code\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Description: Fits the classifier by fitting all ensemble models on bootstrapped data.\n",
    "        You'll be making use of the generateBootStrapData() function. \n",
    "        All heavy lifting is done here. You'll need to store all estimators a list called self.estimators_\n",
    "        self.estimators_ will contain nEstimators LogisticRegression() objects\n",
    "        Input:\n",
    "            X (numpy matrix): A numpy matrix of shape nxm\n",
    "            y (numpy array): A numpy array of length n\n",
    "        Output:\n",
    "            self (ensembleClassifier): Always return self when in the fit function to work with sklearn.\n",
    "        \"\"\"\n",
    "        # Fit nEstimators logistic regressions\n",
    "        # Your code here\n",
    "        self.estimators_ = \n",
    "        # End your code\n",
    "        return(self)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Description: For each of your classification models make a class prediction for each \n",
    "            observation. Take an average of all predictions and use a threshold of 0.5 to make a class prediction\n",
    "            to create an overall ensemble prediction.\n",
    "        Input:\n",
    "            X (numpy matrix): A numpy matrix of shape nxm. Can be unseen or seen data.\n",
    "        Output:\n",
    "            predictions (np.array): A numpy array of 1s or 0s indicating a prediction for all\n",
    "                n inputs in X.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            getattr(self, \"estimators_\")\n",
    "        except AttributeError:\n",
    "            raise RuntimeError(\"You're gonna want to train the your estimators first my (th)dude(et)\")\n",
    "            # fun fact. \"thude\" is a gender neutral term for dude.\n",
    "        # Make some predictions\n",
    "        # Your code here\n",
    " \n",
    "        # End your code\n",
    "        return(predictions)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Description: Using your prediction function predict the classes for X, and assess performance \n",
    "            against y using accuracy.\n",
    "        Input:\n",
    "            X (numpy matrix): A numpy matrix of shape nxm. Can be unseen or seen data.\n",
    "            y (numpy array): A numpy array of classes with the same number of observations as X\n",
    "        Output:\n",
    "            accuracy (float): a float between 0 and 1 indicating how many elements you predicted the\n",
    "                correct class for\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        # Your code here\n",
    "        \n",
    "        # end your code\n",
    "        return(accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ens = ensembleClassifier(nEstimators = 2)\n",
    "clf_ens.fit(X = X_train, y = y_train)\n",
    "score = clf_ens.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testClassifier(X_train, y_train, X_test, y_test, nEstimators):\n",
    "    \"\"\"\n",
    "    Description: Runs your classifier a bunch of times to get an estimate of performance. Expects that\n",
    "        all methods have been completed in the ensembleClassifier class. \n",
    "        NOTE: You should not change this function at all. Your model should be able to be run using the\n",
    "        code presented here.\n",
    "    Input:\n",
    "        X_train, y_train, X_test, y_test (numpy matrices and arrays): X_() is a matrix of observations\n",
    "            y_() is a numpy array of classes of the same shape as its X_() conterpart\n",
    "        nEstimators (int): Creates a new ensemble classifier each time with nEstimators classifiers.\n",
    "            NOTE: You might have to do something similar to this retrain multiple estimators. Notice how \n",
    "            I'm initializing a new object at each iteration. Think about (google) why I might be doing this.\n",
    "    Output\n",
    "        None: Just here to print.\n",
    "    \"\"\"\n",
    "    allScores = []\n",
    "    nRuns = 50\n",
    "    for i in range(nRuns):\n",
    "        clf_ens = ensembleClassifier(nEstimators = nEstimators)\n",
    "        clf_ens.fit(X = X_train, y = y_train)\n",
    "        score = clf_ens.score(X_test, y_test)\n",
    "        allScores.append(score)\n",
    "    mean = np.mean(allScores)\n",
    "    std = np.std(allScores)\n",
    "    print(\"Average score for {} runs and {} estimators: {} (std:{})\".format(nRuns, nEstimators, mean, std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the performance of your model by getting the measure of multiple runs.\n",
    "for i in np.arange(1, 11, 1):\n",
    "    testClassifier(X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test, nEstimators = i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Confidence Intervals\n",
    "No we want to test if a classifier of our choice is better than a simple majority classifier, and assess how sure we are of this claim. We're going to use this by using even more bootstrapping to create new trainig datasets, train our classifier, and assess performance on the same test set.  **You can use any classification model you want as long as it's accessible from sklearn, otherwise you're gonna have a bad time. You can even use your ensemble model.**\n",
    "\n",
    "In order to do this we're going to use our bootstrap model above to assess performance on even more bootstrap data. The general algorthm is as follows.\n",
    "\n",
    "Given $X_{train}$, $y_{train}$, $X_{test}$ and $y_{test}$ as inputs do.\n",
    "1. Sample $X^{bs}_{train}$ from $X_{train}$ and $y^{bs}_{train}$, from $y_{train}$ using the bootstrap algorithm.\n",
    "2. Train our model using $X^{bs}_{train}$ and $y^{bs}_{train}$\n",
    "3. Evaluate the ensemble model on $X_{test}$ and $y_{test}$\n",
    "4. Store our test statistic (accuracy)\n",
    "\n",
    "\n",
    "Next evaluate whether or not a majority classifier trained on $X_{train}$ and evaluated on $X_{test}$ has performance within the 95% confidence intervals of the ensemble model. Note: A majority classifier simple takes the majority class in the training set and assigns the class to all values it's evaluated on\n",
    "\n",
    "\n",
    "### Deliverables\n",
    "Plot a histogram of the ensemble model's **bootstrap performances** with vertical lines for the **lower and upper confidence intervals** as well as for the **majority classifer performance**. You can get all but the majority classifier values from the function `calculateConfidenceIntervals`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateConfidenceIntervals(X_train, y_train, X_test, y_test, R, confInterval = 0.95):\n",
    "    \"\"\"\n",
    "    Description: Calculates confidence intervals of your model by obtaining R bootstrap samples of the \n",
    "        orginal training data, fitting a classifier on this data, and assessing performance on the original\n",
    "        testing data. Then the (1-confInterval)/2% of scores are trimmed from either side of the sorted\n",
    "        scores to assess which scores are within the confidence interval. Please refer to your reading\n",
    "        on confidence intervals in Practical Statistics for Data Scientists. \n",
    "    Input:\n",
    "        X_train, y_train, X_test, y_test (numpy matrices and arrays): X_() is a matrix of observations\n",
    "            y_() is a numpy array of classes of the same shape as its X_() conterpart\n",
    "        R (int): The number of times to generate bootstrap samples and train a model  \n",
    "        confInterval (float): A float between 0.0 and 1.0 indicating the confidence interval.\n",
    "    Output:\n",
    "        allScores (np.array): An array of scores generated using bootstrapped data.\n",
    "        lowerConfBound (float): An elemnt from allScores which indicates the cutoof for the upper bound\n",
    "            of the confidence interval\n",
    "        upperConfBound (float): An elemnt from allScores which indicates the cutoof for the lower bound\n",
    "            of the confidence interval\n",
    "    \"\"\"\n",
    "    allScores = [0.0]*R\n",
    "    # Your code here\n",
    "    for r in tqdm_notebook(range(R)):\n",
    "\n",
    "    # End your code\n",
    "    return(allScores, lowerConfBound, upperConfBound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allScores, lowerConfBound, upperConfBound = calculateConfidenceIntervals(X_train = X_train, y_train = y_train,\n",
    "                                                                         X_test = X_test, y_test = y_test,\n",
    "                                                                         R = 1000,\n",
    "                                                                         confInterval = 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement code to \"train\" a majority classifier and get its accuracy on the test set.\n",
    "# Your code here\n",
    "\n",
    "# End your code\n",
    "print(\"majority classifier performance = {}\".format(majorityClassiferPerf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot what is described in the deliverables\n",
    "# Your code here\n",
    "\n",
    "# End your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Computational Methods)",
   "language": "python",
   "name": "compmeth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
