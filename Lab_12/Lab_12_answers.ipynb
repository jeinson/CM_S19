{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping and Permutation Tests\n",
    "In class we learned how to perform bootstrapping, and permutation tests. In today's lab we'll use both of these methods to 1) build a random forest almost from scratch, and assess how well singificantly different a machine learning model is from luck or random chance performance. After this lab you should have a deeper understanding of how both of these methods are implemented by libraries in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "breastCancer = datasets.load_breast_cancer()\n",
    "X_all = breastCancer.data\n",
    "randomNumGen = np.random.RandomState(seed=23)\n",
    "E = randomNumGen.normal(size=X_all.shape, loc = 0.0, scale = 1000)\n",
    "# Add noisy data to the informative features for make the task harder\n",
    "X_all += E\n",
    "y_all = breastCancer.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size = 0.25, random_state = 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "-14.248636442310712\n"
     ]
    }
   ],
   "source": [
    "print(E.shape)\n",
    "print(E.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Bootstrapping, Aggregation, and Random Forests\n",
    "In this section you'll implement a random forest model. Don't worry, you'll have ample access to scikit-learn libraries, but you won't be able to use the [RandomForestClassifer](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) because that would defeat the purpose of learning some of the ins and outs of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "# scikit-learn bootstrap\n",
    "from sklearn.utils import resample\n",
    "class ensembleLearner(BaseEstimator, ClassifierMixin):  \n",
    "    \"\"\"Random Forest classifer\"\"\"\n",
    "\n",
    "    def __init__(self, nEstimators):\n",
    "        \"\"\"\n",
    "        Called when initializing the classifier\n",
    "        \"\"\"\n",
    "        self.nEstimators = nEstimators\n",
    "\n",
    "\n",
    "\n",
    "    def generateBootStrapData(self, X, y):\n",
    "        bootStrapSampleIdxs = np.array(random.choices(np.arange(X.shape[0]), k = X.shape[0]))\n",
    "        return(X[bootStrapSampleIdxs], y[bootStrapSampleIdxs])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        This should fit classifier. All the \"work\" should be done here.\n",
    "\n",
    "        Note: assert is not a good choice here and you should rather\n",
    "        use try/except blog with exceptions. This is just for short syntax.\n",
    "        \"\"\"\n",
    "        self.estimators_ = [LogisticRegression(solver = \"liblinear\") for i in range(self.nEstimators)]\n",
    "\n",
    "        if self.nEstimators > 1:\n",
    "            for estimator in self.estimators_:\n",
    "                X_bootStrap, y_bootStrap = self.generateBootStrapData(X, y)\n",
    "                estimator.fit(X_bootStrap, y_bootStrap)\n",
    "        else:\n",
    "            self.estimators_[0].fit(X, y)\n",
    "\n",
    "        return(self)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        try:\n",
    "            getattr(self, \"estimators_\")\n",
    "        except AttributeError:\n",
    "            raise RuntimeError(\"You must train classifer before predicting data!\")\n",
    "        predictions = [estimator.predict(X) for estimator in self.estimators_]\n",
    "        predictions = np.array(predictions)\n",
    "        predictions = np.mean(predictions, axis = 0)\n",
    "        predictions[predictions >= 0.5] = 1\n",
    "        predictions[predictions < 0.5] = 0\n",
    "        return(predictions)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        accuracy = np.mean(predictions == y)\n",
    "        return(accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6853146853146853\n"
     ]
    }
   ],
   "source": [
    "clf_ens = ensembleLearner(nEstimators = 1)#newton-cg', 'sag', 'saga' and 'lbfgs'\n",
    "clf_ens.fit(X = X_train, y = y_train)\n",
    "score = clf_ens.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testClassifier(X_train, y_train, X_test, y_test, nEstimators):\n",
    "    allScores = []\n",
    "    nRuns = 100\n",
    "    for i in range(nRuns):\n",
    "        clf_ens = ensembleLearner(nEstimators = nEstimators)#newton-cg', 'sag', 'saga' and 'lbfgs'\n",
    "        clf_ens.fit(X = X_train, y = y_train)\n",
    "        score = clf_ens.score(X_test, y_test)\n",
    "        allScores.append(score)\n",
    "    mean = np.mean(allScores)\n",
    "    std = np.std(allScores)\n",
    "    print(\"Average score for {} runs and {} estimators: {} (std:{})\".format(nRuns, nEstimators, mean, std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score for 100 runs and 1 estimators: 0.6853146853146849 (std:4.440892098500626e-16)\n",
      "Average score for 100 runs and 10 estimators: 0.7004895104895104 (std:0.016039921588081783)\n",
      "Average score for 100 runs and 20 estimators: 0.6989510489510488 (std:0.010759917869530858)\n",
      "Average score for 100 runs and 30 estimators: 0.6950349650349651 (std:0.01105227001580774)\n",
      "Average score for 100 runs and 40 estimators: 0.6958741258741257 (std:0.009036686060078005)\n",
      "Average score for 100 runs and 50 estimators: 0.6931468531468529 (std:0.008637425262458455)\n"
     ]
    }
   ],
   "source": [
    "for i in [1, 10, 20, 30, 40, 50, 100]:\n",
    "    testClassifier(X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test, nEstimators = i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score for 100 runs and 1 estimators: 0.6853146853146849 (std:4.440892098500626e-16)\n",
      "Average score for 100 runs and 2 estimators: 0.705104895104895 (std:0.01727301038809905)\n",
      "Average score for 100 runs and 3 estimators: 0.6916783216783216 (std:0.02005164733406758)\n",
      "Average score for 100 runs and 4 estimators: 0.7093006993006994 (std:0.017890484651022238)\n",
      "Average score for 100 runs and 5 estimators: 0.6941958041958043 (std:0.01633596967815023)\n",
      "Average score for 100 runs and 6 estimators: 0.7055944055944056 (std:0.016592741982803746)\n",
      "Average score for 100 runs and 7 estimators: 0.7008391608391605 (std:0.01424858423396874)\n",
      "Average score for 100 runs and 8 estimators: 0.7037062937062936 (std:0.013292416884013862)\n",
      "Average score for 100 runs and 9 estimators: 0.6976223776223776 (std:0.015132065694947117)\n",
      "Average score for 100 runs and 10 estimators: 0.7022377622377621 (std:0.013745484145452281)\n",
      "Average score for 100 runs and 11 estimators: 0.6988111888111888 (std:0.01484281178105426)\n",
      "Average score for 100 runs and 12 estimators: 0.7020979020979021 (std:0.012548502409039422)\n",
      "Average score for 100 runs and 13 estimators: 0.698951048951049 (std:0.012406430069377746)\n",
      "Average score for 100 runs and 14 estimators: 0.6991608391608392 (std:0.012625428219395813)\n",
      "Average score for 100 runs and 15 estimators: 0.6962937062937061 (std:0.013783676234922633)\n",
      "Average score for 100 runs and 16 estimators: 0.6962237762237762 (std:0.012677224272587032)\n",
      "Average score for 100 runs and 17 estimators: 0.6972727272727275 (std:0.012442640658942072)\n",
      "Average score for 100 runs and 18 estimators: 0.6959440559440558 (std:0.012528221357515353)\n",
      "Average score for 100 runs and 19 estimators: 0.6948251748251748 (std:0.010158892194313214)\n"
     ]
    }
   ],
   "source": [
    "myEstList = []\n",
    "for i in np.arange(1, 20, 1):\n",
    "    testClassifier(X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test, nEstimators = i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Permutation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Computational Methods)",
   "language": "python",
   "name": "computational_methods"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
