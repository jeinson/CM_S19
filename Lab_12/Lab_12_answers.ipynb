{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping and Permutation Tests\n",
    "In class we learned how to perform bootstrapping, and permutation tests. In today's lab we'll use both of these methods to 1) build a random forest almost from scratch, and assess how well singificantly different a machine learning model is from luck or random chance performance. After this lab you should have a deeper understanding of how both of these methods are implemented by libraries in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "breastCancer = datasets.load_breast_cancer()\n",
    "X_all = breastCancer.data\n",
    "# randomNumGen = np.random.RandomState(seed=23)\n",
    "# E = randomNumGen.normal(size=X.shape, loc = 0.0, scale = 5)\n",
    "# Add noisy data to the informative features for make the task harder\n",
    "# X = X + E\n",
    "y_all = breastCancer.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size = 0.25, random_state = 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(E.shape)\n",
    "# print(E.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Bootstrapping, Aggregation, and Random Forests\n",
    "In this section you'll implement a random forest model. Don't worry, you'll have ample access to scikit-learn libraries, but you won't be able to use the [RandomForestClassifer](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) because that would defeat the purpose of learning some of the ins and outs of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "# scikit-learn bootstrap\n",
    "from sklearn.utils import resample\n",
    "class ensembleLearner(BaseEstimator, ClassifierMixin):  \n",
    "    \"\"\"Random Forest classifer\"\"\"\n",
    "\n",
    "    def __init__(self, nEstimators):\n",
    "        \"\"\"\n",
    "        Called when initializing the classifier\n",
    "        \"\"\"\n",
    "        self.nEstimators = nEstimators\n",
    "\n",
    "\n",
    "\n",
    "    def generateBootStrapData(self, X, y):\n",
    "        bootStrapSampleIdxs = np.array(random.choices(np.arange(X.shape[0]), k = X.shape[0]))\n",
    "        return(X[bootStrapSampleIdxs], y[bootStrapSampleIdxs])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        This should fit classifier. All the \"work\" should be done here.\n",
    "\n",
    "        Note: assert is not a good choice here and you should rather\n",
    "        use try/except blog with exceptions. This is just for short syntax.\n",
    "        \"\"\"\n",
    "        self.estimators_ = [LogisticRegression(solver = \"liblinear\") for i in range(self.nEstimators)]\n",
    "\n",
    "        if self.nEstimators > 1:\n",
    "            for estimator in self.estimators_:\n",
    "                X_bootStrap, y_bootStrap = self.generateBootStrapData(X, y)\n",
    "                estimator.fit(X_bootStrap, y_bootStrap)\n",
    "        else:\n",
    "            self.estimators_[0].fit(X, y)\n",
    "\n",
    "        return(self)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        try:\n",
    "            getattr(self, \"estimators_\")\n",
    "        except AttributeError:\n",
    "            raise RuntimeError(\"You must train classifer before predicting data!\")\n",
    "        predictions = [estimator.predict(X) for estimator in self.estimators_]\n",
    "        predictions = np.array(predictions)\n",
    "        predictions = np.mean(predictions, axis = 0)\n",
    "        predictions[predictions >= 0.5] = 1\n",
    "        predictions[predictions < 0.5] = 0\n",
    "        return(predictions)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        accuracy = np.mean(predictions == y)\n",
    "        return(accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.951048951048951\n"
     ]
    }
   ],
   "source": [
    "clf_ens = ensembleLearner(nEstimators = 10)#newton-cg', 'sag', 'saga' and 'lbfgs'\n",
    "clf_ens.fit(X = X_train, y = y_train)\n",
    "score = clf_ens.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testClassifier(X_train, y_train, X_test, y_test, nEstimators):\n",
    "    allScores = []\n",
    "    nRuns = 100\n",
    "    for i in range(nRuns):\n",
    "        clf_ens = ensembleLearner(nEstimators = nEstimators)#newton-cg', 'sag', 'saga' and 'lbfgs'\n",
    "        clf_ens.fit(X = X_train, y = y_train)\n",
    "        score = clf_ens.score(X_test, y_test)\n",
    "        allScores.append(score)\n",
    "    mean = np.mean(allScores)\n",
    "    std = np.std(allScores)\n",
    "    print(\"Average score for {} runs and {} estimators: {} (std:{})\".format(nRuns, nEstimators, mean, std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score for 100 runs and 1 estimators: 0.9510489510489509 (std:1.1102230246251565e-16)\n",
      "Average score for 100 runs and 2 estimators: 0.9516083916083916 (std:0.006900081892891739)\n",
      "Average score for 100 runs and 10 estimators: 0.9532167832167832 (std:0.004276039369569347)\n",
      "Average score for 100 runs and 20 estimators: 0.952867132867133 (std:0.0033711806414528462)\n",
      "Average score for 100 runs and 30 estimators: 0.952027972027972 (std:0.0024264827374681722)\n",
      "Average score for 100 runs and 40 estimators: 0.952027972027972 (std:0.002426482737468173)\n",
      "Average score for 100 runs and 50 estimators: 0.952027972027972 (std:0.0024264827374681722)\n",
      "Average score for 100 runs and 100 estimators: 0.9516083916083915 (std:0.0018971552400350359)\n",
      "Average score for 100 runs and 200 estimators: 0.9510489510489509 (std:1.1102230246251565e-16)\n"
     ]
    }
   ],
   "source": [
    "testClassifier(X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test, nEstimators = 1)\n",
    "testClassifier(X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test, nEstimators = 2)\n",
    "testClassifier(X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test, nEstimators = 10)\n",
    "testClassifier(X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test, nEstimators = 20)\n",
    "testClassifier(X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test, nEstimators = 30)\n",
    "testClassifier(X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test, nEstimators = 40)\n",
    "testClassifier(X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test, nEstimators = 50)\n",
    "testClassifier(X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test, nEstimators = 100)\n",
    "testClassifier(X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test, nEstimators = 200)\n",
    "testClassifier(X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test, nEstimators = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Permutation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Computational Methods)",
   "language": "python",
   "name": "computational_methods"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
