{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(50)\n",
    "pts = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = np.random.rand(50)\n",
    "x_train = np.asarray(x_vals,dtype=np.float32).reshape(-1,1)\n",
    "m = 1\n",
    "alpha = np.random.rand(1)\n",
    "beta = np.random.rand(1)\n",
    "y_correct = np.asarray([2*i+m for i in x_vals], dtype=np.float32).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.61673141])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "\n",
    "        super(LinearRegressionModel, self).__init__() \n",
    "        # Calling Super Class's constructor\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        # nn.linear is defined in nn.Module\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Here the forward pass is simply a linear function\n",
    "\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "input_dim = 1\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionModel(input_dim,output_dim)# create our model just as we do in Scikit-Learn / C / C++//\n",
    "\n",
    "criterion = nn.MSELoss()# Mean Squared Loss\n",
    "l_rate = 0.01\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr = l_rate) #Stochastic Gradient Descent\n",
    "\n",
    "epochs = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of LinearRegressionModel(\n",
       "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = inputs.t()\n",
    "b = model.forward(inputs).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5234)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(inputs, labels).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9892],\n",
       "        [ 1.4562],\n",
       "        [ 1.5109],\n",
       "        [ 1.7927],\n",
       "        [ 1.7546],\n",
       "        [ 2.9931],\n",
       "        [ 1.8164],\n",
       "        [ 2.5438],\n",
       "        [ 2.5211],\n",
       "        [ 1.6200],\n",
       "        [ 1.6931],\n",
       "        [ 1.7035],\n",
       "        [ 1.2909],\n",
       "        [ 2.9453],\n",
       "        [ 2.8184],\n",
       "        [ 2.1199],\n",
       "        [ 1.6272],\n",
       "        [ 2.7764],\n",
       "        [ 2.3491],\n",
       "        [ 1.7822],\n",
       "        [ 2.0144],\n",
       "        [ 2.0482],\n",
       "        [ 2.8560],\n",
       "        [ 2.1427],\n",
       "        [ 2.3367],\n",
       "        [ 1.1045],\n",
       "        [ 1.6541],\n",
       "        [ 1.1128],\n",
       "        [ 1.3597],\n",
       "        [ 2.8519],\n",
       "        [ 2.8760],\n",
       "        [ 2.4282],\n",
       "        [ 2.4654],\n",
       "        [ 1.9235],\n",
       "        [ 2.8627],\n",
       "        [ 1.8128],\n",
       "        [ 2.3664],\n",
       "        [ 2.2998],\n",
       "        [ 2.1975],\n",
       "        [ 1.4441],\n",
       "        [ 2.3647],\n",
       "        [ 2.7561],\n",
       "        [ 2.5934],\n",
       "        [ 1.8640],\n",
       "        [ 2.8358],\n",
       "        [ 2.5637],\n",
       "        [ 2.4515],\n",
       "        [ 1.2497],\n",
       "        [ 2.8326],\n",
       "        [ 1.7754]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 5.940560817718506, linear weight tensor([[ 0.5813]])\n",
      "epoch 2, loss 5.627352237701416, linear weight tensor([[ 0.6098]])\n",
      "epoch 3, loss 5.330681800842285, linear weight tensor([[ 0.6376]])\n",
      "epoch 4, loss 5.04967737197876, linear weight tensor([[ 0.6647]])\n",
      "epoch 5, loss 4.783511161804199, linear weight tensor([[ 0.6910]])\n",
      "epoch 6, loss 4.531399250030518, linear weight tensor([[ 0.7167]])\n",
      "epoch 7, loss 4.292601108551025, linear weight tensor([[ 0.7416]])\n",
      "epoch 8, loss 4.066411018371582, linear weight tensor([[ 0.7659]])\n",
      "epoch 9, loss 3.852166175842285, linear weight tensor([[ 0.7896]])\n",
      "epoch 10, loss 3.649233818054199, linear weight tensor([[ 0.8127]])\n",
      "epoch 11, loss 3.4570159912109375, linear weight tensor([[ 0.8351]])\n",
      "epoch 12, loss 3.274948835372925, linear weight tensor([[ 0.8569]])\n",
      "epoch 13, loss 3.102494716644287, linear weight tensor([[ 0.8782]])\n",
      "epoch 14, loss 2.9391472339630127, linear weight tensor([[ 0.8989]])\n",
      "epoch 15, loss 2.7844250202178955, linear weight tensor([[ 0.9191]])\n",
      "epoch 16, loss 2.637871742248535, linear weight tensor([[ 0.9387]])\n",
      "epoch 17, loss 2.4990570545196533, linear weight tensor([[ 0.9579]])\n",
      "epoch 18, loss 2.367572069168091, linear weight tensor([[ 0.9765]])\n",
      "epoch 19, loss 2.243029832839966, linear weight tensor([[ 0.9946]])\n",
      "epoch 20, loss 2.1250624656677246, linear weight tensor([[ 1.0123]])\n",
      "epoch 21, loss 2.013324499130249, linear weight tensor([[ 1.0295]])\n",
      "epoch 22, loss 1.9074863195419312, linear weight tensor([[ 1.0462]])\n",
      "epoch 23, loss 1.8072360754013062, linear weight tensor([[ 1.0625]])\n",
      "epoch 24, loss 1.7122787237167358, linear weight tensor([[ 1.0784]])\n",
      "epoch 25, loss 1.6223351955413818, linear weight tensor([[ 1.0939]])\n",
      "epoch 26, loss 1.5371403694152832, linear weight tensor([[ 1.1090]])\n",
      "epoch 27, loss 1.456443428993225, linear weight tensor([[ 1.1236]])\n",
      "epoch 28, loss 1.3800071477890015, linear weight tensor([[ 1.1379]])\n",
      "epoch 29, loss 1.3076062202453613, linear weight tensor([[ 1.1518]])\n",
      "epoch 30, loss 1.239027738571167, linear weight tensor([[ 1.1654]])\n",
      "epoch 31, loss 1.174069881439209, linear weight tensor([[ 1.1786]])\n",
      "epoch 32, loss 1.1125407218933105, linear weight tensor([[ 1.1914]])\n",
      "epoch 33, loss 1.0542601346969604, linear weight tensor([[ 1.2040]])\n",
      "epoch 34, loss 0.9990565776824951, linear weight tensor([[ 1.2162]])\n",
      "epoch 35, loss 0.9467669129371643, linear weight tensor([[ 1.2280]])\n",
      "epoch 36, loss 0.8972377777099609, linear weight tensor([[ 1.2396]])\n",
      "epoch 37, loss 0.850322961807251, linear weight tensor([[ 1.2509]])\n",
      "epoch 38, loss 0.8058847784996033, linear weight tensor([[ 1.2619]])\n",
      "epoch 39, loss 0.7637922763824463, linear weight tensor([[ 1.2726]])\n",
      "epoch 40, loss 0.723921537399292, linear weight tensor([[ 1.2830]])\n",
      "epoch 41, loss 0.6861554980278015, linear weight tensor([[ 1.2931]])\n",
      "epoch 42, loss 0.6503827571868896, linear weight tensor([[ 1.3030]])\n",
      "epoch 43, loss 0.6164981722831726, linear weight tensor([[ 1.3127]])\n",
      "epoch 44, loss 0.5844019651412964, linear weight tensor([[ 1.3220]])\n",
      "epoch 45, loss 0.5539997220039368, linear weight tensor([[ 1.3312]])\n",
      "epoch 46, loss 0.5252020359039307, linear weight tensor([[ 1.3401]])\n",
      "epoch 47, loss 0.49792400002479553, linear weight tensor([[ 1.3488]])\n",
      "epoch 48, loss 0.4720853865146637, linear weight tensor([[ 1.3572]])\n",
      "epoch 49, loss 0.4476105570793152, linear weight tensor([[ 1.3655]])\n",
      "epoch 50, loss 0.42442721128463745, linear weight tensor([[ 1.3735]])\n",
      "epoch 51, loss 0.4024670720100403, linear weight tensor([[ 1.3813]])\n",
      "epoch 52, loss 0.38166558742523193, linear weight tensor([[ 1.3890]])\n",
      "epoch 53, loss 0.3619616627693176, linear weight tensor([[ 1.3964]])\n",
      "epoch 54, loss 0.34329739212989807, linear weight tensor([[ 1.4036]])\n",
      "epoch 55, loss 0.32561761140823364, linear weight tensor([[ 1.4107]])\n",
      "epoch 56, loss 0.3088705539703369, linear weight tensor([[ 1.4176]])\n",
      "epoch 57, loss 0.29300692677497864, linear weight tensor([[ 1.4243]])\n",
      "epoch 58, loss 0.2779800295829773, linear weight tensor([[ 1.4308]])\n",
      "epoch 59, loss 0.2637457549571991, linear weight tensor([[ 1.4372]])\n",
      "epoch 60, loss 0.2502622604370117, linear weight tensor([[ 1.4434]])\n",
      "epoch 61, loss 0.237489715218544, linear weight tensor([[ 1.4494]])\n",
      "epoch 62, loss 0.22539085149765015, linear weight tensor([[ 1.4553]])\n",
      "epoch 63, loss 0.21392986178398132, linear weight tensor([[ 1.4611]])\n",
      "epoch 64, loss 0.20307329297065735, linear weight tensor([[ 1.4667]])\n",
      "epoch 65, loss 0.1927889883518219, linear weight tensor([[ 1.4722]])\n",
      "epoch 66, loss 0.18304681777954102, linear weight tensor([[ 1.4775]])\n",
      "epoch 67, loss 0.1738181859254837, linear weight tensor([[ 1.4827]])\n",
      "epoch 68, loss 0.16507597267627716, linear weight tensor([[ 1.4878]])\n",
      "epoch 69, loss 0.1567944586277008, linear weight tensor([[ 1.4927]])\n",
      "epoch 70, loss 0.1489492952823639, linear weight tensor([[ 1.4975]])\n",
      "epoch 71, loss 0.14151759445667267, linear weight tensor([[ 1.5022]])\n",
      "epoch 72, loss 0.13447730243206024, linear weight tensor([[ 1.5068]])\n",
      "epoch 73, loss 0.12780793011188507, linear weight tensor([[ 1.5113]])\n",
      "epoch 74, loss 0.1214897409081459, linear weight tensor([[ 1.5156]])\n",
      "epoch 75, loss 0.11550425738096237, linear weight tensor([[ 1.5199]])\n",
      "epoch 76, loss 0.10983401536941528, linear weight tensor([[ 1.5240]])\n",
      "epoch 77, loss 0.10446221381425858, linear weight tensor([[ 1.5281]])\n",
      "epoch 78, loss 0.09937319904565811, linear weight tensor([[ 1.5320]])\n",
      "epoch 79, loss 0.0945519432425499, linear weight tensor([[ 1.5359]])\n",
      "epoch 80, loss 0.08998443931341171, linear weight tensor([[ 1.5396]])\n",
      "epoch 81, loss 0.08565719425678253, linear weight tensor([[ 1.5433]])\n",
      "epoch 82, loss 0.08155754953622818, linear weight tensor([[ 1.5469]])\n",
      "epoch 83, loss 0.07767345756292343, linear weight tensor([[ 1.5504]])\n",
      "epoch 84, loss 0.07399354875087738, linear weight tensor([[ 1.5538]])\n",
      "epoch 85, loss 0.07050710916519165, linear weight tensor([[ 1.5571]])\n",
      "epoch 86, loss 0.0672038346529007, linear weight tensor([[ 1.5604]])\n",
      "epoch 87, loss 0.0640740841627121, linear weight tensor([[ 1.5635]])\n",
      "epoch 88, loss 0.06110876053571701, linear weight tensor([[ 1.5666]])\n",
      "epoch 89, loss 0.058299094438552856, linear weight tensor([[ 1.5696]])\n",
      "epoch 90, loss 0.05563689395785332, linear weight tensor([[ 1.5726]])\n",
      "epoch 91, loss 0.05311433970928192, linear weight tensor([[ 1.5755]])\n",
      "epoch 92, loss 0.05072413757443428, linear weight tensor([[ 1.5783]])\n",
      "epoch 93, loss 0.0484592579305172, linear weight tensor([[ 1.5810]])\n",
      "epoch 94, loss 0.04631306976079941, linear weight tensor([[ 1.5837]])\n",
      "epoch 95, loss 0.044279370456933975, linear weight tensor([[ 1.5863]])\n",
      "epoch 96, loss 0.04235216602683067, linear weight tensor([[ 1.5889]])\n",
      "epoch 97, loss 0.04052579775452614, linear weight tensor([[ 1.5914]])\n",
      "epoch 98, loss 0.038794972002506256, linear weight tensor([[ 1.5938]])\n",
      "epoch 99, loss 0.037154678255319595, linear weight tensor([[ 1.5962]])\n",
      "epoch 100, loss 0.03560014069080353, linear weight tensor([[ 1.5985]])\n",
      "epoch 101, loss 0.03412676975131035, linear weight tensor([[ 1.6008]])\n",
      "epoch 102, loss 0.0327303446829319, linear weight tensor([[ 1.6030]])\n",
      "epoch 103, loss 0.03140680864453316, linear weight tensor([[ 1.6052]])\n",
      "epoch 104, loss 0.030152272433042526, linear weight tensor([[ 1.6073]])\n",
      "epoch 105, loss 0.0289631225168705, linear weight tensor([[ 1.6094]])\n",
      "epoch 106, loss 0.02783587947487831, linear weight tensor([[ 1.6114]])\n",
      "epoch 107, loss 0.02676730416715145, linear weight tensor([[ 1.6134]])\n",
      "epoch 108, loss 0.025754278525710106, linear weight tensor([[ 1.6154]])\n",
      "epoch 109, loss 0.024793880060315132, linear weight tensor([[ 1.6173]])\n",
      "epoch 110, loss 0.023883327841758728, linear weight tensor([[ 1.6191]])\n",
      "epoch 111, loss 0.023020001128315926, linear weight tensor([[ 1.6209]])\n",
      "epoch 112, loss 0.022201402112841606, linear weight tensor([[ 1.6227]])\n",
      "epoch 113, loss 0.02142517641186714, linear weight tensor([[ 1.6245]])\n",
      "epoch 114, loss 0.020689059048891068, linear weight tensor([[ 1.6262]])\n",
      "epoch 115, loss 0.01999097503721714, linear weight tensor([[ 1.6278]])\n",
      "epoch 116, loss 0.019328894093632698, linear weight tensor([[ 1.6295]])\n",
      "epoch 117, loss 0.01870093308389187, linear weight tensor([[ 1.6311]])\n",
      "epoch 118, loss 0.018105266615748405, linear weight tensor([[ 1.6326]])\n",
      "epoch 119, loss 0.017540203407406807, linear weight tensor([[ 1.6341]])\n",
      "epoch 120, loss 0.01700415089726448, linear weight tensor([[ 1.6356]])\n",
      "epoch 121, loss 0.016495566815137863, linear weight tensor([[ 1.6371]])\n",
      "epoch 122, loss 0.016012974083423615, linear weight tensor([[ 1.6385]])\n",
      "epoch 123, loss 0.015555041842162609, linear weight tensor([[ 1.6399]])\n",
      "epoch 124, loss 0.01512044295668602, linear weight tensor([[ 1.6413]])\n",
      "epoch 125, loss 0.014707952737808228, linear weight tensor([[ 1.6427]])\n",
      "epoch 126, loss 0.01431640237569809, linear weight tensor([[ 1.6440]])\n",
      "epoch 127, loss 0.013944705948233604, linear weight tensor([[ 1.6453]])\n",
      "epoch 128, loss 0.013591810129582882, linear weight tensor([[ 1.6466]])\n",
      "epoch 129, loss 0.013256710954010487, linear weight tensor([[ 1.6478]])\n",
      "epoch 130, loss 0.01293847057968378, linear weight tensor([[ 1.6490]])\n",
      "epoch 131, loss 0.012636210769414902, linear weight tensor([[ 1.6502]])\n",
      "epoch 132, loss 0.012349077500402927, linear weight tensor([[ 1.6514]])\n",
      "epoch 133, loss 0.012076305225491524, linear weight tensor([[ 1.6525]])\n",
      "epoch 134, loss 0.01181710883975029, linear weight tensor([[ 1.6537]])\n",
      "epoch 135, loss 0.011570777744054794, linear weight tensor([[ 1.6548]])\n",
      "epoch 136, loss 0.011336625553667545, linear weight tensor([[ 1.6559]])\n",
      "epoch 137, loss 0.011114022694528103, linear weight tensor([[ 1.6569]])\n",
      "epoch 138, loss 0.010902365669608116, linear weight tensor([[ 1.6580]])\n",
      "epoch 139, loss 0.010701061226427555, linear weight tensor([[ 1.6590]])\n",
      "epoch 140, loss 0.010509571991860867, linear weight tensor([[ 1.6600]])\n",
      "epoch 141, loss 0.010327397845685482, linear weight tensor([[ 1.6610]])\n",
      "epoch 142, loss 0.010154038667678833, linear weight tensor([[ 1.6620]])\n",
      "epoch 143, loss 0.009989011101424694, linear weight tensor([[ 1.6629]])\n",
      "epoch 144, loss 0.009831900708377361, linear weight tensor([[ 1.6639]])\n",
      "epoch 145, loss 0.009682281874120235, linear weight tensor([[ 1.6648]])\n",
      "epoch 146, loss 0.009539758786559105, linear weight tensor([[ 1.6657]])\n",
      "epoch 147, loss 0.00940396636724472, linear weight tensor([[ 1.6666]])\n",
      "epoch 148, loss 0.009274546056985855, linear weight tensor([[ 1.6675]])\n",
      "epoch 149, loss 0.009151170961558819, linear weight tensor([[ 1.6683]])\n",
      "epoch 150, loss 0.009033503010869026, linear weight tensor([[ 1.6692]])\n",
      "epoch 151, loss 0.008921269327402115, linear weight tensor([[ 1.6700]])\n",
      "epoch 152, loss 0.008814160712063313, linear weight tensor([[ 1.6708]])\n",
      "epoch 153, loss 0.008711928501725197, linear weight tensor([[ 1.6716]])\n",
      "epoch 154, loss 0.008614294230937958, linear weight tensor([[ 1.6724]])\n",
      "epoch 155, loss 0.008521036244928837, linear weight tensor([[ 1.6732]])\n",
      "epoch 156, loss 0.008431910537183285, linear weight tensor([[ 1.6740]])\n",
      "epoch 157, loss 0.008346715942025185, linear weight tensor([[ 1.6747]])\n",
      "epoch 158, loss 0.00826523918658495, linear weight tensor([[ 1.6755]])\n",
      "epoch 159, loss 0.008187280036509037, linear weight tensor([[ 1.6762]])\n",
      "epoch 160, loss 0.008112662471830845, linear weight tensor([[ 1.6770]])\n",
      "epoch 161, loss 0.008041208609938622, linear weight tensor([[ 1.6777]])\n",
      "epoch 162, loss 0.007972766645252705, linear weight tensor([[ 1.6784]])\n",
      "epoch 163, loss 0.007907149381935596, linear weight tensor([[ 1.6791]])\n",
      "epoch 164, loss 0.00784424040466547, linear weight tensor([[ 1.6797]])\n",
      "epoch 165, loss 0.007783877197653055, linear weight tensor([[ 1.6804]])\n",
      "epoch 166, loss 0.007725932635366917, linear weight tensor([[ 1.6811]])\n",
      "epoch 167, loss 0.007670285180211067, linear weight tensor([[ 1.6817]])\n",
      "epoch 168, loss 0.007616809569299221, linear weight tensor([[ 1.6824]])\n",
      "epoch 169, loss 0.007565397769212723, linear weight tensor([[ 1.6830]])\n",
      "epoch 170, loss 0.007515931501984596, linear weight tensor([[ 1.6836]])\n",
      "epoch 171, loss 0.007468327879905701, linear weight tensor([[ 1.6843]])\n",
      "epoch 172, loss 0.0074224770069122314, linear weight tensor([[ 1.6849]])\n",
      "epoch 173, loss 0.007378283888101578, linear weight tensor([[ 1.6855]])\n",
      "epoch 174, loss 0.00733567401766777, linear weight tensor([[ 1.6861]])\n",
      "epoch 175, loss 0.007294557522982359, linear weight tensor([[ 1.6867]])\n",
      "epoch 176, loss 0.00725486408919096, linear weight tensor([[ 1.6872]])\n",
      "epoch 177, loss 0.0072165257297456264, linear weight tensor([[ 1.6878]])\n",
      "epoch 178, loss 0.007179452572017908, linear weight tensor([[ 1.6884]])\n",
      "epoch 179, loss 0.0071435971185564995, linear weight tensor([[ 1.6889]])\n",
      "epoch 180, loss 0.00710888858884573, linear weight tensor([[ 1.6895]])\n",
      "epoch 181, loss 0.007075268775224686, linear weight tensor([[ 1.6900]])\n",
      "epoch 182, loss 0.007042683195322752, linear weight tensor([[ 1.6906]])\n",
      "epoch 183, loss 0.007011077832430601, linear weight tensor([[ 1.6911]])\n",
      "epoch 184, loss 0.006980407051742077, linear weight tensor([[ 1.6916]])\n",
      "epoch 185, loss 0.00695061543956399, linear weight tensor([[ 1.6922]])\n",
      "epoch 186, loss 0.006921661552041769, linear weight tensor([[ 1.6927]])\n",
      "epoch 187, loss 0.00689350301399827, linear weight tensor([[ 1.6932]])\n",
      "epoch 188, loss 0.006866095121949911, linear weight tensor([[ 1.6937]])\n",
      "epoch 189, loss 0.006839409004896879, linear weight tensor([[ 1.6942]])\n",
      "epoch 190, loss 0.006813406944274902, linear weight tensor([[ 1.6947]])\n",
      "epoch 191, loss 0.006788042839616537, linear weight tensor([[ 1.6952]])\n",
      "epoch 192, loss 0.0067632985301315784, linear weight tensor([[ 1.6957]])\n",
      "epoch 193, loss 0.0067391349002718925, linear weight tensor([[ 1.6962]])\n",
      "epoch 194, loss 0.006715521682053804, linear weight tensor([[ 1.6966]])\n",
      "epoch 195, loss 0.006692435592412949, linear weight tensor([[ 1.6971]])\n",
      "epoch 196, loss 0.006669855210930109, linear weight tensor([[ 1.6976]])\n",
      "epoch 197, loss 0.006647744681686163, linear weight tensor([[ 1.6980]])\n",
      "epoch 198, loss 0.006626084912568331, linear weight tensor([[ 1.6985]])\n",
      "epoch 199, loss 0.006604858674108982, linear weight tensor([[ 1.6990]])\n",
      "epoch 200, loss 0.006584034767001867, linear weight tensor([[ 1.6994]])\n",
      "epoch 201, loss 0.0065636043436825275, linear weight tensor([[ 1.6999]])\n",
      "epoch 202, loss 0.0065435366705060005, linear weight tensor([[ 1.7003]])\n",
      "epoch 203, loss 0.0065238261595368385, linear weight tensor([[ 1.7007]])\n",
      "epoch 204, loss 0.006504445802420378, linear weight tensor([[ 1.7012]])\n",
      "epoch 205, loss 0.00648537976667285, linear weight tensor([[ 1.7016]])\n",
      "epoch 206, loss 0.006466623395681381, linear weight tensor([[ 1.7020]])\n",
      "epoch 207, loss 0.006448151543736458, linear weight tensor([[ 1.7025]])\n",
      "epoch 208, loss 0.006429955363273621, linear weight tensor([[ 1.7029]])\n",
      "epoch 209, loss 0.0064120180904865265, linear weight tensor([[ 1.7033]])\n",
      "epoch 210, loss 0.006394332740455866, linear weight tensor([[ 1.7037]])\n",
      "epoch 211, loss 0.006376888137310743, linear weight tensor([[ 1.7041]])\n",
      "epoch 212, loss 0.006359665188938379, linear weight tensor([[ 1.7046]])\n",
      "epoch 213, loss 0.006342660170048475, linear weight tensor([[ 1.7050]])\n",
      "epoch 214, loss 0.006325862370431423, linear weight tensor([[ 1.7054]])\n",
      "epoch 215, loss 0.0063092587515711784, linear weight tensor([[ 1.7058]])\n",
      "epoch 216, loss 0.006292841397225857, linear weight tensor([[ 1.7062]])\n",
      "epoch 217, loss 0.006276610307395458, linear weight tensor([[ 1.7066]])\n",
      "epoch 218, loss 0.0062605468556284904, linear weight tensor([[ 1.7070]])\n",
      "epoch 219, loss 0.00624464638531208, linear weight tensor([[ 1.7074]])\n",
      "epoch 220, loss 0.006228906102478504, linear weight tensor([[ 1.7078]])\n",
      "epoch 221, loss 0.0062133073806762695, linear weight tensor([[ 1.7081]])\n",
      "epoch 222, loss 0.006197857670485973, linear weight tensor([[ 1.7085]])\n",
      "epoch 223, loss 0.006182550918310881, linear weight tensor([[ 1.7089]])\n",
      "epoch 224, loss 0.006167371291667223, linear weight tensor([[ 1.7093]])\n",
      "epoch 225, loss 0.0061523145996034145, linear weight tensor([[ 1.7097]])\n",
      "epoch 226, loss 0.006137379910796881, linear weight tensor([[ 1.7101]])\n",
      "epoch 227, loss 0.0061225611716508865, linear weight tensor([[ 1.7104]])\n",
      "epoch 228, loss 0.006107857916504145, linear weight tensor([[ 1.7108]])\n",
      "epoch 229, loss 0.0060932571068406105, linear weight tensor([[ 1.7112]])\n",
      "epoch 230, loss 0.006078761070966721, linear weight tensor([[ 1.7115]])\n",
      "epoch 231, loss 0.006064359098672867, linear weight tensor([[ 1.7119]])\n",
      "epoch 232, loss 0.006050053983926773, linear weight tensor([[ 1.7123]])\n",
      "epoch 233, loss 0.006035846658051014, linear weight tensor([[ 1.7126]])\n",
      "epoch 234, loss 0.006021724548190832, linear weight tensor([[ 1.7130]])\n",
      "epoch 235, loss 0.006007685791701078, linear weight tensor([[ 1.7134]])\n",
      "epoch 236, loss 0.0059937285259366035, linear weight tensor([[ 1.7137]])\n",
      "epoch 237, loss 0.005979849025607109, linear weight tensor([[ 1.7141]])\n",
      "epoch 238, loss 0.005966046825051308, linear weight tensor([[ 1.7144]])\n",
      "epoch 239, loss 0.005952323786914349, linear weight tensor([[ 1.7148]])\n",
      "epoch 240, loss 0.005938665941357613, linear weight tensor([[ 1.7151]])\n",
      "epoch 241, loss 0.005925074219703674, linear weight tensor([[ 1.7155]])\n",
      "epoch 242, loss 0.005911553278565407, linear weight tensor([[ 1.7158]])\n",
      "epoch 243, loss 0.005898097064346075, linear weight tensor([[ 1.7162]])\n",
      "epoch 244, loss 0.005884704180061817, linear weight tensor([[ 1.7165]])\n",
      "epoch 245, loss 0.005871364381164312, linear weight tensor([[ 1.7169]])\n",
      "epoch 246, loss 0.005858090706169605, linear weight tensor([[ 1.7172]])\n",
      "epoch 247, loss 0.0058448705822229385, linear weight tensor([[ 1.7176]])\n",
      "epoch 248, loss 0.005831710062921047, linear weight tensor([[ 1.7179]])\n",
      "epoch 249, loss 0.005818599369376898, linear weight tensor([[ 1.7182]])\n",
      "epoch 250, loss 0.005805541295558214, linear weight tensor([[ 1.7186]])\n",
      "epoch 251, loss 0.005792536772787571, linear weight tensor([[ 1.7189]])\n",
      "epoch 252, loss 0.005779577884823084, linear weight tensor([[ 1.7193]])\n",
      "epoch 253, loss 0.005766669288277626, linear weight tensor([[ 1.7196]])\n",
      "epoch 254, loss 0.005753806326538324, linear weight tensor([[ 1.7199]])\n",
      "epoch 255, loss 0.0057409899309277534, linear weight tensor([[ 1.7203]])\n",
      "epoch 256, loss 0.005728225223720074, linear weight tensor([[ 1.7206]])\n",
      "epoch 257, loss 0.005715497769415379, linear weight tensor([[ 1.7209]])\n",
      "epoch 258, loss 0.005702820606529713, linear weight tensor([[ 1.7212]])\n",
      "epoch 259, loss 0.005690176039934158, linear weight tensor([[ 1.7216]])\n",
      "epoch 260, loss 0.005677580833435059, linear weight tensor([[ 1.7219]])\n",
      "epoch 261, loss 0.005665023811161518, linear weight tensor([[ 1.7222]])\n",
      "epoch 262, loss 0.005652506370097399, linear weight tensor([[ 1.7225]])\n",
      "epoch 263, loss 0.005640031304210424, linear weight tensor([[ 1.7229]])\n",
      "epoch 264, loss 0.005627588834613562, linear weight tensor([[ 1.7232]])\n",
      "epoch 265, loss 0.005615185480564833, linear weight tensor([[ 1.7235]])\n",
      "epoch 266, loss 0.0056028240360319614, linear weight tensor([[ 1.7238]])\n",
      "epoch 267, loss 0.005590496119111776, linear weight tensor([[ 1.7242]])\n",
      "epoch 268, loss 0.0055782003328204155, linear weight tensor([[ 1.7245]])\n",
      "epoch 269, loss 0.0055659436620771885, linear weight tensor([[ 1.7248]])\n",
      "epoch 270, loss 0.005553720984607935, linear weight tensor([[ 1.7251]])\n",
      "epoch 271, loss 0.005541530437767506, linear weight tensor([[ 1.7254]])\n",
      "epoch 272, loss 0.005529378540813923, linear weight tensor([[ 1.7257]])\n",
      "epoch 273, loss 0.005517251323908567, linear weight tensor([[ 1.7261]])\n",
      "epoch 274, loss 0.00550515903159976, linear weight tensor([[ 1.7264]])\n",
      "epoch 275, loss 0.005493104457855225, linear weight tensor([[ 1.7267]])\n",
      "epoch 276, loss 0.005481084808707237, linear weight tensor([[ 1.7270]])\n",
      "epoch 277, loss 0.005469087511301041, linear weight tensor([[ 1.7273]])\n",
      "epoch 278, loss 0.005457127001136541, linear weight tensor([[ 1.7276]])\n",
      "epoch 279, loss 0.005445200949907303, linear weight tensor([[ 1.7279]])\n",
      "epoch 280, loss 0.005433299578726292, linear weight tensor([[ 1.7282]])\n",
      "epoch 281, loss 0.005421426147222519, linear weight tensor([[ 1.7286]])\n",
      "epoch 282, loss 0.005409582983702421, linear weight tensor([[ 1.7289]])\n",
      "epoch 283, loss 0.00539777148514986, linear weight tensor([[ 1.7292]])\n",
      "epoch 284, loss 0.005385991185903549, linear weight tensor([[ 1.7295]])\n",
      "epoch 285, loss 0.005374239757657051, linear weight tensor([[ 1.7298]])\n",
      "epoch 286, loss 0.005362519063055515, linear weight tensor([[ 1.7301]])\n",
      "epoch 287, loss 0.005350825376808643, linear weight tensor([[ 1.7304]])\n",
      "epoch 288, loss 0.005339161027222872, linear weight tensor([[ 1.7307]])\n",
      "epoch 289, loss 0.0053275213576853275, linear weight tensor([[ 1.7310]])\n",
      "epoch 290, loss 0.005315911024808884, linear weight tensor([[ 1.7313]])\n",
      "epoch 291, loss 0.005304328165948391, linear weight tensor([[ 1.7316]])\n",
      "epoch 292, loss 0.005292776972055435, linear weight tensor([[ 1.7319]])\n",
      "epoch 293, loss 0.005281250458210707, linear weight tensor([[ 1.7322]])\n",
      "epoch 294, loss 0.00526974955573678, linear weight tensor([[ 1.7325]])\n",
      "epoch 295, loss 0.005258277058601379, linear weight tensor([[ 1.7328]])\n",
      "epoch 296, loss 0.005246829241514206, linear weight tensor([[ 1.7331]])\n",
      "epoch 297, loss 0.005235412623733282, linear weight tensor([[ 1.7334]])\n",
      "epoch 298, loss 0.005224023014307022, linear weight tensor([[ 1.7337]])\n",
      "epoch 299, loss 0.0052126566879451275, linear weight tensor([[ 1.7340]])\n",
      "epoch 300, loss 0.005201314110308886, linear weight tensor([[ 1.7343]])\n",
      "epoch 301, loss 0.005190000403672457, linear weight tensor([[ 1.7346]])\n",
      "epoch 302, loss 0.0051787118427455425, linear weight tensor([[ 1.7349]])\n",
      "epoch 303, loss 0.005167452152818441, linear weight tensor([[ 1.7352]])\n",
      "epoch 304, loss 0.005156216211616993, linear weight tensor([[ 1.7355]])\n",
      "epoch 305, loss 0.005145004019141197, linear weight tensor([[ 1.7358]])\n",
      "epoch 306, loss 0.005133818369358778, linear weight tensor([[ 1.7361]])\n",
      "epoch 307, loss 0.005122664384543896, linear weight tensor([[ 1.7364]])\n",
      "epoch 308, loss 0.005111527629196644, linear weight tensor([[ 1.7367]])\n",
      "epoch 309, loss 0.0051004160195589066, linear weight tensor([[ 1.7369]])\n",
      "epoch 310, loss 0.005089334677904844, linear weight tensor([[ 1.7372]])\n",
      "epoch 311, loss 0.005078272428363562, linear weight tensor([[ 1.7375]])\n",
      "epoch 312, loss 0.005067236255854368, linear weight tensor([[ 1.7378]])\n",
      "epoch 313, loss 0.0050562238320708275, linear weight tensor([[ 1.7381]])\n",
      "epoch 314, loss 0.005045239347964525, linear weight tensor([[ 1.7384]])\n",
      "epoch 315, loss 0.005034275818616152, linear weight tensor([[ 1.7387]])\n",
      "epoch 316, loss 0.005023340694606304, linear weight tensor([[ 1.7390]])\n",
      "epoch 317, loss 0.00501242745667696, linear weight tensor([[ 1.7393]])\n",
      "epoch 318, loss 0.005001542158424854, linear weight tensor([[ 1.7396]])\n",
      "epoch 319, loss 0.004990677814930677, linear weight tensor([[ 1.7398]])\n",
      "epoch 320, loss 0.004979842808097601, linear weight tensor([[ 1.7401]])\n",
      "epoch 321, loss 0.004969025030732155, linear weight tensor([[ 1.7404]])\n",
      "epoch 322, loss 0.004958233796060085, linear weight tensor([[ 1.7407]])\n",
      "epoch 323, loss 0.004947468172758818, linear weight tensor([[ 1.7410]])\n",
      "epoch 324, loss 0.004936723038554192, linear weight tensor([[ 1.7413]])\n",
      "epoch 325, loss 0.004926000721752644, linear weight tensor([[ 1.7416]])\n",
      "epoch 326, loss 0.004915307275950909, linear weight tensor([[ 1.7418]])\n",
      "epoch 327, loss 0.004904636647552252, linear weight tensor([[ 1.7421]])\n",
      "epoch 328, loss 0.004893988836556673, linear weight tensor([[ 1.7424]])\n",
      "epoch 329, loss 0.004883365705609322, linear weight tensor([[ 1.7427]])\n",
      "epoch 330, loss 0.004872763063758612, linear weight tensor([[ 1.7430]])\n",
      "epoch 331, loss 0.004862186033278704, linear weight tensor([[ 1.7433]])\n",
      "epoch 332, loss 0.004851628560572863, linear weight tensor([[ 1.7435]])\n",
      "epoch 333, loss 0.004841095767915249, linear weight tensor([[ 1.7438]])\n",
      "epoch 334, loss 0.004830587655305862, linear weight tensor([[ 1.7441]])\n",
      "epoch 335, loss 0.004820097237825394, linear weight tensor([[ 1.7444]])\n",
      "epoch 336, loss 0.004809634294360876, linear weight tensor([[ 1.7447]])\n",
      "epoch 337, loss 0.004799194633960724, linear weight tensor([[ 1.7450]])\n",
      "epoch 338, loss 0.004788776859641075, linear weight tensor([[ 1.7452]])\n",
      "epoch 339, loss 0.004778382834047079, linear weight tensor([[ 1.7455]])\n",
      "epoch 340, loss 0.004768012091517448, linear weight tensor([[ 1.7458]])\n",
      "epoch 341, loss 0.004757662303745747, linear weight tensor([[ 1.7461]])\n",
      "epoch 342, loss 0.004747334402054548, linear weight tensor([[ 1.7463]])\n",
      "epoch 343, loss 0.004737030249089003, linear weight tensor([[ 1.7466]])\n",
      "epoch 344, loss 0.004726744256913662, linear weight tensor([[ 1.7469]])\n",
      "epoch 345, loss 0.00471648620441556, linear weight tensor([[ 1.7472]])\n",
      "epoch 346, loss 0.004706250503659248, linear weight tensor([[ 1.7475]])\n",
      "epoch 347, loss 0.004696038085967302, linear weight tensor([[ 1.7477]])\n",
      "epoch 348, loss 0.004685846623033285, linear weight tensor([[ 1.7480]])\n",
      "epoch 349, loss 0.0046756756491959095, linear weight tensor([[ 1.7483]])\n",
      "epoch 350, loss 0.004665532615035772, linear weight tensor([[ 1.7486]])\n",
      "epoch 351, loss 0.004655401222407818, linear weight tensor([[ 1.7488]])\n",
      "epoch 352, loss 0.0046453014947474, linear weight tensor([[ 1.7491]])\n",
      "epoch 353, loss 0.004635223653167486, linear weight tensor([[ 1.7494]])\n",
      "epoch 354, loss 0.004625162575393915, linear weight tensor([[ 1.7497]])\n",
      "epoch 355, loss 0.004615128971636295, linear weight tensor([[ 1.7499]])\n",
      "epoch 356, loss 0.004605113063007593, linear weight tensor([[ 1.7502]])\n",
      "epoch 357, loss 0.004595120437443256, linear weight tensor([[ 1.7505]])\n",
      "epoch 358, loss 0.004585146438330412, linear weight tensor([[ 1.7508]])\n",
      "epoch 359, loss 0.004575192462652922, linear weight tensor([[ 1.7510]])\n",
      "epoch 360, loss 0.004565265960991383, linear weight tensor([[ 1.7513]])\n",
      "epoch 361, loss 0.004555358551442623, linear weight tensor([[ 1.7516]])\n",
      "epoch 362, loss 0.0045454720966517925, linear weight tensor([[ 1.7518]])\n",
      "epoch 363, loss 0.004535607993602753, linear weight tensor([[ 1.7521]])\n",
      "epoch 364, loss 0.004525767173618078, linear weight tensor([[ 1.7524]])\n",
      "epoch 365, loss 0.0045159440487623215, linear weight tensor([[ 1.7527]])\n",
      "epoch 366, loss 0.004506143741309643, linear weight tensor([[ 1.7529]])\n",
      "epoch 367, loss 0.004496365319937468, linear weight tensor([[ 1.7532]])\n",
      "epoch 368, loss 0.004486608784645796, linear weight tensor([[ 1.7535]])\n",
      "epoch 369, loss 0.004476872272789478, linear weight tensor([[ 1.7537]])\n",
      "epoch 370, loss 0.004467160906642675, linear weight tensor([[ 1.7540]])\n",
      "epoch 371, loss 0.004457464441657066, linear weight tensor([[ 1.7543]])\n",
      "epoch 372, loss 0.00444779172539711, linear weight tensor([[ 1.7545]])\n",
      "epoch 373, loss 0.004438137169927359, linear weight tensor([[ 1.7548]])\n",
      "epoch 374, loss 0.0044285086914896965, linear weight tensor([[ 1.7551]])\n",
      "epoch 375, loss 0.004418895114213228, linear weight tensor([[ 1.7553]])\n",
      "epoch 376, loss 0.004409302957355976, linear weight tensor([[ 1.7556]])\n",
      "epoch 377, loss 0.0043997373431921005, linear weight tensor([[ 1.7559]])\n",
      "epoch 378, loss 0.004390191286802292, linear weight tensor([[ 1.7561]])\n",
      "epoch 379, loss 0.0043806638568639755, linear weight tensor([[ 1.7564]])\n",
      "epoch 380, loss 0.004371158313006163, linear weight tensor([[ 1.7567]])\n",
      "epoch 381, loss 0.004361671395599842, linear weight tensor([[ 1.7569]])\n",
      "epoch 382, loss 0.004352210089564323, linear weight tensor([[ 1.7572]])\n",
      "epoch 383, loss 0.004342763219028711, linear weight tensor([[ 1.7575]])\n",
      "epoch 384, loss 0.004333341494202614, linear weight tensor([[ 1.7577]])\n",
      "epoch 385, loss 0.004323939792811871, linear weight tensor([[ 1.7580]])\n",
      "epoch 386, loss 0.004314554389566183, linear weight tensor([[ 1.7583]])\n",
      "epoch 387, loss 0.00430518900975585, linear weight tensor([[ 1.7585]])\n",
      "epoch 388, loss 0.004295843653380871, linear weight tensor([[ 1.7588]])\n",
      "epoch 389, loss 0.004286525771021843, linear weight tensor([[ 1.7590]])\n",
      "epoch 390, loss 0.004277224652469158, linear weight tensor([[ 1.7593]])\n",
      "epoch 391, loss 0.004267947748303413, linear weight tensor([[ 1.7596]])\n",
      "epoch 392, loss 0.00425868621096015, linear weight tensor([[ 1.7598]])\n",
      "epoch 393, loss 0.0042494419030845165, linear weight tensor([[ 1.7601]])\n",
      "epoch 394, loss 0.004240220878273249, linear weight tensor([[ 1.7603]])\n",
      "epoch 395, loss 0.004231019411236048, linear weight tensor([[ 1.7606]])\n",
      "epoch 396, loss 0.004221836104989052, linear weight tensor([[ 1.7609]])\n",
      "epoch 397, loss 0.0042126779444515705, linear weight tensor([[ 1.7611]])\n",
      "epoch 398, loss 0.00420353515073657, linear weight tensor([[ 1.7614]])\n",
      "epoch 399, loss 0.0041944123804569244, linear weight tensor([[ 1.7616]])\n",
      "epoch 400, loss 0.004185308702290058, linear weight tensor([[ 1.7619]])\n",
      "epoch 401, loss 0.004176227375864983, linear weight tensor([[ 1.7622]])\n",
      "epoch 402, loss 0.004167163744568825, linear weight tensor([[ 1.7624]])\n",
      "epoch 403, loss 0.004158126190304756, linear weight tensor([[ 1.7627]])\n",
      "epoch 404, loss 0.004149101208895445, linear weight tensor([[ 1.7629]])\n",
      "epoch 405, loss 0.004140099510550499, linear weight tensor([[ 1.7632]])\n",
      "epoch 406, loss 0.004131113179028034, linear weight tensor([[ 1.7635]])\n",
      "epoch 407, loss 0.00412214919924736, linear weight tensor([[ 1.7637]])\n",
      "epoch 408, loss 0.004113203380256891, linear weight tensor([[ 1.7640]])\n",
      "epoch 409, loss 0.004104278516024351, linear weight tensor([[ 1.7642]])\n",
      "epoch 410, loss 0.004095373209565878, linear weight tensor([[ 1.7645]])\n",
      "epoch 411, loss 0.004086485598236322, linear weight tensor([[ 1.7647]])\n",
      "epoch 412, loss 0.004077616613358259, linear weight tensor([[ 1.7650]])\n",
      "epoch 413, loss 0.004068769980221987, linear weight tensor([[ 1.7653]])\n",
      "epoch 414, loss 0.004059939179569483, linear weight tensor([[ 1.7655]])\n",
      "epoch 415, loss 0.004051127005368471, linear weight tensor([[ 1.7658]])\n",
      "epoch 416, loss 0.004042339511215687, linear weight tensor([[ 1.7660]])\n",
      "epoch 417, loss 0.004033569246530533, linear weight tensor([[ 1.7663]])\n",
      "epoch 418, loss 0.004024812486022711, linear weight tensor([[ 1.7665]])\n",
      "epoch 419, loss 0.004016081802546978, linear weight tensor([[ 1.7668]])\n",
      "epoch 420, loss 0.004007367417216301, linear weight tensor([[ 1.7670]])\n",
      "epoch 421, loss 0.003998673055320978, linear weight tensor([[ 1.7673]])\n",
      "epoch 422, loss 0.003989997319877148, linear weight tensor([[ 1.7675]])\n",
      "epoch 423, loss 0.00398133834823966, linear weight tensor([[ 1.7678]])\n",
      "epoch 424, loss 0.00397269893437624, linear weight tensor([[ 1.7680]])\n",
      "epoch 425, loss 0.003964077681303024, linear weight tensor([[ 1.7683]])\n",
      "epoch 426, loss 0.0039554741233587265, linear weight tensor([[ 1.7685]])\n",
      "epoch 427, loss 0.003946890588849783, linear weight tensor([[ 1.7688]])\n",
      "epoch 428, loss 0.003938326612114906, linear weight tensor([[ 1.7691]])\n",
      "epoch 429, loss 0.003929778467863798, linear weight tensor([[ 1.7693]])\n",
      "epoch 430, loss 0.003921255003660917, linear weight tensor([[ 1.7696]])\n",
      "epoch 431, loss 0.003912741784006357, linear weight tensor([[ 1.7698]])\n",
      "epoch 432, loss 0.003904251381754875, linear weight tensor([[ 1.7701]])\n",
      "epoch 433, loss 0.003895782632753253, linear weight tensor([[ 1.7703]])\n",
      "epoch 434, loss 0.0038873241282999516, linear weight tensor([[ 1.7706]])\n",
      "epoch 435, loss 0.003878889139741659, linear weight tensor([[ 1.7708]])\n",
      "epoch 436, loss 0.003870474174618721, linear weight tensor([[ 1.7711]])\n",
      "epoch 437, loss 0.0038620762061327696, linear weight tensor([[ 1.7713]])\n",
      "epoch 438, loss 0.0038536908105015755, linear weight tensor([[ 1.7715]])\n",
      "epoch 439, loss 0.003845332656055689, linear weight tensor([[ 1.7718]])\n",
      "epoch 440, loss 0.003836988238617778, linear weight tensor([[ 1.7720]])\n",
      "epoch 441, loss 0.003828659886494279, linear weight tensor([[ 1.7723]])\n",
      "epoch 442, loss 0.003820351092144847, linear weight tensor([[ 1.7725]])\n",
      "epoch 443, loss 0.0038120630197227, linear weight tensor([[ 1.7728]])\n",
      "epoch 444, loss 0.003803789848461747, linear weight tensor([[ 1.7730]])\n",
      "epoch 445, loss 0.003795536234974861, linear weight tensor([[ 1.7733]])\n",
      "epoch 446, loss 0.0037873017136007547, linear weight tensor([[ 1.7735]])\n",
      "epoch 447, loss 0.0037790825590491295, linear weight tensor([[ 1.7738]])\n",
      "epoch 448, loss 0.003770883660763502, linear weight tensor([[ 1.7740]])\n",
      "epoch 449, loss 0.003762700827792287, linear weight tensor([[ 1.7743]])\n",
      "epoch 450, loss 0.003754537785425782, linear weight tensor([[ 1.7745]])\n",
      "epoch 451, loss 0.0037463889457285404, linear weight tensor([[ 1.7748]])\n",
      "epoch 452, loss 0.0037382584996521473, linear weight tensor([[ 1.7750]])\n",
      "epoch 453, loss 0.0037301473785191774, linear weight tensor([[ 1.7752]])\n",
      "epoch 454, loss 0.0037220539525151253, linear weight tensor([[ 1.7755]])\n",
      "epoch 455, loss 0.0037139770574867725, linear weight tensor([[ 1.7757]])\n",
      "epoch 456, loss 0.0037059152964502573, linear weight tensor([[ 1.7760]])\n",
      "epoch 457, loss 0.0036978754214942455, linear weight tensor([[ 1.7762]])\n",
      "epoch 458, loss 0.0036898525431752205, linear weight tensor([[ 1.7765]])\n",
      "epoch 459, loss 0.0036818450316786766, linear weight tensor([[ 1.7767]])\n",
      "epoch 460, loss 0.003673853585496545, linear weight tensor([[ 1.7769]])\n",
      "epoch 461, loss 0.0036658828612416983, linear weight tensor([[ 1.7772]])\n",
      "epoch 462, loss 0.003657926805317402, linear weight tensor([[ 1.7774]])\n",
      "epoch 463, loss 0.003649989143013954, linear weight tensor([[ 1.7777]])\n",
      "epoch 464, loss 0.0036420682445168495, linear weight tensor([[ 1.7779]])\n",
      "epoch 465, loss 0.003634166903793812, linear weight tensor([[ 1.7782]])\n",
      "epoch 466, loss 0.003626283025369048, linear weight tensor([[ 1.7784]])\n",
      "epoch 467, loss 0.003618411486968398, linear weight tensor([[ 1.7786]])\n",
      "epoch 468, loss 0.0036105576436966658, linear weight tensor([[ 1.7789]])\n",
      "epoch 469, loss 0.0036027238238602877, linear weight tensor([[ 1.7791]])\n",
      "epoch 470, loss 0.0035949083976447582, linear weight tensor([[ 1.7794]])\n",
      "epoch 471, loss 0.0035871057771146297, linear weight tensor([[ 1.7796]])\n",
      "epoch 472, loss 0.0035793206188827753, linear weight tensor([[ 1.7798]])\n",
      "epoch 473, loss 0.003571554785594344, linear weight tensor([[ 1.7801]])\n",
      "epoch 474, loss 0.0035638008266687393, linear weight tensor([[ 1.7803]])\n",
      "epoch 475, loss 0.003556071314960718, linear weight tensor([[ 1.7806]])\n",
      "epoch 476, loss 0.0035483550745993853, linear weight tensor([[ 1.7808]])\n",
      "epoch 477, loss 0.0035406555980443954, linear weight tensor([[ 1.7810]])\n",
      "epoch 478, loss 0.003532970789819956, linear weight tensor([[ 1.7813]])\n",
      "epoch 479, loss 0.0035253027454018593, linear weight tensor([[ 1.7815]])\n",
      "epoch 480, loss 0.0035176544915884733, linear weight tensor([[ 1.7817]])\n",
      "epoch 481, loss 0.0035100234672427177, linear weight tensor([[ 1.7820]])\n",
      "epoch 482, loss 0.0035024043172597885, linear weight tensor([[ 1.7822]])\n",
      "epoch 483, loss 0.0034948058892041445, linear weight tensor([[ 1.7825]])\n",
      "epoch 484, loss 0.003487220499664545, linear weight tensor([[ 1.7827]])\n",
      "epoch 485, loss 0.003479652339592576, linear weight tensor([[ 1.7829]])\n",
      "epoch 486, loss 0.003472104435786605, linear weight tensor([[ 1.7832]])\n",
      "epoch 487, loss 0.003464571898803115, linear weight tensor([[ 1.7834]])\n",
      "epoch 488, loss 0.0034570510033518076, linear weight tensor([[ 1.7836]])\n",
      "epoch 489, loss 0.003449549898505211, linear weight tensor([[ 1.7839]])\n",
      "epoch 490, loss 0.0034420660231262445, linear weight tensor([[ 1.7841]])\n",
      "epoch 491, loss 0.003434594254940748, linear weight tensor([[ 1.7843]])\n",
      "epoch 492, loss 0.003427142510190606, linear weight tensor([[ 1.7846]])\n",
      "epoch 493, loss 0.0034197065979242325, linear weight tensor([[ 1.7848]])\n",
      "epoch 494, loss 0.003412288147956133, linear weight tensor([[ 1.7850]])\n",
      "epoch 495, loss 0.0034048825036734343, linear weight tensor([[ 1.7853]])\n",
      "epoch 496, loss 0.003397494787350297, linear weight tensor([[ 1.7855]])\n",
      "epoch 497, loss 0.0033901173155754805, linear weight tensor([[ 1.7857]])\n",
      "epoch 498, loss 0.003382764756679535, linear weight tensor([[ 1.7860]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 499, loss 0.003375425934791565, linear weight tensor([[ 1.7862]])\n",
      "epoch 500, loss 0.0033681029453873634, linear weight tensor([[ 1.7864]])\n",
      "epoch 501, loss 0.0033607936929911375, linear weight tensor([[ 1.7867]])\n",
      "epoch 502, loss 0.003353501670062542, linear weight tensor([[ 1.7869]])\n",
      "epoch 503, loss 0.003346224781125784, linear weight tensor([[ 1.7871]])\n",
      "epoch 504, loss 0.0033389620948582888, linear weight tensor([[ 1.7874]])\n",
      "epoch 505, loss 0.003331716638058424, linear weight tensor([[ 1.7876]])\n",
      "epoch 506, loss 0.0033244870137423277, linear weight tensor([[ 1.7878]])\n",
      "epoch 507, loss 0.0033172720577567816, linear weight tensor([[ 1.7880]])\n",
      "epoch 508, loss 0.0033100745640695095, linear weight tensor([[ 1.7883]])\n",
      "epoch 509, loss 0.0033028912730515003, linear weight tensor([[ 1.7885]])\n",
      "epoch 510, loss 0.0032957252115011215, linear weight tensor([[ 1.7887]])\n",
      "epoch 511, loss 0.0032885754480957985, linear weight tensor([[ 1.7890]])\n",
      "epoch 512, loss 0.0032814382575452328, linear weight tensor([[ 1.7892]])\n",
      "epoch 513, loss 0.0032743182964622974, linear weight tensor([[ 1.7894]])\n",
      "epoch 514, loss 0.0032672134693711996, linear weight tensor([[ 1.7897]])\n",
      "epoch 515, loss 0.0032601230777800083, linear weight tensor([[ 1.7899]])\n",
      "epoch 516, loss 0.003253048751503229, linear weight tensor([[ 1.7901]])\n",
      "epoch 517, loss 0.0032459867652505636, linear weight tensor([[ 1.7903]])\n",
      "epoch 518, loss 0.0032389431726187468, linear weight tensor([[ 1.7906]])\n",
      "epoch 519, loss 0.0032319175079464912, linear weight tensor([[ 1.7908]])\n",
      "epoch 520, loss 0.0032249062787741423, linear weight tensor([[ 1.7910]])\n",
      "epoch 521, loss 0.003217908088117838, linear weight tensor([[ 1.7912]])\n",
      "epoch 522, loss 0.0032109247986227274, linear weight tensor([[ 1.7915]])\n",
      "epoch 523, loss 0.003203958971425891, linear weight tensor([[ 1.7917]])\n",
      "epoch 524, loss 0.00319700688123703, linear weight tensor([[ 1.7919]])\n",
      "epoch 525, loss 0.003190070390701294, linear weight tensor([[ 1.7922]])\n",
      "epoch 526, loss 0.0031831476371735334, linear weight tensor([[ 1.7924]])\n",
      "epoch 527, loss 0.0031762386206537485, linear weight tensor([[ 1.7926]])\n",
      "epoch 528, loss 0.003169347532093525, linear weight tensor([[ 1.7928]])\n",
      "epoch 529, loss 0.0031624692492187023, linear weight tensor([[ 1.7931]])\n",
      "epoch 530, loss 0.0031556051690131426, linear weight tensor([[ 1.7933]])\n",
      "epoch 531, loss 0.003148755058646202, linear weight tensor([[ 1.7935]])\n",
      "epoch 532, loss 0.0031419266015291214, linear weight tensor([[ 1.7937]])\n",
      "epoch 533, loss 0.0031351044308394194, linear weight tensor([[ 1.7940]])\n",
      "epoch 534, loss 0.003128303913399577, linear weight tensor([[ 1.7942]])\n",
      "epoch 535, loss 0.003121515503153205, linear weight tensor([[ 1.7944]])\n",
      "epoch 536, loss 0.0031147426925599575, linear weight tensor([[ 1.7946]])\n",
      "epoch 537, loss 0.003107985481619835, linear weight tensor([[ 1.7948]])\n",
      "epoch 538, loss 0.0031012417748570442, linear weight tensor([[ 1.7951]])\n",
      "epoch 539, loss 0.003094513202086091, linear weight tensor([[ 1.7953]])\n",
      "epoch 540, loss 0.003087799297645688, linear weight tensor([[ 1.7955]])\n",
      "epoch 541, loss 0.003081100294366479, linear weight tensor([[ 1.7957]])\n",
      "epoch 542, loss 0.0030744164250791073, linear weight tensor([[ 1.7960]])\n",
      "epoch 543, loss 0.0030677455943077803, linear weight tensor([[ 1.7962]])\n",
      "epoch 544, loss 0.003061088966205716, linear weight tensor([[ 1.7964]])\n",
      "epoch 545, loss 0.003054446540772915, linear weight tensor([[ 1.7966]])\n",
      "epoch 546, loss 0.003047819482162595, linear weight tensor([[ 1.7968]])\n",
      "epoch 547, loss 0.003041204996407032, linear weight tensor([[ 1.7971]])\n",
      "epoch 548, loss 0.003034607507288456, linear weight tensor([[ 1.7973]])\n",
      "epoch 549, loss 0.003028023522347212, linear weight tensor([[ 1.7975]])\n",
      "epoch 550, loss 0.0030214539729058743, linear weight tensor([[ 1.7977]])\n",
      "epoch 551, loss 0.0030148967634886503, linear weight tensor([[ 1.7979]])\n",
      "epoch 552, loss 0.003008355852216482, linear weight tensor([[ 1.7982]])\n",
      "epoch 553, loss 0.003001829609274864, linear weight tensor([[ 1.7984]])\n",
      "epoch 554, loss 0.0029953152406960726, linear weight tensor([[ 1.7986]])\n",
      "epoch 555, loss 0.002988816238939762, linear weight tensor([[ 1.7988]])\n",
      "epoch 556, loss 0.00298233050853014, linear weight tensor([[ 1.7990]])\n",
      "epoch 557, loss 0.0029758575838059187, linear weight tensor([[ 1.7993]])\n",
      "epoch 558, loss 0.0029694032855331898, linear weight tensor([[ 1.7995]])\n",
      "epoch 559, loss 0.002962959697470069, linear weight tensor([[ 1.7997]])\n",
      "epoch 560, loss 0.0029565307777374983, linear weight tensor([[ 1.7999]])\n",
      "epoch 561, loss 0.0029501125682145357, linear weight tensor([[ 1.8001]])\n",
      "epoch 562, loss 0.002943712752312422, linear weight tensor([[ 1.8003]])\n",
      "epoch 563, loss 0.0029373271390795708, linear weight tensor([[ 1.8006]])\n",
      "epoch 564, loss 0.0029309531673789024, linear weight tensor([[ 1.8008]])\n",
      "epoch 565, loss 0.002924594795331359, linear weight tensor([[ 1.8010]])\n",
      "epoch 566, loss 0.002918248064815998, linear weight tensor([[ 1.8012]])\n",
      "epoch 567, loss 0.002911916933953762, linear weight tensor([[ 1.8014]])\n",
      "epoch 568, loss 0.0029055983759462833, linear weight tensor([[ 1.8016]])\n",
      "epoch 569, loss 0.002899295650422573, linear weight tensor([[ 1.8019]])\n",
      "epoch 570, loss 0.00289300549775362, linear weight tensor([[ 1.8021]])\n",
      "epoch 571, loss 0.0028867297805845737, linear weight tensor([[ 1.8023]])\n",
      "epoch 572, loss 0.002880462910979986, linear weight tensor([[ 1.8025]])\n",
      "epoch 573, loss 0.0028742116410285234, linear weight tensor([[ 1.8027]])\n",
      "epoch 574, loss 0.0028679741080850363, linear weight tensor([[ 1.8029]])\n",
      "epoch 575, loss 0.0028617503121495247, linear weight tensor([[ 1.8031]])\n",
      "epoch 576, loss 0.002855541417375207, linear weight tensor([[ 1.8034]])\n",
      "epoch 577, loss 0.0028493443969637156, linear weight tensor([[ 1.8036]])\n",
      "epoch 578, loss 0.0028431592509150505, linear weight tensor([[ 1.8038]])\n",
      "epoch 579, loss 0.0028369887731969357, linear weight tensor([[ 1.8040]])\n",
      "epoch 580, loss 0.0028308317996561527, linear weight tensor([[ 1.8042]])\n",
      "epoch 581, loss 0.0028246897272765636, linear weight tensor([[ 1.8044]])\n",
      "epoch 582, loss 0.002818560227751732, linear weight tensor([[ 1.8046]])\n",
      "epoch 583, loss 0.0028124437667429447, linear weight tensor([[ 1.8048]])\n",
      "epoch 584, loss 0.002806341042742133, linear weight tensor([[ 1.8051]])\n",
      "epoch 585, loss 0.0028002525214105844, linear weight tensor([[ 1.8053]])\n",
      "epoch 586, loss 0.0027941782027482986, linear weight tensor([[ 1.8055]])\n",
      "epoch 587, loss 0.0027881162241101265, linear weight tensor([[ 1.8057]])\n",
      "epoch 588, loss 0.002782065887004137, linear weight tensor([[ 1.8059]])\n",
      "epoch 589, loss 0.002776032080873847, linear weight tensor([[ 1.8061]])\n",
      "epoch 590, loss 0.0027700052596628666, linear weight tensor([[ 1.8063]])\n",
      "epoch 591, loss 0.0027639928739517927, linear weight tensor([[ 1.8065]])\n",
      "epoch 592, loss 0.0027579942252486944, linear weight tensor([[ 1.8067]])\n",
      "epoch 593, loss 0.0027520102448761463, linear weight tensor([[ 1.8070]])\n",
      "epoch 594, loss 0.0027460369747132063, linear weight tensor([[ 1.8072]])\n",
      "epoch 595, loss 0.0027400769758969545, linear weight tensor([[ 1.8074]])\n",
      "epoch 596, loss 0.0027341339737176895, linear weight tensor([[ 1.8076]])\n",
      "epoch 597, loss 0.002728200750425458, linear weight tensor([[ 1.8078]])\n",
      "epoch 598, loss 0.0027222824282944202, linear weight tensor([[ 1.8080]])\n",
      "epoch 599, loss 0.0027163743507117033, linear weight tensor([[ 1.8082]])\n",
      "epoch 600, loss 0.002710482804104686, linear weight tensor([[ 1.8084]])\n",
      "epoch 601, loss 0.0027045991737395525, linear weight tensor([[ 1.8086]])\n",
      "epoch 602, loss 0.00269872834905982, linear weight tensor([[ 1.8088]])\n",
      "epoch 603, loss 0.002692872192710638, linear weight tensor([[ 1.8090]])\n",
      "epoch 604, loss 0.002687030239030719, linear weight tensor([[ 1.8092]])\n",
      "epoch 605, loss 0.0026811985298991203, linear weight tensor([[ 1.8095]])\n",
      "epoch 606, loss 0.00267538009211421, linear weight tensor([[ 1.8097]])\n",
      "epoch 607, loss 0.002669575158506632, linear weight tensor([[ 1.8099]])\n",
      "epoch 608, loss 0.0026637855917215347, linear weight tensor([[ 1.8101]])\n",
      "epoch 609, loss 0.0026580081321299076, linear weight tensor([[ 1.8103]])\n",
      "epoch 610, loss 0.002652236260473728, linear weight tensor([[ 1.8105]])\n",
      "epoch 611, loss 0.00264648231677711, linear weight tensor([[ 1.8107]])\n",
      "epoch 612, loss 0.0026407388504594564, linear weight tensor([[ 1.8109]])\n",
      "epoch 613, loss 0.002635006560012698, linear weight tensor([[ 1.8111]])\n",
      "epoch 614, loss 0.0026292887050658464, linear weight tensor([[ 1.8113]])\n",
      "epoch 615, loss 0.002623585518449545, linear weight tensor([[ 1.8115]])\n",
      "epoch 616, loss 0.002617891412228346, linear weight tensor([[ 1.8117]])\n",
      "epoch 617, loss 0.002612212672829628, linear weight tensor([[ 1.8119]])\n",
      "epoch 618, loss 0.002606545342132449, linear weight tensor([[ 1.8121]])\n",
      "epoch 619, loss 0.002600888255983591, linear weight tensor([[ 1.8123]])\n",
      "epoch 620, loss 0.002595242578536272, linear weight tensor([[ 1.8125]])\n",
      "epoch 621, loss 0.002589612267911434, linear weight tensor([[ 1.8127]])\n",
      "epoch 622, loss 0.00258399429731071, linear weight tensor([[ 1.8129]])\n",
      "epoch 623, loss 0.0025783858727663755, linear weight tensor([[ 1.8131]])\n",
      "epoch 624, loss 0.002572793746367097, linear weight tensor([[ 1.8133]])\n",
      "epoch 625, loss 0.0025672069750726223, linear weight tensor([[ 1.8135]])\n",
      "epoch 626, loss 0.0025616385973989964, linear weight tensor([[ 1.8137]])\n",
      "epoch 627, loss 0.002556079300120473, linear weight tensor([[ 1.8139]])\n",
      "epoch 628, loss 0.0025505335070192814, linear weight tensor([[ 1.8142]])\n",
      "epoch 629, loss 0.002545000286772847, linear weight tensor([[ 1.8144]])\n",
      "epoch 630, loss 0.0025394780095666647, linear weight tensor([[ 1.8146]])\n",
      "epoch 631, loss 0.0025339669082313776, linear weight tensor([[ 1.8148]])\n",
      "epoch 632, loss 0.0025284665171056986, linear weight tensor([[ 1.8150]])\n",
      "epoch 633, loss 0.002522979164496064, linear weight tensor([[ 1.8152]])\n",
      "epoch 634, loss 0.0025175034534186125, linear weight tensor([[ 1.8154]])\n",
      "epoch 635, loss 0.0025120412465184927, linear weight tensor([[ 1.8156]])\n",
      "epoch 636, loss 0.0025065934751182795, linear weight tensor([[ 1.8158]])\n",
      "epoch 637, loss 0.002501154551282525, linear weight tensor([[ 1.8160]])\n",
      "epoch 638, loss 0.002495724940672517, linear weight tensor([[ 1.8162]])\n",
      "epoch 639, loss 0.002490309067070484, linear weight tensor([[ 1.8164]])\n",
      "epoch 640, loss 0.002484905067831278, linear weight tensor([[ 1.8166]])\n",
      "epoch 641, loss 0.0024795145727694035, linear weight tensor([[ 1.8168]])\n",
      "epoch 642, loss 0.0024741352535784245, linear weight tensor([[ 1.8170]])\n",
      "epoch 643, loss 0.0024687652476131916, linear weight tensor([[ 1.8172]])\n",
      "epoch 644, loss 0.0024634068831801414, linear weight tensor([[ 1.8174]])\n",
      "epoch 645, loss 0.0024580624885857105, linear weight tensor([[ 1.8176]])\n",
      "epoch 646, loss 0.0024527297355234623, linear weight tensor([[ 1.8177]])\n",
      "epoch 647, loss 0.002447406994178891, linear weight tensor([[ 1.8179]])\n",
      "epoch 648, loss 0.0024420982226729393, linear weight tensor([[ 1.8181]])\n",
      "epoch 649, loss 0.0024367973674088717, linear weight tensor([[ 1.8183]])\n",
      "epoch 650, loss 0.0024315102491527796, linear weight tensor([[ 1.8185]])\n",
      "epoch 651, loss 0.002426235005259514, linear weight tensor([[ 1.8187]])\n",
      "epoch 652, loss 0.002420971868559718, linear weight tensor([[ 1.8189]])\n",
      "epoch 653, loss 0.0024157147854566574, linear weight tensor([[ 1.8191]])\n",
      "epoch 654, loss 0.002410474931821227, linear weight tensor([[ 1.8193]])\n",
      "epoch 655, loss 0.002405245788395405, linear weight tensor([[ 1.8195]])\n",
      "epoch 656, loss 0.0024000247940421104, linear weight tensor([[ 1.8197]])\n",
      "epoch 657, loss 0.0023948184680193663, linear weight tensor([[ 1.8199]])\n",
      "epoch 658, loss 0.0023896212223917246, linear weight tensor([[ 1.8201]])\n",
      "epoch 659, loss 0.0023844351526349783, linear weight tensor([[ 1.8203]])\n",
      "epoch 660, loss 0.00237925979308784, linear weight tensor([[ 1.8205]])\n",
      "epoch 661, loss 0.0023740995675325394, linear weight tensor([[ 1.8207]])\n",
      "epoch 662, loss 0.002368948655202985, linear weight tensor([[ 1.8209]])\n",
      "epoch 663, loss 0.002363808685913682, linear weight tensor([[ 1.8211]])\n",
      "epoch 664, loss 0.0023586789611727, linear weight tensor([[ 1.8213]])\n",
      "epoch 665, loss 0.0023535608779639006, linear weight tensor([[ 1.8215]])\n",
      "epoch 666, loss 0.002348455134779215, linear weight tensor([[ 1.8217]])\n",
      "epoch 667, loss 0.0023433594033122063, linear weight tensor([[ 1.8219]])\n",
      "epoch 668, loss 0.0023382746148854494, linear weight tensor([[ 1.8221]])\n",
      "epoch 669, loss 0.002333197509869933, linear weight tensor([[ 1.8222]])\n",
      "epoch 670, loss 0.0023281401954591274, linear weight tensor([[ 1.8224]])\n",
      "epoch 671, loss 0.0023230849765241146, linear weight tensor([[ 1.8226]])\n",
      "epoch 672, loss 0.002318045124411583, linear weight tensor([[ 1.8228]])\n",
      "epoch 673, loss 0.002313014352694154, linear weight tensor([[ 1.8230]])\n",
      "epoch 674, loss 0.0023079966194927692, linear weight tensor([[ 1.8232]])\n",
      "epoch 675, loss 0.002302985405549407, linear weight tensor([[ 1.8234]])\n",
      "epoch 676, loss 0.0022979893255978823, linear weight tensor([[ 1.8236]])\n",
      "epoch 677, loss 0.0022930018603801727, linear weight tensor([[ 1.8238]])\n",
      "epoch 678, loss 0.0022880281321704388, linear weight tensor([[ 1.8240]])\n",
      "epoch 679, loss 0.002283063717186451, linear weight tensor([[ 1.8242]])\n",
      "epoch 680, loss 0.0022781093139201403, linear weight tensor([[ 1.8244]])\n",
      "epoch 681, loss 0.0022731649223715067, linear weight tensor([[ 1.8245]])\n",
      "epoch 682, loss 0.002268236130475998, linear weight tensor([[ 1.8247]])\n",
      "epoch 683, loss 0.002263313625007868, linear weight tensor([[ 1.8249]])\n",
      "epoch 684, loss 0.0022584025282412767, linear weight tensor([[ 1.8251]])\n",
      "epoch 685, loss 0.0022535023745149374, linear weight tensor([[ 1.8253]])\n",
      "epoch 686, loss 0.002248612930998206, linear weight tensor([[ 1.8255]])\n",
      "epoch 687, loss 0.0022437325678765774, linear weight tensor([[ 1.8257]])\n",
      "epoch 688, loss 0.0022388650104403496, linear weight tensor([[ 1.8259]])\n",
      "epoch 689, loss 0.0022340069990605116, linear weight tensor([[ 1.8261]])\n",
      "epoch 690, loss 0.0022291589993983507, linear weight tensor([[ 1.8263]])\n",
      "epoch 691, loss 0.002224321011453867, linear weight tensor([[ 1.8264]])\n",
      "epoch 692, loss 0.002219493966549635, linear weight tensor([[ 1.8266]])\n",
      "epoch 693, loss 0.002214680425822735, linear weight tensor([[ 1.8268]])\n",
      "epoch 694, loss 0.0022098729386925697, linear weight tensor([[ 1.8270]])\n",
      "epoch 695, loss 0.0022050810512155294, linear weight tensor([[ 1.8272]])\n",
      "epoch 696, loss 0.0022002949845045805, linear weight tensor([[ 1.8274]])\n",
      "epoch 697, loss 0.0021955177653580904, linear weight tensor([[ 1.8276]])\n",
      "epoch 698, loss 0.0021907552145421505, linear weight tensor([[ 1.8278]])\n",
      "epoch 699, loss 0.0021859994158148766, linear weight tensor([[ 1.8279]])\n",
      "epoch 700, loss 0.002181258285418153, linear weight tensor([[ 1.8281]])\n",
      "epoch 701, loss 0.0021765248384326696, linear weight tensor([[ 1.8283]])\n",
      "epoch 702, loss 0.002171801868826151, linear weight tensor([[ 1.8285]])\n",
      "epoch 703, loss 0.002167089842259884, linear weight tensor([[ 1.8287]])\n",
      "epoch 704, loss 0.002162387128919363, linear weight tensor([[ 1.8289]])\n",
      "epoch 705, loss 0.0021576948929578066, linear weight tensor([[ 1.8291]])\n",
      "epoch 706, loss 0.002153011504560709, linear weight tensor([[ 1.8292]])\n",
      "epoch 707, loss 0.002148338360711932, linear weight tensor([[ 1.8294]])\n",
      "epoch 708, loss 0.0021436773240566254, linear weight tensor([[ 1.8296]])\n",
      "epoch 709, loss 0.002139025367796421, linear weight tensor([[ 1.8298]])\n",
      "epoch 710, loss 0.00213438319042325, linear weight tensor([[ 1.8300]])\n",
      "epoch 711, loss 0.002129749860614538, linear weight tensor([[ 1.8302]])\n",
      "epoch 712, loss 0.002125129336491227, linear weight tensor([[ 1.8304]])\n",
      "epoch 713, loss 0.0021205164957791567, linear weight tensor([[ 1.8305]])\n",
      "epoch 714, loss 0.002115913899615407, linear weight tensor([[ 1.8307]])\n",
      "epoch 715, loss 0.002111322246491909, linear weight tensor([[ 1.8309]])\n",
      "epoch 716, loss 0.00210674200206995, linear weight tensor([[ 1.8311]])\n",
      "epoch 717, loss 0.0021021694410592318, linear weight tensor([[ 1.8313]])\n",
      "epoch 718, loss 0.0020976075902581215, linear weight tensor([[ 1.8315]])\n",
      "epoch 719, loss 0.0020930571481585503, linear weight tensor([[ 1.8316]])\n",
      "epoch 720, loss 0.0020885143894702196, linear weight tensor([[ 1.8318]])\n",
      "epoch 721, loss 0.0020839814096689224, linear weight tensor([[ 1.8320]])\n",
      "epoch 722, loss 0.002079457975924015, linear weight tensor([[ 1.8322]])\n",
      "epoch 723, loss 0.0020749433897435665, linear weight tensor([[ 1.8324]])\n",
      "epoch 724, loss 0.0020704418420791626, linear weight tensor([[ 1.8326]])\n",
      "epoch 725, loss 0.0020659479778259993, linear weight tensor([[ 1.8327]])\n",
      "epoch 726, loss 0.0020614657551050186, linear weight tensor([[ 1.8329]])\n",
      "epoch 727, loss 0.002056990982964635, linear weight tensor([[ 1.8331]])\n",
      "epoch 728, loss 0.0020525273866951466, linear weight tensor([[ 1.8333]])\n",
      "epoch 729, loss 0.0020480717066675425, linear weight tensor([[ 1.8335]])\n",
      "epoch 730, loss 0.0020436281338334084, linear weight tensor([[ 1.8336]])\n",
      "epoch 731, loss 0.0020391924772411585, linear weight tensor([[ 1.8338]])\n",
      "epoch 732, loss 0.002034766599535942, linear weight tensor([[ 1.8340]])\n",
      "epoch 733, loss 0.00203035119920969, linear weight tensor([[ 1.8342]])\n",
      "epoch 734, loss 0.002025943249464035, linear weight tensor([[ 1.8344]])\n",
      "epoch 735, loss 0.0020215483382344246, linear weight tensor([[ 1.8345]])\n",
      "epoch 736, loss 0.0020171618089079857, linear weight tensor([[ 1.8347]])\n",
      "epoch 737, loss 0.002012784592807293, linear weight tensor([[ 1.8349]])\n",
      "epoch 738, loss 0.0020084166899323463, linear weight tensor([[ 1.8351]])\n",
      "epoch 739, loss 0.0020040578674525023, linear weight tensor([[ 1.8353]])\n",
      "epoch 740, loss 0.001999707892537117, linear weight tensor([[ 1.8354]])\n",
      "epoch 741, loss 0.0019953688606619835, linear weight tensor([[ 1.8356]])\n",
      "epoch 742, loss 0.001991039840504527, linear weight tensor([[ 1.8358]])\n",
      "epoch 743, loss 0.0019867182709276676, linear weight tensor([[ 1.8360]])\n",
      "epoch 744, loss 0.001982408342882991, linear weight tensor([[ 1.8362]])\n",
      "epoch 745, loss 0.0019781053997576237, linear weight tensor([[ 1.8363]])\n",
      "epoch 746, loss 0.001973812934011221, linear weight tensor([[ 1.8365]])\n",
      "epoch 747, loss 0.0019695330411195755, linear weight tensor([[ 1.8367]])\n",
      "epoch 748, loss 0.0019652589689940214, linear weight tensor([[ 1.8369]])\n",
      "epoch 749, loss 0.0019609942100942135, linear weight tensor([[ 1.8370]])\n",
      "epoch 750, loss 0.0019567394629120827, linear weight tensor([[ 1.8372]])\n",
      "epoch 751, loss 0.0019524923991411924, linear weight tensor([[ 1.8374]])\n",
      "epoch 752, loss 0.0019482574425637722, linear weight tensor([[ 1.8376]])\n",
      "epoch 753, loss 0.001944029238075018, linear weight tensor([[ 1.8377]])\n",
      "epoch 754, loss 0.0019398111617192626, linear weight tensor([[ 1.8379]])\n",
      "epoch 755, loss 0.001935602747835219, linear weight tensor([[ 1.8381]])\n",
      "epoch 756, loss 0.0019314034143462777, linear weight tensor([[ 1.8383]])\n",
      "epoch 757, loss 0.001927210483700037, linear weight tensor([[ 1.8384]])\n",
      "epoch 758, loss 0.0019230287289246917, linear weight tensor([[ 1.8386]])\n",
      "epoch 759, loss 0.0019188555888831615, linear weight tensor([[ 1.8388]])\n",
      "epoch 760, loss 0.001914691529236734, linear weight tensor([[ 1.8390]])\n",
      "epoch 761, loss 0.0019105367828160524, linear weight tensor([[ 1.8392]])\n",
      "epoch 762, loss 0.0019063896033912897, linear weight tensor([[ 1.8393]])\n",
      "epoch 763, loss 0.001902252435684204, linear weight tensor([[ 1.8395]])\n",
      "epoch 764, loss 0.0018981233006343246, linear weight tensor([[ 1.8397]])\n",
      "epoch 765, loss 0.001894006971269846, linear weight tensor([[ 1.8398]])\n",
      "epoch 766, loss 0.001889894949272275, linear weight tensor([[ 1.8400]])\n",
      "epoch 767, loss 0.001885796431452036, linear weight tensor([[ 1.8402]])\n",
      "epoch 768, loss 0.001881703152321279, linear weight tensor([[ 1.8404]])\n",
      "epoch 769, loss 0.0018776196520775557, linear weight tensor([[ 1.8405]])\n",
      "epoch 770, loss 0.0018735462799668312, linear weight tensor([[ 1.8407]])\n",
      "epoch 771, loss 0.0018694805912673473, linear weight tensor([[ 1.8409]])\n",
      "epoch 772, loss 0.0018654230516403913, linear weight tensor([[ 1.8411]])\n",
      "epoch 773, loss 0.0018613759893923998, linear weight tensor([[ 1.8412]])\n",
      "epoch 774, loss 0.001857337192632258, linear weight tensor([[ 1.8414]])\n",
      "epoch 775, loss 0.0018533067777752876, linear weight tensor([[ 1.8416]])\n",
      "epoch 776, loss 0.001849287305958569, linear weight tensor([[ 1.8417]])\n",
      "epoch 777, loss 0.0018452705116942525, linear weight tensor([[ 1.8419]])\n",
      "epoch 778, loss 0.0018412676872685552, linear weight tensor([[ 1.8421]])\n",
      "epoch 779, loss 0.0018372716149315238, linear weight tensor([[ 1.8423]])\n",
      "epoch 780, loss 0.0018332840409129858, linear weight tensor([[ 1.8424]])\n",
      "epoch 781, loss 0.0018293061293661594, linear weight tensor([[ 1.8426]])\n",
      "epoch 782, loss 0.0018253357848152518, linear weight tensor([[ 1.8428]])\n",
      "epoch 783, loss 0.001821377081796527, linear weight tensor([[ 1.8429]])\n",
      "epoch 784, loss 0.001817423151805997, linear weight tensor([[ 1.8431]])\n",
      "epoch 785, loss 0.0018134786514565349, linear weight tensor([[ 1.8433]])\n",
      "epoch 786, loss 0.0018095445120707154, linear weight tensor([[ 1.8435]])\n",
      "epoch 787, loss 0.0018056173576042056, linear weight tensor([[ 1.8436]])\n",
      "epoch 788, loss 0.001801700098440051, linear weight tensor([[ 1.8438]])\n",
      "epoch 789, loss 0.001797790639102459, linear weight tensor([[ 1.8440]])\n",
      "epoch 790, loss 0.0017938907258212566, linear weight tensor([[ 1.8441]])\n",
      "epoch 791, loss 0.0017899982631206512, linear weight tensor([[ 1.8443]])\n",
      "epoch 792, loss 0.001786113134585321, linear weight tensor([[ 1.8445]])\n",
      "epoch 793, loss 0.0017822367371991277, linear weight tensor([[ 1.8446]])\n",
      "epoch 794, loss 0.0017783711664378643, linear weight tensor([[ 1.8448]])\n",
      "epoch 795, loss 0.0017745099030435085, linear weight tensor([[ 1.8450]])\n",
      "epoch 796, loss 0.0017706603975966573, linear weight tensor([[ 1.8452]])\n",
      "epoch 797, loss 0.001766818342730403, linear weight tensor([[ 1.8453]])\n",
      "epoch 798, loss 0.001762984087690711, linear weight tensor([[ 1.8455]])\n",
      "epoch 799, loss 0.0017591571668162942, linear weight tensor([[ 1.8457]])\n",
      "epoch 800, loss 0.00175534188747406, linear weight tensor([[ 1.8458]])\n",
      "epoch 801, loss 0.0017515324288979173, linear weight tensor([[ 1.8460]])\n",
      "epoch 802, loss 0.0017477317014709115, linear weight tensor([[ 1.8462]])\n",
      "epoch 803, loss 0.00174393889028579, linear weight tensor([[ 1.8463]])\n",
      "epoch 804, loss 0.0017401552759110928, linear weight tensor([[ 1.8465]])\n",
      "epoch 805, loss 0.0017363792285323143, linear weight tensor([[ 1.8467]])\n",
      "epoch 806, loss 0.001732612494379282, linear weight tensor([[ 1.8468]])\n",
      "epoch 807, loss 0.001728852279484272, linear weight tensor([[ 1.8470]])\n",
      "epoch 808, loss 0.0017251009121537209, linear weight tensor([[ 1.8472]])\n",
      "epoch 809, loss 0.001721358159556985, linear weight tensor([[ 1.8473]])\n",
      "epoch 810, loss 0.0017176223918795586, linear weight tensor([[ 1.8475]])\n",
      "epoch 811, loss 0.0017138965195044875, linear weight tensor([[ 1.8477]])\n",
      "epoch 812, loss 0.0017101773992180824, linear weight tensor([[ 1.8478]])\n",
      "epoch 813, loss 0.0017064656130969524, linear weight tensor([[ 1.8480]])\n",
      "epoch 814, loss 0.001702763489447534, linear weight tensor([[ 1.8481]])\n",
      "epoch 815, loss 0.0016990696312859654, linear weight tensor([[ 1.8483]])\n",
      "epoch 816, loss 0.00169538171030581, linear weight tensor([[ 1.8485]])\n",
      "epoch 817, loss 0.001691702171228826, linear weight tensor([[ 1.8486]])\n",
      "epoch 818, loss 0.0016880310140550137, linear weight tensor([[ 1.8488]])\n",
      "epoch 819, loss 0.0016843697521835566, linear weight tensor([[ 1.8490]])\n",
      "epoch 820, loss 0.0016807143110781908, linear weight tensor([[ 1.8491]])\n",
      "epoch 821, loss 0.001677068299613893, linear weight tensor([[ 1.8493]])\n",
      "epoch 822, loss 0.0016734303208068013, linear weight tensor([[ 1.8495]])\n",
      "epoch 823, loss 0.001669798162765801, linear weight tensor([[ 1.8496]])\n",
      "epoch 824, loss 0.0016661742702126503, linear weight tensor([[ 1.8498]])\n",
      "epoch 825, loss 0.0016625584103167057, linear weight tensor([[ 1.8500]])\n",
      "epoch 826, loss 0.001658952096477151, linear weight tensor([[ 1.8501]])\n",
      "epoch 827, loss 0.0016553530003875494, linear weight tensor([[ 1.8503]])\n",
      "epoch 828, loss 0.0016517608892172575, linear weight tensor([[ 1.8504]])\n",
      "epoch 829, loss 0.00164817797485739, linear weight tensor([[ 1.8506]])\n",
      "epoch 830, loss 0.0016446018125861883, linear weight tensor([[ 1.8508]])\n",
      "epoch 831, loss 0.0016410336829721928, linear weight tensor([[ 1.8509]])\n",
      "epoch 832, loss 0.0016374725382775068, linear weight tensor([[ 1.8511]])\n",
      "epoch 833, loss 0.0016339186113327742, linear weight tensor([[ 1.8512]])\n",
      "epoch 834, loss 0.0016303712036460638, linear weight tensor([[ 1.8514]])\n",
      "epoch 835, loss 0.0016268336912617087, linear weight tensor([[ 1.8516]])\n",
      "epoch 836, loss 0.0016233054921030998, linear weight tensor([[ 1.8517]])\n",
      "epoch 837, loss 0.001619781251065433, linear weight tensor([[ 1.8519]])\n",
      "epoch 838, loss 0.0016162667889147997, linear weight tensor([[ 1.8521]])\n",
      "epoch 839, loss 0.0016127581475302577, linear weight tensor([[ 1.8522]])\n",
      "epoch 840, loss 0.0016092603327706456, linear weight tensor([[ 1.8524]])\n",
      "epoch 841, loss 0.0016057668253779411, linear weight tensor([[ 1.8525]])\n",
      "epoch 842, loss 0.0016022828640416265, linear weight tensor([[ 1.8527]])\n",
      "epoch 843, loss 0.0015988058876246214, linear weight tensor([[ 1.8529]])\n",
      "epoch 844, loss 0.0015953368274495006, linear weight tensor([[ 1.8530]])\n",
      "epoch 845, loss 0.0015918761491775513, linear weight tensor([[ 1.8532]])\n",
      "epoch 846, loss 0.0015884204767644405, linear weight tensor([[ 1.8533]])\n",
      "epoch 847, loss 0.0015849735355004668, linear weight tensor([[ 1.8535]])\n",
      "epoch 848, loss 0.0015815338119864464, linear weight tensor([[ 1.8537]])\n",
      "epoch 849, loss 0.0015781022375449538, linear weight tensor([[ 1.8538]])\n",
      "epoch 850, loss 0.0015746760182082653, linear weight tensor([[ 1.8540]])\n",
      "epoch 851, loss 0.001571261091157794, linear weight tensor([[ 1.8541]])\n",
      "epoch 852, loss 0.001567850005812943, linear weight tensor([[ 1.8543]])\n",
      "epoch 853, loss 0.001564451027661562, linear weight tensor([[ 1.8544]])\n",
      "epoch 854, loss 0.0015610550763085485, linear weight tensor([[ 1.8546]])\n",
      "epoch 855, loss 0.001557667856104672, linear weight tensor([[ 1.8548]])\n",
      "epoch 856, loss 0.0015542864566668868, linear weight tensor([[ 1.8549]])\n",
      "epoch 857, loss 0.0015509133227169514, linear weight tensor([[ 1.8551]])\n",
      "epoch 858, loss 0.001547548221424222, linear weight tensor([[ 1.8552]])\n",
      "epoch 859, loss 0.001544190221466124, linear weight tensor([[ 1.8554]])\n",
      "epoch 860, loss 0.0015408413019031286, linear weight tensor([[ 1.8555]])\n",
      "epoch 861, loss 0.0015374961076304317, linear weight tensor([[ 1.8557]])\n",
      "epoch 862, loss 0.0015341605758294463, linear weight tensor([[ 1.8559]])\n",
      "epoch 863, loss 0.001530830399133265, linear weight tensor([[ 1.8560]])\n",
      "epoch 864, loss 0.0015275077894330025, linear weight tensor([[ 1.8562]])\n",
      "epoch 865, loss 0.0015241949586197734, linear weight tensor([[ 1.8563]])\n",
      "epoch 866, loss 0.0015208859695121646, linear weight tensor([[ 1.8565]])\n",
      "epoch 867, loss 0.0015175878070294857, linear weight tensor([[ 1.8566]])\n",
      "epoch 868, loss 0.0015142938354983926, linear weight tensor([[ 1.8568]])\n",
      "epoch 869, loss 0.0015110084787011147, linear weight tensor([[ 1.8570]])\n",
      "epoch 870, loss 0.0015077302232384682, linear weight tensor([[ 1.8571]])\n",
      "epoch 871, loss 0.0015044559258967638, linear weight tensor([[ 1.8573]])\n",
      "epoch 872, loss 0.0015011931536719203, linear weight tensor([[ 1.8574]])\n",
      "epoch 873, loss 0.0014979359693825245, linear weight tensor([[ 1.8576]])\n",
      "epoch 874, loss 0.0014946858864277601, linear weight tensor([[ 1.8577]])\n",
      "epoch 875, loss 0.0014914418570697308, linear weight tensor([[ 1.8579]])\n",
      "epoch 876, loss 0.001488205511122942, linear weight tensor([[ 1.8580]])\n",
      "epoch 877, loss 0.0014849761500954628, linear weight tensor([[ 1.8582]])\n",
      "epoch 878, loss 0.0014817535411566496, linear weight tensor([[ 1.8583]])\n",
      "epoch 879, loss 0.001478538615629077, linear weight tensor([[ 1.8585]])\n",
      "epoch 880, loss 0.0014753318391740322, linear weight tensor([[ 1.8587]])\n",
      "epoch 881, loss 0.001472129370085895, linear weight tensor([[ 1.8588]])\n",
      "epoch 882, loss 0.0014689348172396421, linear weight tensor([[ 1.8590]])\n",
      "epoch 883, loss 0.0014657469000667334, linear weight tensor([[ 1.8591]])\n",
      "epoch 884, loss 0.0014625658513978124, linear weight tensor([[ 1.8593]])\n",
      "epoch 885, loss 0.001459393184632063, linear weight tensor([[ 1.8594]])\n",
      "epoch 886, loss 0.0014562265714630485, linear weight tensor([[ 1.8596]])\n",
      "epoch 887, loss 0.0014530665939673781, linear weight tensor([[ 1.8597]])\n",
      "epoch 888, loss 0.0014499116223305464, linear weight tensor([[ 1.8599]])\n",
      "epoch 889, loss 0.0014467689907178283, linear weight tensor([[ 1.8600]])\n",
      "epoch 890, loss 0.0014436292694881558, linear weight tensor([[ 1.8602]])\n",
      "epoch 891, loss 0.0014404940884560347, linear weight tensor([[ 1.8603]])\n",
      "epoch 892, loss 0.0014373682206496596, linear weight tensor([[ 1.8605]])\n",
      "epoch 893, loss 0.0014342510839924216, linear weight tensor([[ 1.8606]])\n",
      "epoch 894, loss 0.0014311398845165968, linear weight tensor([[ 1.8608]])\n",
      "epoch 895, loss 0.0014280346222221851, linear weight tensor([[ 1.8609]])\n",
      "epoch 896, loss 0.001424935064278543, linear weight tensor([[ 1.8611]])\n",
      "epoch 897, loss 0.0014218431897461414, linear weight tensor([[ 1.8612]])\n",
      "epoch 898, loss 0.0014187585329636931, linear weight tensor([[ 1.8614]])\n",
      "epoch 899, loss 0.0014156794641166925, linear weight tensor([[ 1.8615]])\n",
      "epoch 900, loss 0.001412607030943036, linear weight tensor([[ 1.8617]])\n",
      "epoch 901, loss 0.001409542397595942, linear weight tensor([[ 1.8618]])\n",
      "epoch 902, loss 0.001406484516337514, linear weight tensor([[ 1.8620]])\n",
      "epoch 903, loss 0.0014034330379217863, linear weight tensor([[ 1.8621]])\n",
      "epoch 904, loss 0.0014003872638568282, linear weight tensor([[ 1.8623]])\n",
      "epoch 905, loss 0.0013973478926345706, linear weight tensor([[ 1.8624]])\n",
      "epoch 906, loss 0.001394317951053381, linear weight tensor([[ 1.8626]])\n",
      "epoch 907, loss 0.001391290221363306, linear weight tensor([[ 1.8627]])\n",
      "epoch 908, loss 0.00138827261980623, linear weight tensor([[ 1.8629]])\n",
      "epoch 909, loss 0.0013852602569386363, linear weight tensor([[ 1.8630]])\n",
      "epoch 910, loss 0.0013822526670992374, linear weight tensor([[ 1.8632]])\n",
      "epoch 911, loss 0.0013792545069009066, linear weight tensor([[ 1.8633]])\n",
      "epoch 912, loss 0.0013762605376541615, linear weight tensor([[ 1.8635]])\n",
      "epoch 913, loss 0.0013732729712501168, linear weight tensor([[ 1.8636]])\n",
      "epoch 914, loss 0.0013702930882573128, linear weight tensor([[ 1.8638]])\n",
      "epoch 915, loss 0.0013673203065991402, linear weight tensor([[ 1.8639]])\n",
      "epoch 916, loss 0.0013643550919368863, linear weight tensor([[ 1.8641]])\n",
      "epoch 917, loss 0.0013613933697342873, linear weight tensor([[ 1.8642]])\n",
      "epoch 918, loss 0.0013584393309429288, linear weight tensor([[ 1.8644]])\n",
      "epoch 919, loss 0.0013554921606555581, linear weight tensor([[ 1.8645]])\n",
      "epoch 920, loss 0.0013525508111342788, linear weight tensor([[ 1.8647]])\n",
      "epoch 921, loss 0.0013496148167178035, linear weight tensor([[ 1.8648]])\n",
      "epoch 922, loss 0.0013466867385432124, linear weight tensor([[ 1.8650]])\n",
      "epoch 923, loss 0.0013437664601951838, linear weight tensor([[ 1.8651]])\n",
      "epoch 924, loss 0.0013408479280769825, linear weight tensor([[ 1.8652]])\n",
      "epoch 925, loss 0.0013379412703216076, linear weight tensor([[ 1.8654]])\n",
      "epoch 926, loss 0.0013350382214412093, linear weight tensor([[ 1.8655]])\n",
      "epoch 927, loss 0.0013321398291736841, linear weight tensor([[ 1.8657]])\n",
      "epoch 928, loss 0.001329250750131905, linear weight tensor([[ 1.8658]])\n",
      "epoch 929, loss 0.0013263663277029991, linear weight tensor([[ 1.8660]])\n",
      "epoch 930, loss 0.0013234869111329317, linear weight tensor([[ 1.8661]])\n",
      "epoch 931, loss 0.001320616458542645, linear weight tensor([[ 1.8663]])\n",
      "epoch 932, loss 0.001317750196903944, linear weight tensor([[ 1.8664]])\n",
      "epoch 933, loss 0.0013148908037692308, linear weight tensor([[ 1.8666]])\n",
      "epoch 934, loss 0.0013120388612151146, linear weight tensor([[ 1.8667]])\n",
      "epoch 935, loss 0.0013091920409351587, linear weight tensor([[ 1.8668]])\n",
      "epoch 936, loss 0.0013063512742519379, linear weight tensor([[ 1.8670]])\n",
      "epoch 937, loss 0.0013035140000283718, linear weight tensor([[ 1.8671]])\n",
      "epoch 938, loss 0.0013006868539378047, linear weight tensor([[ 1.8673]])\n",
      "epoch 939, loss 0.0012978651793673635, linear weight tensor([[ 1.8674]])\n",
      "epoch 940, loss 0.0012950482778251171, linear weight tensor([[ 1.8676]])\n",
      "epoch 941, loss 0.0012922375462949276, linear weight tensor([[ 1.8677]])\n",
      "epoch 942, loss 0.0012894350802525878, linear weight tensor([[ 1.8679]])\n",
      "epoch 943, loss 0.0012866363395005465, linear weight tensor([[ 1.8680]])\n",
      "epoch 944, loss 0.0012838445836678147, linear weight tensor([[ 1.8681]])\n",
      "epoch 945, loss 0.0012810587650164962, linear weight tensor([[ 1.8683]])\n",
      "epoch 946, loss 0.0012782778358086944, linear weight tensor([[ 1.8684]])\n",
      "epoch 947, loss 0.0012755051720887423, linear weight tensor([[ 1.8686]])\n",
      "epoch 948, loss 0.0012727376306429505, linear weight tensor([[ 1.8687]])\n",
      "epoch 949, loss 0.0012699755607172847, linear weight tensor([[ 1.8689]])\n",
      "epoch 950, loss 0.0012672190787270665, linear weight tensor([[ 1.8690]])\n",
      "epoch 951, loss 0.0012644689995795488, linear weight tensor([[ 1.8691]])\n",
      "epoch 952, loss 0.0012617256725206971, linear weight tensor([[ 1.8693]])\n",
      "epoch 953, loss 0.0012589865364134312, linear weight tensor([[ 1.8694]])\n",
      "epoch 954, loss 0.001256255549378693, linear weight tensor([[ 1.8696]])\n",
      "epoch 955, loss 0.001253529917448759, linear weight tensor([[ 1.8697]])\n",
      "epoch 956, loss 0.0012508091749623418, linear weight tensor([[ 1.8699]])\n",
      "epoch 957, loss 0.0012480950681492686, linear weight tensor([[ 1.8700]])\n",
      "epoch 958, loss 0.001245388062670827, linear weight tensor([[ 1.8701]])\n",
      "epoch 959, loss 0.0012426846660673618, linear weight tensor([[ 1.8703]])\n",
      "epoch 960, loss 0.0012399876723065972, linear weight tensor([[ 1.8704]])\n",
      "epoch 961, loss 0.0012372974306344986, linear weight tensor([[ 1.8706]])\n",
      "epoch 962, loss 0.0012346121948212385, linear weight tensor([[ 1.8707]])\n",
      "epoch 963, loss 0.0012319323141127825, linear weight tensor([[ 1.8708]])\n",
      "epoch 964, loss 0.0012292602332308888, linear weight tensor([[ 1.8710]])\n",
      "epoch 965, loss 0.001226593041792512, linear weight tensor([[ 1.8711]])\n",
      "epoch 966, loss 0.0012239309726282954, linear weight tensor([[ 1.8713]])\n",
      "epoch 967, loss 0.001221274840645492, linear weight tensor([[ 1.8714]])\n",
      "epoch 968, loss 0.0012186265084892511, linear weight tensor([[ 1.8715]])\n",
      "epoch 969, loss 0.0012159812031313777, linear weight tensor([[ 1.8717]])\n",
      "epoch 970, loss 0.0012133435811847448, linear weight tensor([[ 1.8718]])\n",
      "epoch 971, loss 0.001210709917359054, linear weight tensor([[ 1.8720]])\n",
      "epoch 972, loss 0.0012080814922228456, linear weight tensor([[ 1.8721]])\n",
      "epoch 973, loss 0.0012054613325744867, linear weight tensor([[ 1.8722]])\n",
      "epoch 974, loss 0.0012028453638777137, linear weight tensor([[ 1.8724]])\n",
      "epoch 975, loss 0.001200236496515572, linear weight tensor([[ 1.8725]])\n",
      "epoch 976, loss 0.0011976317036896944, linear weight tensor([[ 1.8726]])\n",
      "epoch 977, loss 0.0011950322659686208, linear weight tensor([[ 1.8728]])\n",
      "epoch 978, loss 0.0011924398131668568, linear weight tensor([[ 1.8729]])\n",
      "epoch 979, loss 0.001189853297546506, linear weight tensor([[ 1.8731]])\n",
      "epoch 980, loss 0.001187270157970488, linear weight tensor([[ 1.8732]])\n",
      "epoch 981, loss 0.0011846956331282854, linear weight tensor([[ 1.8733]])\n",
      "epoch 982, loss 0.0011821250664070249, linear weight tensor([[ 1.8735]])\n",
      "epoch 983, loss 0.0011795585742220283, linear weight tensor([[ 1.8736]])\n",
      "epoch 984, loss 0.0011770001146942377, linear weight tensor([[ 1.8738]])\n",
      "epoch 985, loss 0.0011744453804567456, linear weight tensor([[ 1.8739]])\n",
      "epoch 986, loss 0.0011718968162313104, linear weight tensor([[ 1.8740]])\n",
      "epoch 987, loss 0.001169355120509863, linear weight tensor([[ 1.8742]])\n",
      "epoch 988, loss 0.001166817732155323, linear weight tensor([[ 1.8743]])\n",
      "epoch 989, loss 0.0011642861645668745, linear weight tensor([[ 1.8744]])\n",
      "epoch 990, loss 0.0011617604177445173, linear weight tensor([[ 1.8746]])\n",
      "epoch 991, loss 0.0011592390947043896, linear weight tensor([[ 1.8747]])\n",
      "epoch 992, loss 0.0011567241745069623, linear weight tensor([[ 1.8748]])\n",
      "epoch 993, loss 0.0011542142601683736, linear weight tensor([[ 1.8750]])\n",
      "epoch 994, loss 0.0011517094681039453, linear weight tensor([[ 1.8751]])\n",
      "epoch 995, loss 0.00114921061322093, linear weight tensor([[ 1.8752]])\n",
      "epoch 996, loss 0.0011467168806120753, linear weight tensor([[ 1.8754]])\n",
      "epoch 997, loss 0.0011442294344305992, linear weight tensor([[ 1.8755]])\n",
      "epoch 998, loss 0.0011417452478781343, linear weight tensor([[ 1.8757]])\n",
      "epoch 999, loss 0.001139265950769186, linear weight tensor([[ 1.8758]])\n",
      "epoch 1000, loss 0.0011367953848093748, linear weight tensor([[ 1.8759]])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "\n",
    "    epoch +=1\n",
    "    inputs = Variable(torch.from_numpy(x_train))\n",
    "    labels = Variable(torch.from_numpy(y_correct))\n",
    "\n",
    "    #clear grads\n",
    "    optimiser.zero_grad()\n",
    "    #forward to get predicted values\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()# back props\n",
    "    optimiser.step()# update the parameters\n",
    "    print('epoch {}, loss {}, linear weight {}'.format(epoch,loss.data,model.state_dict()['linear.weight']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('linear.weight', tensor([[ 1.8705]])), ('linear.bias', tensor([ 1.0775]))])\n"
     ]
    }
   ],
   "source": [
    "predicted = model.forward(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "\n",
    "plt.plot(x_train, y_correct, 'go', label = 'from data', alpha = .5)\n",
    "plt.plot(x_train, predicted, label = 'prediction', alpha = 0.5)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear.weight tensor([[ 1.8309]])\n",
      "linear.bias tensor([ 1.1011])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_items([('linear.weight', tensor([[ 1.8309]])), ('linear.bias', tensor([ 1.1011]))])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Computational Methods)",
   "language": "python",
   "name": "cm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
