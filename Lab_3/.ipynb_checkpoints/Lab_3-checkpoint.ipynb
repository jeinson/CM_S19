{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![spicy_meme](https://jeinson.github.io/images/mem2.jpg)\n",
    "\n",
    "<center>Depends who you ask...</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Linear Regression with Stochatic Gradient Descent \n",
    "\n",
    "In the previous lab, we introduced auto differentiation, then used gradient descent to maximize the likelihood for simple model parameters. Today, we'll build on this knowledge by implementing linear regression using pytorch's Neural Network package, `torch.nn`. \n",
    "\n",
    "To review, in least squares linear regression we have a design matrix $X$ and a set of corresponding outcomes $Y$, and the goal is to learn a $\\beta$ such that $\\hat{Y} = X^\\top \\beta + \\epsilon$ minimizes the loss function $\\frac{1}{n}\\sum_n (Y - \\hat{Y})^2$, with $\\mathbb{E}[\\epsilon] = 0$\n",
    "\n",
    "In the simple case of one feature and one output (see above meme):\n",
    "\n",
    "$$ \n",
    "Y = \n",
    "\\begin{bmatrix}\n",
    "Y_1 \\\\\n",
    "Y_2 \\\\\n",
    "\\vdots \\\\\n",
    "Y_n \n",
    "\\end{bmatrix}  , \n",
    "X = \\begin{bmatrix}\n",
    "1 & X_1 \\\\\n",
    "1 & X_2 \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & X_n \n",
    "\\end{bmatrix} ,\n",
    "\\beta = \n",
    " \\begin{bmatrix}\n",
    "\\beta_1 \\\\\n",
    "\\beta_2 \n",
    "\\end{bmatrix}, \\text{and } \n",
    "\\epsilon = \n",
    "\\begin{bmatrix}\n",
    "\\epsilon_1 \\\\\n",
    "\\epsilon_2 \\\\ \n",
    "\\vdots \\\\\n",
    "\\epsilon_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In $X$, the leading column of 1s allows for an intercept parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common method used for solving linear equations is the **Method of Least Squares**. This is what popular packages like scikit learn or R's `lm` function are doing under the hood. With some fun matrix algebra, the solution pops out fairly easily. \n",
    "\n",
    "$$\\begin{align}\n",
    "X\\beta &= Y\\\\\n",
    "X^\\top X\\beta &= X^\\top Y \\\\\n",
    "(X^\\top X)^{-1}X^\\top X \\beta&= (X^\\top X)^{-1} X^\\top Y \\\\\n",
    "&\\implies \\\\\n",
    "\\hat{\\beta}&= (X^\\top X)^{-1} X^\\top Y\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This solution involves taking the inverse of a matrix, which gets computationally very expensive as the size of your dataset increases. Therefore, we can think of the problem of optimizing $\\beta$ in terms of the machine learning workflow discussed in class, where essentially, we try to mimize the quantity $Q$ in \n",
    "$$ (X^\\top \\beta -  Y)^\\top (X^\\top \\beta - Y) = Q $$\n",
    "The matrix calculus needed to optimize $\\beta$ is pretty ugly, but fortunately we can use autodifferentiation and gradient descent to arrive at an optimal solution. The task of modeling linear regression can be thought of in terms of the machine learning workflow discussed in class. \n",
    "\n",
    "![Learning Algorithm](https://jeinson.github.io/images/Learning_Algorithm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the following modules. These are the only ones needed to complete the lab. \n",
    "# -.5 if you import any other modules ;-)\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Simulate some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using torch tensors, randomly draw a `[2 x 1] `$\\beta$ vector, and 50 random $X_i$s for training. Then matrix multiply $X$ and $\\beta$, and add some Gaussian noise, to make a $Y$ vector. This will be the training set used for training your regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot X and Y to verify their linear relationship. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Solve Ordinary Least Squares\n",
    "\n",
    "Using the formula derived in the introduction, find the least squares solution for your sample dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot a trend line on your data using your least squares estimate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Defining the model as a neural network\n",
    "\n",
    "In pytorch, neural networks are are typically constructed by defining a class that includes at the very least a `forward` method. Some information on the `torch.nn` module is [here](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)\n",
    "\n",
    "Fill in the missing pieces to make a neural network class called `LinearRegression` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    \n",
    "    # The class constructor defines the parameters (ie layers) of the neural network\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # This neural network will have a single linear layer\n",
    "        #### your code here ####\n",
    "    \n",
    "    # The forward method ties the layers together to build the network.\n",
    "    # We take the gradient of this composite function using back propogation\n",
    "    def forward(self, x):\n",
    "        #### your code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training, the model initializes with random parameters. Define a new LinearRegression object, and print the values of the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LinearRegression()\n",
    "#### your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is defining a loss function, which our model will attempt to miminize during training, and an optimization function, which specifies the learning method and learning rate. The most commonly used optimizer is **Stochastic Gradient Descent**, which is conveniently implemented as `torch.optim.SGD`. More on this [here](https://pytorch.org/docs/stable/optim.html). Which loss function should you use to optimizer the parameters in linear regression? [Check here for some options](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "\n",
    "Define a loss object called `criterion` and an optimizer object called `optimizer`, for the parameters of your new `LinearRegression` object, and with a learning rate of .01. (You may have to play around with the learning rate if your model diverges off to infinity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = #### your code here ###\n",
    "optimizer = #### your code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's train the model. This is where the magic happens! In training, we iterate through the process of choosing parameters, calculating the loss, then marching down the gradient until the parameters converge. Fill in the following code chunk to optimize the parameters of `lr_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "\n",
    "for epoch in range(epochs):\n",
    "        \n",
    "    # Estimate Y_hat with the current model\n",
    "\n",
    "    # Compute the loss\n",
    "        \n",
    "    # Compute the gradient of the loss, and update the model parameters with the optimizer\n",
    "    # Don't forget to zero the gradient after each epoch!\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"loss {}, beta 1 {}\".format(loss,list(lr_model.parameters())[1].detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the dataset with a trend line defined by the parameters your neural network learned, in addition to the OLS trend line, and label which is which. Try setting the alpha if they end up right on top of each other, or moving one of the lines over a bit so they are both easy to see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool huh? I'll leave it up to you to decide which method you like better, but hopefully this demonstrates the flexibility of a neural network. Later on, we'll see how this package can be used to learn far more complicated patterns in data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Computational Methods)",
   "language": "python",
   "name": "cm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
