{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Scikit-learn into your computational methods environment with `conda install scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Let's get this data, fam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n Train: 10\n",
      "n Val: 10\n"
     ]
    }
   ],
   "source": [
    "TESTING = False\n",
    "data = load_breast_cancer()\n",
    "n = data.data.shape[0]\n",
    "valFrac = 0.2\n",
    "X = data.data\n",
    "Y = data.target.reshape([X.shape[0], 1])\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = valFrac, random_state = 1876)\n",
    "if TESTING:\n",
    "    X_train = X_train[0:10,:]\n",
    "    X_val = X_train\n",
    "    Y_train = Y_train[0:10]\n",
    "    Y_val = Y_train\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "X_val = torch.from_numpy(X_val).float()\n",
    "Y_train = torch.from_numpy(Y_train).float()#.long()\n",
    "Y_val = torch.from_numpy(Y_val).float()#.long()\n",
    "print(\"n Train: {}\\nn Val: {}\".format(X_train.shape[0], X_val.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Classification Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "Logistic regression is your bread and butter baseline classification model. Before implementing anything too fancy, you should see how far logistic regression can get you. When should you use logistic regression? Whenever you're performing supervised learning and have a binary response variable (only two classes). Luckily for us, most of the math for a logistic regression model has already been worked out in Lab 3 with linear regression. But there's just one problem, linear regression is unbounded, and if we're doing classification we want our model to predict the probability of a model belonging to one class or the other. So instead of letting our model's outputs be between $-\\infty$ and $\\infty$ we want to constrain it between $0$ and $1$.\n",
    "\n",
    "To blatantly plagiarize another TAs work, in least squares linear regression we have a feature matrix $X$ and a set of corresponding outcomes $Y$, and the goal is to learn a $\\beta$ such that $\\hat{Y} = X^\\top \\beta + \\epsilon$ minimizes the loss function $\\sum_i (Y - \\hat{Y})^2$, with $\\mathbb{E}[\\epsilon] = 0$.\n",
    "\n",
    "Using the input $X$ and our model parameters $\\beta$ we'll convert linear regression into logistic regression. First, why do we want to bound our model between $0$ and $1$? Because we're doing classification we need an easy way to define when our prediction is for one class or the other, and if our model can only output probabilities then we can use a cutoff (say $0.5$) and bin every observation into a class. Squashing inputs between $0$ and $1$ is done using the sigmoid function $\\sigma(a) = \\frac{1}{1 + exp(-a)}$. Using our inputs $X$, our learned parameters $\\beta$ and $\\sigma(\\cdot)$ we have the makings of greatness, or at least some kind of baseline model. We write the probability of our 'positive' class (an arbiterary designation) as $$p(Y = 1|X;\\beta) = \\sigma(\\beta^{T}X)$$ and our 'negative' class as $$p(Y = 0|X;\\beta) = 1 - \\sigma(\\beta^{T}X)$$. For simplicity's sake let $a = beta^{T}X$ for he remainder of this cell\n",
    "\n",
    "The loss function for logistic regression is similar to what we used in Lab 2 for MLE. For a single observation $x_i$ and its response variable $y_i$ we define our prediction's loss as $L(\\beta;y_i, x_i) = \\sigma(a)^{i_i}(1 - \\sigma(a))^{1-y_i}$. And for an entire dataset of $n$ examples after taking the log our loss is\n",
    "\n",
    "$$\\sum_{i=1}^n y_ilog(\\sigma(a)) + (1-y_i)log(1-\\sigma(a))$$\n",
    "\n",
    "Using our usual tools of gradient descent, and stochastic gradient descent we can now learn the parameters $\\beta$ which minimize our loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class binaryClassifier(nn.Module):\n",
    "    \n",
    "    # The class constructor defines the parameters (ie layers) of the neural network\n",
    "    def __init__(self, nFeats, activationFunction = None):#nn.LogSoftmax(dim = 1)):\n",
    "        super(binaryClassifier, self).__init__()\n",
    "        # What type of parameters do we need to add?\n",
    "        self.linear = nn.Linear(in_features = nFeats, out_features = 1)\n",
    "        self.linear.weight= torch.nn.init.xavier_uniform_(self.linear.weight, gain = 0.001)\n",
    "        self.activationFunction = activationFunction        \n",
    "    # The forward method ties the layers together to build the network.\n",
    "    # We take the gradient of this composite function using back propogation\n",
    "    def forward(self, x):\n",
    "        if self.activationFunction is None:\n",
    "            out = self.linear(x)\n",
    "        else:\n",
    "            out = self.activationFunction(self.linear(x))\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normDist = torch.distributions.normal.Normal(0, 1)\n",
    "probitRegressionModel = binaryClassifier(nFeats = X_train.shape[1], activationFunction = normDist.cdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0205, -0.1408,  0.0549, -0.1256,  0.1180,  0.0550, -0.0599,  0.0754,\n",
       "         -0.0383, -0.0102,  0.1401, -0.1564,  0.1770,  0.0602, -0.1437, -0.1688,\n",
       "          0.1500, -0.0555, -0.1377, -0.1282,  0.0701,  0.0496, -0.0673,  0.0175,\n",
       "         -0.0326, -0.0566, -0.0307, -0.0974, -0.1259,  0.1155]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probitRegressionModel.linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 6.5663e-06, -1.9557e-05, -4.1058e-05,  2.2804e-05, -4.3453e-05,\n",
       "          9.6415e-06, -1.4594e-05,  4.0654e-05,  9.1947e-06,  1.7787e-05,\n",
       "          3.1158e-05, -3.9275e-06,  1.0329e-06,  3.0537e-05, -1.8159e-05,\n",
       "         -2.4567e-05,  2.7381e-05,  3.4858e-06, -6.1768e-06, -6.1581e-06,\n",
       "         -9.5324e-06,  4.1520e-05, -2.1943e-05,  4.1130e-05,  1.3174e-05,\n",
       "          3.6454e-05,  8.1693e-06,  1.2613e-05, -1.6743e-05,  3.1575e-05]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.xavier_uniform_(probitRegressionModel.linear.weight, gain = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = nn.Sigmoid()\n",
    "logisticRegressionModel = binaryClassifier(nFeats = X_train.shape[1], activationFunction = None)\n",
    "lossFunction = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(logisticRegressionModel.parameters(), lr = .0001, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 6.628029823303223\n",
      "loss 0.00011123196600237861\n",
      "loss 5.960463411724959e-08\n",
      "loss 5.960463411724959e-08\n",
      "loss 5.960463411724959e-08\n",
      "loss 5.960463411724959e-08\n",
      "loss 5.960463411724959e-08\n",
      "loss 5.960463411724959e-08\n",
      "loss 5.960463411724959e-08\n",
      "loss 5.960463411724959e-08\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "        \n",
    "    # Estimate Y_hat with the current model\n",
    "    \n",
    "    Y_hat = logisticRegressionModel(X_train)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = lossFunction(Y_hat, Y_train)\n",
    "        \n",
    "    # Compute the gradient of the loss\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(\"loss {}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probit Model\n",
    "\n",
    "We defined $p(Y = 1|a) = \\sigma(a)$ in logistic regression, but the more general form of a linear model would be $p(Y = 1|X;\\beta) = f(\\beta^{T}X)$ where $f(\\cdot)$ is known as an activation function. Another activation function we could have used is known as the inverse probit function which is the cumulative distribution function of a standard normal defined as $\\Phi(a) = \\frac{1}{2}(1 + erf(\\frac{1}{\\sqrt{2}}))$ (Did you hear that? That was the sound of an absolute ton of details being skipped over. For more information about probit regression on page 210 [here](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf)). Here $erf(\\cdot)$ is known as the error function. All the same steps apply for the logistic regression example, except instead of $\\sigma(\\cdot)$ we use $\\Phi(\\cdot)$\n",
    "\n",
    "$$\\sum_{i=1}^n y_ilog(\\Phi(a)) + (1-y_i)log(1-\\Phi(a))$$\n",
    "\n",
    "\n",
    "\n",
    "Tips:\n",
    "1. For the love of your weekend don't try to implement the CDF of a normal distribution. Search around and find out how you can get the cdf of different distributions in using Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "normDist = torch.distributions.normal.Normal(0, 1)\n",
    "probitRegressionModel = binaryClassifier(nFeats = X_train.shape[1], activationFunction = normDist.cdf)\n",
    "lossFunction = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(probitRegressionModel.parameters(), lr = .000001, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.7132978439331055\n",
      "loss 0.32142892479896545\n",
      "loss 0.21147653460502625\n",
      "loss 0.15404944121837616\n",
      "loss 0.11979521811008453\n",
      "loss 0.09743179380893707\n",
      "loss 0.08183403313159943\n",
      "loss 0.07039885222911835\n",
      "loss 0.061685435473918915\n",
      "loss 0.05484015494585037\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "        \n",
    "    # Estimate Y_hat with the current model\n",
    "    \n",
    "    Y_hat = probitRegressionModel(X_train)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = lossFunction(Y_hat, Y_train)\n",
    "        \n",
    "    # Compute the gradient of the loss\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(\"loss {}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hinge Loss\n",
    "We can also change the loss function we use in logistic regression to the hinge loss by reconfiguring how we view the data. To do this we formulate our response variable as either $-1$ or $1$. Now, $p(Y = 1|X;\\beta) = \\sigma(a)$ remains unchanged, but $p(Y = -1|X;\\beta) = 1 - \\sigma(a) = \\sigma(-a) = \\sigma(ya)$. In the last step recall that $Y \\in \\{-1, 1\\}$ so $p(Y|X;\\beta) = \\sigma(ya)$\n",
    "\n",
    "Using the log likelihood and our updated probability functions our loss now becomes:\n",
    "\n",
    "$$\\sum_{i=1}^{n}\\sigma(ya)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def myHingeLoss(a, y):\n",
    "    m = nn.LogSigmoid()\n",
    "    out = -m(Y_hat*Y_train_hinge)\n",
    "    out = torch.mean(out)\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegressionHingeModel = binaryClassifier(nFeats = X_train.shape[1], activationFunction = None)\n",
    "lossFunction = myHingeLoss\n",
    "optimizer = torch.optim.SGD(logisticRegressionHingeModel.parameters(), lr = .0001, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train_hinge = Y_train.clone().detach()\n",
    "Y_train_hinge[Y_train_hinge == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.7578908205032349\n",
      "loss 1.4084984064102173\n",
      "loss 7.104622000042582e-06\n",
      "loss 3.7550223623838974e-06\n",
      "loss 2.6464110760571202e-06\n",
      "loss 2.0503787254710915e-06\n",
      "loss 1.6689160702298977e-06\n",
      "loss 1.4185804957378423e-06\n",
      "loss 1.227848088092287e-06\n",
      "loss 1.084798668671283e-06\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "        \n",
    "    # Estimate Y_hat with the current model\n",
    "    \n",
    "    Y_hat = logisticRegressionHingeModel(X_train)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = lossFunction(Y_hat,Y_train_hinge)\n",
    "        \n",
    "    # Compute the gradient of the loss\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(\"loss {}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [1.1202e-24],\n",
       "        [0.0000e+00],\n",
       "        [1.4829e-11],\n",
       "        [2.9223e-10],\n",
       "        [1.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [9.9999e-01]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_hat\n",
    "m = nn.Sigmoid()\n",
    "m(Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  17.4905],\n",
       "        [  26.0904],\n",
       "        [  30.7312],\n",
       "        [ -55.1486],\n",
       "        [-119.8049],\n",
       "        [ -24.9344],\n",
       "        [ -21.9535],\n",
       "        [  30.1257],\n",
       "        [  32.1253],\n",
       "        [  11.5451]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_hinge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Computational Methods)",
   "language": "python",
   "name": "compmeth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
